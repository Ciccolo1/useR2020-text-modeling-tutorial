description,language,year
"Have you ever wanted to write a GUI application in Python that you can run on both your laptop and your phone? Have you been looking to contribute to an open source project, but you don't know where to start?

BeeWare is a set of software libraries for cross-platform native app development from a single Python codebase and tools to simplify app deployment. The project aims to build, deploy, and run apps for Windows, Linux, macOS, Android, iPhone, and the web. It is native because it is actually using your platform's native GUI widgets, not a theme, icon pack, or webpage wrapper.

This talk will teach you how Toga, the BeeWare GUI toolkit, is architected and then show you how you can contribute to Toga by creating your own GUI widget in five easy steps.",python,2019
"We rarely think about the dot “.” between our objects and their fields, but there are quite a lot of things that happen every time we use one in Python. This talk will explore the details of what happens, how the descriptor protocol works, and how it can be used to alter the Python object model.
",python,2019
"Account security means making sure your users are only ones who can access their accounts. Account takeovers happen for a variety of reasons -- password re-use, compromised computers, guessable passwords, and more. This talk gives you concepts and concrete skills that will help you identify and prevent account takeovers and limit the damage. It’s inspired by practices in use at GitHub, Google, and the Python Package Index.
",python,2019
"Do you feel overwhelmed by the prospect of having to find a new software engineering job because you dread the technical interviewing process? Have you been putting off submitting your job applications because you think you won't be ready to interview until you do ""just one more"" day of studying? 

In this talk, you'll learn which concepts are the most important to study for entry-level roles and how to use your Python skills to convey your understanding of these concepts with confidence and clarity. ",python,2019
"Everyone’s talking about it. Everyone’s using it. But most likely, they’re doing it wrong, just like we did.

By building a simplified chaos monkey service, we will walk through how to create a good foundation for an asyncio-based service, including graceful shutdowns, proper exception handling, and testing asynchronous code. We’ll get into the hairier topics as well, covering topics like working with synchronous code, debugging and profiling, and working with threaded code. We’ll learn how to approach asynchronous and concurrent programming with Python’s `asyncio` library, take away some best practices, and learn what pitfalls to avoid.",python,2019
"We will look into a day in the life of a Software Engineer with limited vision to Understand their difficulties at work and how they can overcome those difficulties to become successful in their role.

I am a backend Software Engineer at Yelp who uses Python extensively for building Yelps infrastructure and internal tools. I also suffer from a genetic disorder called Albinism which often results in limited visual acuity that can range from 20/120 to 20/200 in most common cases. With such a low vision it's extremely difficult to read the computer screen without the use of on-screen magnifiers. In this talk, we will see how a person with adverse visual acuity can thrive and be successful in the field of Software Engineering. We will address the importance and meaning of accessibility for Software Engineers with partial vision and recommend some best practices that are available today. We will also talk about the importance of an inclusive work culture that can help foster creativity and ease ramp up for a Software Engineer with a disability.
",python,2019
"Medieval European Nobility was obsessed with Lineage. They created a Heraldic System to track families, which assigned each family a unique Coat of Arms.

Any painting of the Coat of Arms was not the official version. The official version was a ""Blazon"" - a precise, terse description in heraldic language.   This heraldic language reads like English, Latin, French, and XML had a baby.  It's a fully recursive language with a formal grammar, variable assignment, positional arguments, and also, Lions, Bears, and Pythons.

Here's an example: _Sable, on a fesse or three lions gules_

In this talk, we look at parsing this Medieval _Domain Specific Language_ with Python. Along the way, we'll learn a little history, and the tools for parsing and writing your own DSL.
",python,2019
"In July of 2018, Guido van Rossum stepped down as “Benevolent Dictator for Life” of Python.  In December, Python core developers voted on a new governance structure to guide Python going forward.  This talk explores what’s changing and how it may impact the Python community.

Based on analysis of the relevant PEPs and other documentation, and strengthened via a series of interviews with core developers and other community leaders, we’ll cover Python governance past, present and future.  Particular care will be taken to explain why the core developers chose the governance model they did, what the new system entails, and how community members can participate in it.",python,2019
"If you maintain a library, how can you innovate without breaking the projects that depend on it? Follow semantic versioning, add APIs conservatively, add parameters compatibly, use DeprecationWarnings and publish a deprecation policy, guide your users on how to upgrade, and make wise choices about when to break backwards compatibility.",python,2019
"Embroidery is a technology that dates back centuries and is still popular in the present day among craftspeople around the world. Cross-stitch refers to the creation of crosses in a grid that combines to build up an image, based on a 'chart' or pattern of the intended design.

Even though entire pieces could be created based on completely manual processes, much of the technology behind automating chart creation is locked behind paid software. 

During this presentation, we will discuss how we can leverage the Python Imaging Library (PIL, now Pillow) in order to take source images and turn them into cross-stitch charts. 

The resulting art piece from the talk, the PyCon US 2019 logo, will be auctioned off at the PyLadies Charity Auction. Tickets to the Charity Auction available separately, see website for details. 

",python,2019
"Security incident response is an intense, high stress, high skill job that relies heavily on human judgement. Despite that, for reasons that we can't begin to understand, a big part of an incident responder's job seems to be opening numerous browser tabs and copy-pasting bits of text from one system to another. The hard parts of incident response can't be automated, but there are entire classes of busy-work that we can eliminate with a few web hooks and some artisanal Python.

In this talk we're going to discuss how to use Python to automate security incident response team (SIRT) operations. We'll give an overview of what a typical SecOps/SIRT infrastructure looks like, how and where automation fits in, and dive into some code. We'll walk through a simple example, with screenshots and code, of automating a SecOps process. We want to show that  getting started with security automation doesn't have to be difficult or expensive (though vendors will happily take your money). Just a little bit of Python can make some great quality of life improvements for incident responders.",python,2019
"There's one part of building a Django app I hate: setting up handling of assets and media. There are so many moving pieces — static assets,  asset compilation and compression, file uploads, storage engines, etc. etc. I can never remember how it all fits together. I wrote this talk for one very selfish reason: to document how this all works, in one place, once and for all. 

This talk is primarily intended for Django developers who want to build an asset pipeline that Just Works. The bulk of the talk covers front-end tooling (Webpack, PostCSS, Babel, etc.), so full-stack developers of any stripe will find something here, too.",python,2019
"Many new coders seek out open source projects, intending to contribute, and then get overwhelmed and leave. Project maintainers often want the help, but don’t realize how they are inadvertently appearing unwelcoming. I will discuss some of the most common complaints I’ve heard from new coders who tried to contribute but left in frustration, and ways that these can be addressed without putting too much burden on the maintainers.",python,2019
"Bayesian A/B testing has gained much popularity over the years. It seems, however, that the examples stop at two groups. This begs the questions: should we not be able to do more than simple two-group, case/control comparisons? Is there a special procedure that's necessary, or is there a natural extension of commonly-used Bayesian methods?

In this talk, I will use life-like, simulated examples, inspired from work and from meeting others at conferences, to show how to generalize A/B testing beyond the rigid assumptions commonly highlighted. Specifically, I will show two examples, one involving Bayesian estimation on click data on a website, and another on 4-parameter dose-response curves. 

There will be plenty of code from the modern PyData stack, involving the use of PyMC3, pandas, holoviews, and more. ",python,2019
"Find yourself doing the same thing over and over again? Does it take more than one command to run your tests? build your docs? publish your project? deploy?

It is often difficult to share your code because others can run or test it? Does your README have a series of complicated steps to get things set up?

This talk explores three open-source tools that are wonderful at helping you and your project automate tasks. We'll look at Tox, which specializes in Python test environments, Nox, which offers a slightly different approach, and finally, PyInvoke, which you can use to automate just about anything.",python,2019
"Observability is often thought of as just a new word for monitoring. While it encompasses traditional devops areas such as monitoring, metrics, and infrastructure management, it’s much deeper and empowers developers at all levels of the stack. **Observability is about achieving a deep understanding of your software**. This not only helps you localize and debug production issues but removes uncertainty and speculation, **empowering developers** to know their tools and improving engineering excellence. Observability helps developers “understand the narrative” of what’s going on in their software.

This talk is about how we’ve driven adoption of a culture of observability within our engineering culture. We'll define and motivate for our focus on observability; discuss the tangible tools we’ve built and best practices we’ve adopted to ingrain observability into our engineering culture; and provide some specific, real-world results we’ve achieved as part of this effort. We'll will focus particularly on the tooling we’ve adopted around Django and Celery  and some interesting experiences we had extending their internals.",python,2019
"Have you ever thought about what open source software or hardware could achieve? What if it could help improve people's lives by solving some of their health problems?

After the medical tech industry kept promising a system to help automatically manage insulin for type 1 diabetic people and never delivering, some people got together to find ways to do it with the tech they already had. Over the past few years, a ""closed-loop"" system has been developed to algorithmically regulate people's blood sugars. After reverse engineering bluetooth sensors and 915 MHz insulin pumps, the system became possible. As a diabetic, I also built this system and saw my sugar values stabilize much more than I could ever achieve doing it manually myself. Now I'm working on contributing back to the projects as well.

I want to talk about this system, from a technical side as well as a personal side. I'll talk about OpenAPS (the open artificial pancreas system) and how it works, what problems it solves, and its safety and security concerns. I also want to show how it's helped me, and what this means for my health now and in the future. I ultimately want to show how we, as software developers, can change people's lives through the code we write.",python,2019
"We all have to package Python based applications for various environments, starting from command line tools, to web applications. And depending on the users, it can be installed on thousands on computers or on a selected few systems. https://pypi.org is our goto place for finding any dependencies and also in most of the time we install binary wheels directly
from there, thus saving a lot time.

But, Python is also being used in many environments where security is the utter most important, and validating the dependencies of project is also very critical along with the actual project source code.  Many of noticed the recent incident where people were being able to [steal bticoins using a popular library](https://www.theregister.co.uk/2018/11/26/npm_repo_bitcoin_stealer/).This talk will take [SecureDrop client application](https://github.com/freedomofpress/securedrop-client) for journalists as an example project and see how we tried to tackle the similar problem. SecureDrop is an Open Source whistleblower system which is deployed over 75 news organizations all over the world. Our threat model has nation state actors as possible threats, so, security and privacy of the users of the system is a very important point of the whole project. The tools in this case are build and packaged into reproducible Debian deb packages and are installed on Qubes OS in the final end user systems.

There are two basic ways we handle Python project dependencies, for most of the development work, we use a virtualenv, and directly install the dependencies using wheels built from pypi.org. When we package the application for the end users, many times we package them using a operating system based package manager and ask the users to install using those (say RPM or Debian's deb package). In the second case, all the dependencies come as separate packages (and most of the time from the OS itself). The dependency is being handled by the OS package manager itself. That case, we can not update the dependencies fast enough if required, it depends on the packagers from the community who maintains those said packages in the distribution.

We use [dh-virtualenv](https://dh-virtualenv.readthedocs.io/en/1.0/) project to help us to use our own wheels + a virtualenv for the project to be packaged inside the debian .deb package.
This talk will go throuh [the process](https://github.com/freedomofpress/securedrop-client) of building wheels from known (based on sha256sum) source tarballs, and then having a gpg signed list of updated wheels and [a private index](https://github.com/freedomofpress/securedrop-debian-packaging/tree/master/simple) for the same. And also how we are verifying the wheels' sha256sum (and the signature of that list) during the build process.  The final output is reproducible Debian packages.",python,2019
"The admin interface that comes built-in with Django is one of the most-loved (and oft-abused) features.  However, early converts are often disappointed to find that the admin doesn't seem to be scaling as their database grows in size, forcing them (so they think) to switch to a custom interface much sooner than they would prefer.

However, many common performance issues can be fixed with a few small configuration changes that are much easier than a rewrite!

In this talk, we'll use an example project to demonstrate the most common performance pitfalls encountered when using the Django admin, and fix them - live!  We'll use django-debug-toolbar, a powerful debugging interface that everyone who uses Django should be familiar with, to identify our issues and confirm that they are fixed.",python,2019
"It’s 2019, and Moore’s Law is dead. CPU performance is plateauing, but GPUs provide a chance for continued hardware performance gains, if you can structure your programs to make good use of them.

CUDA is a platform developed by Nvidia for GPGPU--general purpose computing with GPUs. It backs some of the most popular deep learning libraries, like Tensorflow and Pytorch, but has broader uses in data analysis, data science, and machine learning. 

There are several ways that you can start taking advantage of CUDA in your Python programs. 

For some common Python libraries, there are drop-in replacements that let you start running computations on the GPU while still using familiar APIs. For example, CuPy provides a NumPy-like API for interacting with multi-dimensional arrays. Similarly, cuDF is a recent project that mimics the pandas interface for dataframes.

If you want more control over your use of CUDA APIs, you can use the PyCUDA library, which provides bindings for the CUDA API that you can call from your Python code. Compared with drop-in libraries, it gives you the ability to manually allocate memory on the GPU, and write custom CUDA functions (called kernels). However, its drawbacks include writing your CUDA code as large strings in Python, and compiling your CUDA code at runtime.

Finally, for the best performance you can use the Python C/C++ extension interface, the approach taken by deep learning libraries like Pytorch. One of the strengths of Python is the ability to drop down into C/C++, and libraries like NumPy take advantage of this for increased speed. If you use Nvidia’s nvcc compiler for CUDA, you can use the same extension interface to write custom CUDA kernels, and then call them from your Python code.

This talk will explore each of these methods, provide examples to get started, and discuss in more detail the pros and cons of each approach. ",python,2019
"Python is known for its ""batteries included"" philosophy but no Python developer can live without the language's rich library ecosystem. Unfortunately, as the number of libraries increases, so does the risk of cross-library incompatibilities, or ""dependency hell"".

Dependency hell arises when two libraries have mutually conflicting requirements. These can be very difficult for developers to diagnose and may not be fixable without avoiding certain libraries entirely.

After this talk, you - the library author - will have a practical set of simple best practices to follow that will allow you to build libraries that are compatible across the Python ecosystem.",python,2019
"Django Channels allows developers to make real-time web applications using websockets while maintaining access to the full Django batteries-included model for web applications. This talk will focus on what it takes to run a channels application in production, what's possible with Django Channels beyond chat rooms, and what pitfalls & idiosyncrasies you can expect to run into when using Channels in practice.",python,2019
"Spoiler alert:  yes, remote work really does work!

With nearly nine years of experience as a remote employee across three different companies, Lauren knows the ups and downs of remote work. In this session, Lauren will dive into what the research says about remote work and share her personal stories of failures and successes.

You'll walk away from this session knowing why remote work is awesome, empowered to convince your boss to let you work remotely, and armed with the tools you need to be a happy, successful remote employee.

If you've been thinking about making the transition to working remotely, you're a manager of people who are or could work remotely, or you've made the leap to remote work and are struggling to make it work, this is the session for you!",python,2019
"Managing a large open source project like CPython is no easy task. Learn how the Python core team automated their GitHub workflow with bots, making it easier for maintainers and contributors to collaborate together. Even if you’re not managing a large project, you can still build your own bot! Hear some ideas on what you can automate on GitHub and personalize your bot based on your own workflow. All you need is Python. Don’t be a robot; build the bot.",python,2019
"According to the always trustworthy Wikipedia, there are approximately 360 million native English speakers in the world. We, as developers, are so used to write code and documentation in English that we may not realize that this number only represents 4.67% of the world population. It is very useful to have a common language for the communication between developers, but this doesn’t mean that the user shouldn’t feel a little bit more comfortable when using your product. 

Translation of terms is only one step in the whole Internationalization (i18n) and Localization (l10n) process. It also entails number, date and time formatting, currency conversion, sorting, legal requirements, among other issues. This talk will go through the definition of i18n and l10n as well as show the main tools available for developers to support multiple languages and regional related preferences in their Python program. We will also see how one can enable local support for their website in Django. Finally, this presentation will discuss how we can manage Internationalization and Localization for a bigger product running in different platforms (front and back end) and how to incorporate i18n and l10n into our current development and deploy processes.

Oh, and by the way, “eita!” is a Brazilian interjection to show yourself surprised with something. 🙂 ",python,2019
"It seems that every week there is a news story that prompts software developers to think about the ethical implications of their work. As individuals, teams, and communities we need to consider the impact of the code we write. But what about code that we are using, but did not create? 

How does Open Source Software factor into this equation? OSS is used across the industry, but is written and maintained primarily by volunteers. What are best practices for maintainers, contributors, individual users, and companies who incorporate open source into their work? How can we protect ourselves and our users, while still benefiting from the innovation and collaboration of open source software?

",python,2019
"If we knew all of the bugs we needed to write tests for, wouldn't we just... not write the bugs?  So how can testing find bugs that nobody would think of?

The answer is to have a computer *write your tests for you!*  You declare what kind of input should work - from 'an integer' to 'matching this regex' to 'this Django model' and write a test which should always pass... then Hypothesis searches for the smallest inputs that cause an error.

If you’ve ever written tests that didn't find all your bugs, this talk is for you.  We'll cover the theory of property-based testing, a worked example, and then jump into a whirlwind tour of the library: how to use, define, compose, and infer strategies for input; properties and testing tactics for your code; and how to debug your tests if everything seems to go wrong.

By the end of this talk, you'll be ready to find real bugs with Hypothesis in anything from web apps to big data pipelines to CPython itself.  Be the change you want to see in your codebase - or contribute to Hypothesis itself and help drag the world kicking and screaming into a new and terrifying age of high quality software!",python,2019
"Telenovelas are beloved for their over the top drama and intricate plot twists. In this talk, we’ll review popular telenovelas to synthesize a typical telenovela arc and use it to train a deep learning model.

What would a telenovela script look like as imagined by a neural network? To answer this question, we’ll examine three Python deep learning frameworks - Keras, PyTorch, and TensorFlow - to determine the process of translating a telenovela into a neural network and ultimately determine which one will be best for the task at hand. Be prepared for amor, pasiòn, and y el misterioso!",python,2019
"Python makes it incredibly easy to build programs that do what you want. But what happens when you want to do what you want, but with more input? One of the easiest things to do is to make a program concurrent so that you can get more performance on large data sets. But what's involved with that?

Right now, there are any number of ways to do this, and that can be confusing! How does `asyncio` work? What's the difference between a thread and a process? And what's this Hadoop thing everyone keeps talking about?

In this talk, we'll cover some broad ground of what the different concurrency models available to you as a Python developer are, the tradeoffs and advantages of each, and explain how you can select the right one for your purpose.",python,2019
"Did you know there are multiple ways to raise and capture exceptions? Have you ever wondered if you should raise a built-in exception or create your own hierarchy? Did you ever find it hard to understand what an exception meant?

This talk will go through the decisions needed to raise and capture exceptions when creating a library. We will look at how to translate and handle errors, create your own exceptions, and make exceptions clear and easy to troubleshoot, while also understanding how they actually work, common pitfalls.
",python,2019
"Extracting tables from PDFs is hard. The Portable Document Format was not designed for tabular data. Sadly, a lot of open data is shared as PDFs and getting tables out for analysis is a pain. A simple copy-and-paste from a PDF into a text file or spreadsheet program doesn't work.

This talk will briefly touch upon the history of the Portable Document Format, discuss some problems that arise when extracting tabular data from PDFs using the current ecosystem of libraries and tools and demonstrate how Camelot and Excalibur solve this problem better and in a scalable manner. These easy-to-use packages automatically detect and extract tables from PDFs and give you access to the extracted tables in pandas DataFrames. You can also download them as CSVs or Excel files.
",python,2019
"Methane, the primary component of natural gas, is a 60 times more powerful climate change agent than carbon dioxide.  Current technologies for finding methane leaks in oil and gas infrastructure rely on driving well to well with a handheld camera.  At Kairos Aerospace, we have developed a plane-mounted sensor for detecting methane leaks, but the sensor is only part of the solution: getting information off the sensor and into customers’ hands required us to build an entire plane-to-report pipeline.  I’ll discuss the challenges we faced in developing a scalable, reliable, and cost-effective scientific computing platform in Python, with examples of novel solutions using Python’s extensive ecosystem of GIS, cloud computing and machine learning tools.
",python,2019
"Floating point numbers have been given a bad rap. They're mocked, maligned, and feared; the but of every joke, the scapegoat for every rounding error.

But this stigma is not deserved. Floats are friends! Friends that have been stuck between a rock and a computationally hard place, and been forced to make some compromises along the way… but friends never the less!

In this talk we'll look at the compromises that were made while designing the floating point standard (IEEE754), how to work within those compromises to make sure that `0.1 + 0.2 = 0.3` and not `0.30000000000000004`, how and when floats can and cannot be safely used, and some interesting history around fixed point number representation.

This talk is ideal for anyone who understands (at least in principle) binary numbers, anyone who has been frustrated by `nan` or the fact that `0.3 == 0.1 + 0.2 => False`, and anyone who wants to be the life of their next party.

This talk will _not_ cover more complicated numerical methods for, ex, ensuring that algorithms are floating-point safe. Also, if you're already familiar with the significance of ""52"" and the term ""mantissa"", this talk might be more entertaining than it will be educational for you.",python,2019
"Object Relational Mappers (ORMs) are awesome enhancers of developer productivity. The freedom of having the library write that SQL and give you back a useful, rich model instance (or a bunch of them) instead of just a tuple or a list of records is simply amazing.

But if you forget you have an actual database behind all that convenience, then it'll bite you back, usually when you've been in production for a while, after you've accumulated enough data that your once speedy application starts slowing down do a crawl.

Databases work best when you ask them once for (or to do) a bunch of stuff, instead of asking them lots of times for small stuff.

We'll discuss how innocent looking attribute accesses on your model instances translate to sequential queries (the infamous [N+1 problem][1]).

Then we'll go through some practical solutions, taken from real cases that resulted in massive speed ups. We'll cover how changes in Python code resulted in changes to the resulting SQL Queries

Solutions not only for queries, but also inserts and updates, which tend to be less well documented.

Though this talk focuses on SQLAlchemy, the lessons should be applicable to most ORMs in most programing languages. The ideas discussed, and solutions proposed are also valid for any storage backend, not only SQL databases. 

  [1]: https://docs.sqlalchemy.org/en/latest/glossary.html#term-n-plus-one-problem
",python,2019
"*How do you know if your data science results are correct?* Robust software usually has tests asserting that certain conditions hold, but as a data scientist it’s often not straightforward or obvious how to integrate these best practices. Our workflow includes exploration, statistical models, and one-off analysis. This talk will give concrete examples of when and how testing should play a role, and provide you with enough introduction to get started writing your first data science tests using `pytest` & `hypothesis`.",python,2019
"Deep learning is a useful tool for problems in computer vision, natural language processing, and medicine. While it might seem difficult to get started in deep learning, Python libraries, such as Keras make deep learning quite accessible. In this talk, we will discuss what deep learning is, introduce NumPy and Keras, and discuss common mistakes and debugging strategies. Throughout the talk, we will return to an example project in the medical domain, which used deep learning on vocal data to determine whether a patient has a voice disorder called vocal hyperfunction. ",python,2019
"Dropbox is a heavy user of the mypy type checker, recently passing three million lines of type-annotated Python code, with over half of that added in 2018. Type checking is helping find bugs, making code easier to under stand, enabling refactors, and is an important aid to our ongoing Python 3 migration.

In this talk, we discuss how we got there. We’ll talk about what we tried in order to get our engineers to type annotate their code—what worked, what didn’t, and what our engineers had to say about it. 

Additionally, we’ll discuss the performance problems we faced as the size of our checked codebase grew, and the techniques we employed to allow mypy—which is implemented in Python—to efficiently check (faster than a second, for most incremental checks) millions of lines of code, which culminated in mypyc, a new ahead-of-time compiler for type-annotated Python!",python,2019
"At Dropbox, we’ve always used Python to power our application for Windows, macOS and Linux (until recently, Python 2.7). Over the years, a growing lack of features and the need for outdated compilers/toolchains made migrating to Python 3 a necessity. Join us to hear the tale of our unique journey from Python 2 to 3 and the lessons we learned along the way:

- We’ll discuss the reasons that led to our decision to make the jump.
- We’ll dive into how we sequenced the transition by using the C-API to ship both versions of Python and choose one at runtime.
- We’ll reveal the tools we used to enforce a hybrid (2/3) syntax for over hundreds of thousands of lines of Python code.
- We’ll discuss some of our most spectacular bugs and gotchas, and how you can avoid them!",python,2019
"After attending your local dev meetup for months, you suddenly get the dreaded email: ""Your Organizer just stepped down without nominating a replacement.""

But the community relies on this meetup! It brings together devs from all around to engage in networking, learning, and comradery! So you step up. I mean, how hard could it be, right?

Oh no. This is much harder than you thought. You have to organize a venue, figure out refreshments, get a speaker, ensure people show up. 

In this talk,  you'll learn the skills need to start and sustain a vibrant meetup and tech community.",python,2019
"Diagnosing a patient requires consideration of a wide variety of factors: past medical history and comorbidities, physical exam findings, lab results, imaging, ECG findings, and in some cases, genomic testing. Clinical diagnosis and prognostic assessment currently relies on expert knowledge of the treating physician. Recent developments in machine learning make it possible to build automated clinical diagnostic and risk assessment tools using data from the electronic medical record. 

This talk walks through the steps involved in building a clinical risk assessment model, using sepsis as a case study. A large part of the talk will focus on the tools and techniques involved in pre-processing complex medical data, and strategies for evaluating model results.",python,2019
"The CPython project is now 28 years old. It has active core developers, but almost all of them are volunteers. It's difficult to ask someone to be committed into a project for 5 years without being paid. Helping newcomers and mentoring contributors takes time and few developers are available for that. We are working on improving the diversity of CPython core developers and get more active core developers, but it's a slow process.",python,2019
"Have you ever wondered how a JIT compiler works?  Production quality JIT compilers are large, complicated pieces of software that can seem inscrutable at first glance. However, building a simple JIT compiler is surprisingly easy. We'll walk through how to build a template-style JIT compiler for Python from first principles, in Python!",python,2019
"The Python world has a staggering array of data visualization tools, and choosing which to use can seem like a daunting task. But which tool you use is far less important than how you use it. In this talk I’ll walk through some of the important considerations involved in visualizing your data, so that you can create more effective visualizations no matter which plotting package you use.",python,2019
"Serverless computing is all about paying only for what you use: it can scale up to handle millions of requests, but it can also scale down to 0, costing you nothing if your application is not receiving any traffic.

Serverless tends to get expensive when databases are involved.... but if your data is static or changes infrequently, you can use serverless tools to provide powerful interactive APIs extremely cheaply.

[Datasette](https://datasette.readthedocs.io/) is an open-source Python tool that provides an instant, read-only JSON API for any SQLite database. It also provides tools for packaging the database up as a Docker container and instantly deploying that container to a number of different serverless hosting platforms.

This makes it a powerful tool for sharing interesting data online, in a way that allows users to both explore that data themselves and build their own interpretations of the data using the Datasette JSON API.

In this session I'll show you how to use Datasette to publish data, and illustrate examples of the exciting things people have already built using the tool - including a number of real-world data journalism projects.

I'll also teach people how to use some of the other tools in the Datasette ecosystem:

* [Datasette Publish](https://publish.datasettes.com/), which allows CSV data to be published using Datasette to a serverless hosting account owned by the user, without any engineering experience required.
* [csvs-to-sqlite](https://pypi.org/project/csvs-to-sqlite/), a tool for efficiently converting large numbers of CSV files into a Datasette-compatible SQLite database.
* [sqlite-utils](https://sqlite-utils.readthedocs.io/), a library that lets users create complex databases from custom data feeds in just a few lines of Python code (ideal for working with Jupyter notebooks).

I'll discuss the philosophy and design behind Datasette, including how immutable SQLite databases make for an impressively scalable solution for inexpensively serving complex data on the internet. Finally, I'll be exploring how Datasette takes advantage of Python 3 asyncio and the new ASGI specification.",python,2019
"&nbsp;&nbsp;&nbsp;&nbsp;Feature flags can be powerful tools in mitigating risk in your development cycle — _if you use them correctly_. Failing to do so can have enormous consequences for yourself and your business. In 2012 one improperly deployed feature flag sent a $365 million company into bankruptcy in 45 minutes. So let’s talk about feature flags, specifically in how they can help us with intentional deployment. Feature flags give us a high degree of control over the features we release — but what ensures we have a high degree of control over our feature flags? 

&nbsp;&nbsp;&nbsp;&nbsp;In this talk, I’ll go over the best practices which will make your feature flagging program a success. The humble Feature Flag can transform into many different things: release toggle, experiment, kill switch, permissioning and more. I’ll talk briefly about the possibilities Feature Flags open up, and then describe how to use best practices of visibility and accountability to align those different flags into a cohesive feature flagging system.

&nbsp;&nbsp;&nbsp;&nbsp;After this talk, you’ll know what best practices make a successful feature flagging program, and be able to implement them into your current solution to deploy faster and with less risk.

",python,2019
"Starting a few years ago, Capital One has committed to go all-in on public cloud and open source software for many of our core business operations, processes, and machine learning models. To support this transformation, we embarked on a multi-year journey to build a Python community with critical mass of users, and scale adoption of Python in our business analyst and data analyst workforces.

Python has been envisioned since its early days as a programming language which can be used to ""create better, easier to use tools for program development and analysis"", as well as ""build a user community around all of the above, encouraging feedback and self-help"". [1] In our experience scaling Python adoption amongst analyst communities within a Fortune 500 company, we have found the aforementioned visions true to form - not only is Python a great first programming language for our analysts to learn, it also comes with ""batteries included"" and contains many of the data-related tools and libraries which allows our analysts to get productive very quickly.

This talk will highlight our multi-pronged approaches to overcome organizational inertia to build a community of Python users, provide Python and OSS training, and encourage Python adoption (with mixed success). We'll share what (we think) best practices are out there, and lessons learned along the way.

Reference:
[1] Computer Programming for Everybody (http://www.python.org/doc/essays/cp4e.html)
",python,2019
"Application security remains a long-term and high-stakes problem for most projects that interact with external users. Python's type system is already widely used for readability, refactoring, and bug detection — this talk will demonstrate how types can also be leveraged to make your project systematically more secure. We'll investigate (1) how static type checkers like Pyre or MyPy can be extended with simple library modifications to catch vulnerable patterns, and (2) how deeper type-based static analysis can reliably flag remaining use cases to security engineers. As an example, I'll focus on a basic security problem and how you might use both tools in combination, drawing from our experience deploying these methods to build more secure applications at Facebook and Instagram.",python,2019
"What good is a code style if it's not internally consistent? What good is a linter when it slows you down? What if you could out-source your worries about code formatting, adopt a consistent style, and make your team faster all at the same time?

Come hear about Black: a new code style and a tool that allows you to format your Python code automatically. In the talk you'll learn not only how the style looks like but why it is the way it is. I will do my best to convince you not only that it's good but that it's *good enough*. You'll see how you can integrate it with your current workflow and how it speeds up your life while making your code prettier on average.

Lose your attachments, delegate the boring job of moving tokens around to satisfy the linter, and save time for more important matters. Guaranteed to increase the life expectancy of space bars and Enter keys on your new MacBook's keyboard.",python,2019
"When it takes hours or days to run your computation, it can take a long time before you notice something has gone wrong, so your feedback cycle for fixes can be very slow. If you want to solve problems quickly—whether it's inconsistent results, crashes, or slowness—you need to understand what was going on in your process as it was running: you need logging.

In this talk you'll learn how to use the Eliot logging library to create causal traces of your computation, including intermediate inputs and outputs. You'll then see how these traces can help you:

* Ensure the computation ran as expected.
* Debug problems that may have occurred far along within your computation.
* Identify and localize performance problems: which parts of your computation were slow, and what arguments were involved.",python,2019
"Failure can be scary. There are real costs to a company and its users when software crashes, models are inaccurate, or when systems go down. The emotional stakes feel high-- no one wants to be responsible for a failure. We can lower the stakes by creating spaces to learn from failures, and minimize their impact. This talk introduces two ways to address failure: blameless post-mortems, to learn from an incident; and pre-mortems, to identify modes of failure upfront.",python,2019
"Python is a prevalent programming language in machine learning (ML) community. A lot of Python engineers and data scientists feel the lack of engineering practices like versioning large datasets and ML models, and the lack of reproducibility. This lack is particularly acute for engineers who just moved to ML space.

We will discuss the current practices of organizing ML projects using traditional open-source toolset like Git and Git-LFS as well as this toolset limitation. Thereby motivation for developing new ML specific version control systems will be explained.

Data Version Control or [DVC.ORG][1] is an [open source][2], command-line tool written in Python. We will show how to version datasets with dozens of gigabytes of data and version ML models, how to use your favorite cloud storage (S3, GCS, or bare metal SSH server) as a data file backend and how to embrace the best engineering practices in your ML projects.

[1]: http://dvc.org
[2]: https://github.com/iterative/dvc
",python,2019
"PyPI is a gold mine of great packages but those packages have to be written first.  More often than not, projects that millions of people depend on are written and maintained by only one person.  If you’re unlucky, that person is you!

So how do you square delivering a *high quality* Python package you can be proud of and having only limited time at your disposal?  The answer is not “try harder,” the answer is to **do less**.

This talk will help you get there by talking about how you can make your life easier, remove causes of friction with your contributors, and empower said contributors to take over tasks that you can’t make time for anymore.",python,2019
"Learn how to make music with Python, SuperCollider and FoxDot. We'll create a song together in this live coded adventure. ",python,2019
"Within the last few years, researchers have come to understand that machine learning systems may display discriminatory behavior with regards to certain protected characteristics, such as gender or race. To combat these harmful behaviors, we have created multiple definitions of fairness to enable equity in machine learning algorithms. In this talk, I will cover these different definitions of algorithmic fairness and discuss both the strengths and limitations of these formalizations.  In addition, I will cover other best practices to better mitigate the  unintended bias of data products.",python,2019
"When machine learning models make decisions that affect people’s lives, how can you be sure those decisions are fair? When you build a machine learning product, how can you be sure your product isn't biased? What does it even mean for an algorithm to be ‘fair’? As machine learning becomes more prevalent in socially impactful domains like policing, lending, and education these questions take on a new urgency.

In this talk I’ll introduce several common metrics which measure the fairness of model predictions. Next I’ll relate these metrics to different notions of fairness and show how the context in which a model or product is used determines which metrics (if any) are applicable. To illustrate this context-dependence I'll describe a case study of anonymized real-world data. Next, I'll highlight some open source tools in the Python ecosystem which address model fairness. Finally, I'll conclude by arguing that if your job involves building these kinds models or products then it is your responsibility to think about the answers to these questions.",python,2019
"Over the course of nearly a year, we migrated Pinterest's primary systems from Python2 to Python3.  A large, tightly coupled codebase with over 2 million lines of code, the Pinterest codebase contained nearly every edge case that might exist in a Py2 to Py3 migration.

We'll cover our approach, gotchas, and tools, and the incredible impact our migration has made on infra spend and code quality.",python,2019
"Mocking and patching are powerful techniques for testing, but they can be easily abused, with negative effects on code quality, maintenance, and application architecture.  These pain-points can be hard to verbalize, and consequently hard to address.  If your unit tests are a PITA, but you cannot explain why, this talk may be for you.

Mocking as a technique has deep roots within OOD and TDD, going back 20+ years, but many Python developers know mocks and patches merely as a technique to isolate code under test.  In the absence of knowledge around OOD and TDD, best practices around mocking are completely unknown, misunderstood, or ignored.   Developers who use mocks and patches without doing TDD or OOD are susceptible to falling into many well-understood and documented traps.

This talk will draw a historical connection between the way mocks are taught today, and their origins in TDD, OOD, and Java.  It will also demonstrate some pitfalls, and provide some guidance and alternatives to mocking and patching (e.g., dependency injection, test doubles, functional style).",python,2019
"Every programmer should learn to use solvers, tools that reason directly from a description of a problem to its solution.  

Tools like AlphaZero can formulate winning strategies for games given only a description of the rules of the game.  For certain classes of problems, we really can just let the computer do the work.

In this talk, we learn principles, techniques, and multiple examples for three solvers available in Python.

The first tool is a generic puzzle-solving framework that employs tree search strategies.  We apply it to a simple sequencing problem and then to a harder sliding-block puzzle. Next, we'll look at the solver code to learn how it works.  I'll also show an essential optimization technique and how to humanize the output.  We demonstrate our skills by solving another famous puzzle.

The second tool is called a SAT solver. It is one of the miracles of the 21st century. From first principles, I'll show you what problems it solves and the way problems need to be described for modules like *PycoSAT*.  I'll provide helper functions to humanize our interactions with this great tool.  Then, we'll demonstrate our skills by creating a Sudoku solver and a readable logic problem solver.

The third tool is the ""multi-armed bandit"". It is a generic reinforcement learning algorithm that is easy to learn, powerful, and applicable to a broad class of problems.  We apply it to winning rock-paper-scissors using pattern recognition.

Lastly, I'll summarize DeepMind's paper on AlphaZero which was published in the December 2018 edition of *Science*.  This gives us hints at the full potential of these techniques.  

Pure Python source code and examples are provided for all of the tools.",python,2019
"&lt;announcer> This one quick trick will help you measure the diversity of your hiring pipeline!  Read on to hear how! &lt;/announcer>

One challenge in improving diversity within a hiring pipeline is the struggle to measure what exists in the first place. It's hard to know where to focus your resources until you know what you have to work with, and can identify what steps make a difference and what efforts don't. Using python/django and an api key for our recruiting vendor, you can make this information visible and, therefore, actionable, with very little work.",python,2019
"In this talk, we'll learn about a highly controversial change to Python syntax, the rationale for it, and the fallout as the result of it.

Along the way we'll go in-depth on how new ideas about Python are proposed, discussed, and become part of the language, what's unique about Python's process compared to other open source projects, and recent developments and what they mean for the future of the wider Python community.",python,2019
"Maps are powerful tools that we use every day. Python is well-equipped to handle spatial data and with well documented robust libraries to help you perform spatial analysis and create beautiful maps. In this talk, we'll discover the fascinating world of spatial analysis by solving a fun problem: where can we go to see an upcoming solar eclipse? Along the way we'll learn about mapping topics like projections and coordinate systems, best practices for map making, and intricacies of spatial data.",python,2019
"Python is a flexible language. Your Python app, on the other hand, is usually more set in stone: buttons, functions, displays are all explicitly defined.

In this talk you'll learn how to take advantage of features like decorators and functions as first-class objects to set up a simple plugin system that allows your app to be more flexible. In fact, you can allow your users to add or customize functionality they want after you ship.

By using plugins, your code becomes more modular and maintainable. At the same time your users may be able to use your great app to work with data or challenges you didn't even know existed.",python,2019
"Digital Signal Processing and Player Piano don't normally come together in the same sentance. Player Pianos that are 100+ years old are awesome artisan artifacts, but they don't play digital formats very well. This talk will show how we take a 100+ year old technology and marry it to the digital age via Python libraries and precision lasers! 

In this discussion we will cover how we are creating our own ""Plug-n-Stream Player Piano"". We will take a look at the different digital signal processing Python libraries, their functionality, and requirements for converting audio streams to piano playable audio files. After a brief walk through of our prototyped hardware, we will dissect the digital signal processing, converting streaming music to data for the Player Piano. With a real Player Piano in the room we will demo streaming music from our devices onto the piano. 

LIVE(ish) Piano Playing!",python,2019
"Decorators are one of Python's most powerful features.  But even if you understand what they do, it's not always obvious what you can do with them.  Sure, from a practical perspective, they let you remove repeated code from your callables.  And semantically, they let you think at a higher level of abstraction, applying the same treatment to functions and classes.

But what can you actually do with them?  For many Python developers I've encountered, ecorators sometimes appear to be a solution looking for a problem.

In this talk, I'll show you some practical uses for decorators, and how you can use them to make your code more readable and maintainable, while also providing more semantic power.  Moreover, you'll see examples of things would be hard to do without decorators.  I hope that after this talk, you'll have a good sense of how to use decorators in your own Python projects.",python,2019
"Notebooks have traditionally been a tool for drafting code and avoiding repeated expensive computations while exploring solutions. However, with new tools like nteract's papermill and scrapbook libraries, this technology has been expanded to make a reusable and parameterizable template for execution.

We'll walk though how Jupyter notebooks are being programmatically used at Netflix and how this helps with our batch processing world. We'll also explore how these use cases connect back with users and why we've adopted these tools for Python and non-Python execution.",python,2019
"The deep learning hype is real, and the Python ecosystem makes it easier than ever to neural networks to everything from speech recognition to generating memes. But when picking a model architecture to apply to your work, you should consider more than just state of the art results from NeurIPS. The amount of time, money and data available to you are equally, if not more, important. This talk will cover some alternatives to deep learning, including regression, tree-based methods and distance based methods. More importantly, it will include a frank discussion of the pros and cons of different methods and when it makes sense to use each in practice.",python,2019
"Packages that won't install, encodings that don't work, installers that ask too many questions, and having to own a PC are all great reasons to just ignore Windows. Or they would be, if they were true.

Despite community perception, more than half of Python usage is on Windows, including web development, system administration, and data science, just like on Linux and Mac. And for the most part, Python works the same regardless of what operating system you happen to be using. Still, many library developers will unnecessarily exclude half of their potential audience by not even attempting to be compatible.

This session will walk through the things to be aware of when creating cross-platform libraries. From simple things like using `pathlib` rather than `bytes`, through to all the ways you can get builds and tests running on Windows for free, by the end of this session you will have a checklist of easy tasks for your project that will really enable the whole Python world to benefit from your work.",python,2019
"While high-level security concepts may transcend languages, each language has its own sets of tools and edge cases that are worth knowing.  Python is one of many popular languages that is rarely the focus in security training, but that doesn't mean python code is automatically secure (no matter what the internet tells you).  Learn why people who say “pylint will help you with security” aren’t doing you any favours, how to use Bandit for security-focused linting and talk about other options for static analysis.  Take a deeper look at why scanning for publicly known vulnerabilities is complicated, and how to use Pyup Safety to make it easier.  We’ll also explore some language myths and best practices.",python,2019
"Being able to release rapidly and continuously allows businesses to react to opportunities, shorten feedback loop for product iteration cycle and reduce debug effort for erroneous changes. At Instagram, we operate the world's largest fleet of servers running on Python and we continuously deploy every X minutes. Anyone can do it, this talk will teach you the practical steps and talk about the ideas and problems we faced at every phase of our automation journey.",python,2019
"In the month of August 2018, Kerala, the southernmost state of India, received 250 % of normal rainfall, resulting in all of its 44 dams to be opened. Over 483 people died due to the flooding caused by the opening of dams and a million people were evacuated. 

I started a website ([keralarescue.in][1]), written in Django. The main purpose of the site was effective collaboration and communication between authorities, volunteers and public. The site was open source from Day 0. About 1500 developers and volunteers onboard our slack group in a couple of days. Within a week, the community united to forge a critical piece of software that saved thousands of lives.

The site initiated as a portal for refugees to request essential resources like food and water and for volunteers to see their needs, all sorted by geographical location. Additionally, we provided direct information for the government and became the official website later on.

The Minimum Viable Product was delivered in fourteen hours. In the initial days, it was only used by the volunteers and Point of Contacts assigned by the government. Later, when the situation became critical, we started getting rescue requests from stranded refugees. The Github repo of the website went viral, and we started to receive feature requests rapidly. We received more than five hundred pull requests in the span of three weeks. 

The story I want to present is about the community and technical aspects of keralarescue.in, how people from different backgrounds came together to build a critical piece of software that saved many lives.

  [1]: https://keralarescue.in
",python,2019
"I love Pokemon. However, I don't love how some players make the community less welcoming towards beginners by hiding their strategies. So I did what any defiant engineer would. I signed up for a free AWS account and began (responsibly) scraping millions of their unauthenticated Pokemon battles.

We'll journey together through this passion project of mine and draw on specific examples to better understand the trade-offs of working with distributed systems or microservice architectures in the cloud.",python,2019
"Key takeaways:

1. Set operations enable simpler and faster solutions for many tasks;
1. Python's set classes are lessons in elegant, idiomatic API design;
1. A set class is a suitable context for implementing operator overloading.

Boolean logic and set theory are closely related. In practice, we will see cases where set operations provide simple and fast declarative solutions to programming problems that otherwise require complicated and slow procedural coding.

Python's set built-ins and ABCs provide a rich and well designed API. We will consider their interfaces, and how they can inspire the creation of Pythonic APIs for your own classes.

Finally, we will discuss operator overloading — a technique that is not suitable everywhere, but certainly makes sense with sets. Taking a few operators as examples, we will study their implementation in a new `UintSet` class for integer elements. `UintSet` fully implements the `MutableSet` interface over a totally different internal representation based on a bit array instead of a hash table. Membership tests run in _O(1)_ time like the built-in sets (however, `UintSet` is currently pure Python, so YMMV). Using bit arrays allow core set operations like intersection and union to be implemented with fast bitwise operators, and provides compact storage for dense sets of integers.
",python,2019
One of the best things about Python is the vast ecosystem of packages available on the Python Package Index. Shipping your first Python package can be intimidating. This talk aims to remove the mystery of Python packaging and enable you to share your code with other Pythonistas. We will also discuss more recent additions to packaging that can be used even if you are an experienced package maintainer. The latter portion of the talk will be dedicated to talking about how you can automate releasing via CI to make future updates a breeze.,python,2019
"Profiling involves computing a set of data about how often and how
long various parts of your program are executed. Profiling is useful
to understand what makes your program slow and how you can improve
it. After a quick review of deterministic profiling tools and
techniques, I will describe how you can do statistical profiling with
existing packages or write your own from scratch.

Statistical profiling involves occasionally sampling what your program
is doing instead of watching each line or function. A key feature of
statistical profiling is that by using a moderate sampling frequency,
you can profile your production code with almost no overhead. This lets
you find the actual bottlenecks in real use cases.

The core technical focus of the talk is python's sys module and how it
lets you easily examine a running program. I also describe some tricks
to be aware of related to threading, context switches, locks, and so on.
At the conclusion of the talk, you will hopefully understand how to use
an existing statistical profiler or write a customized version yourself.
",python,2019
"Testing code is important. Testing, primarily unit-testing async code
requires heading off the the standard roadway of unit testing in python. This talk will provide a map to help you along the new path towards testing async code.

Topics include:

 - a brief intro to `asyncio` and challengs in testing with it
 - running coroutines (and other awaitables) under test
 - mocking coroutines
 - testing ""main"" `asyncio` loops",python,2019
"People live with mental health stigma because we learn that we're supposed to be strong and resilient. It's okay not to be strong or resilient all the time. Discussing mental illness is uncomfortable. In this talk, I will help you overcome that discomfort by examining the most common mental health issues, how you can get help for yourself, and how you can best support your coworkers, friends, and family. No one should have to deal with mental illness alone. Bring your tissues.
",python,2019
"Manually updating a million line code base is tedious. Thankfully syntax trees provide a safe and quick way to  automatically apply repetitive transformations. Leveraging syntax tree based tooling (based on lib2to3), has been a critical component of Pinterest's Python 3 upgrade strategy, and saved us countless hours of work.  Learn how syntax trees work, how they are used to transform code, and how you can quickly write your own transformations.",python,2019
"GraphQL is an exciting technology that can help simplify web logic. Most of the attention has been focused on client-side improvements, such as reducing payload sizes and reducing total number of requests. This talk will show how GraphQL can structure your backend logic to reduce the client-side dependencies or remove them entirely!",python,2019
"Seeing the Earth from above is truly breathtaking, but it takes a lot of time, fuel and opportunity - so instead, why not make miniature art of the world's famous terrains?

This talk explores using Python to take raw terrain data - from aerial lidar and space-based radar scans - and processing it into 3D models, and CAD/CAM toolpaths, with the ultimate result of making Python-powered artwork of some of Earth's natural wonders.

See how to reduce each National Park to a small, intricately-milled metal carving, how to laser-cut a side-on relief of a whole Hawaiian island, or how to 3D print tiny versions of cities where you can make out each individual building - and the strengths and challenges of using Python to handle 3D and GIS data.

We'll also look at some basic 3D modelling code, discuss the wonders of different map projections, and how personal LiDAR is slowly, but surely, becoming affordable.",python,2019
"If you’ve ever `pip install`ed a Python package with C extensions on Linux, it was probably a painful experience, having to download and install development headers for libraries you’ve never even heard of. Maybe you’ve given up on pip and have switched to Conda. But it doesn’t have to be this way! The Python Packaging Authority has been working hard to solve this problem with a new distribution format for compiled Python code, called “wheels.”

In this talk, we’ll descend into the practice of PEPs 513 and 571: arcane scrolls that can equip Python developers with spells to pre-compile applications and libraries in a way that allows most Linux end users to run them directly. I’ll show you how to hex compiled artifacts and source code into the wheel format, harness application binary interfaces (ABIs) to use external libraries, brave the eldritch horrors of the dynamic linker, and bind these all together in the manylinux environment. Come learn to harness the black magic of Python wheels, and you too can spare your users pain… for a price.",python,2019
"For the past two years, an Open Source social network has been building on the foundations laid by years of Open Web work and quietly growing to provide a viable alternative to the social network megacorps we’ve grown used to. It’s name is Mastodon, and it wants you to be part of the community.

In this talk, we’ll learn what Mastodon is, how it works, how you can participate, and how you can interact with the community using Python. Along the way, we’ll learn about OAuth, APIs, and best practices for writing bots. Attendees will walk away with the knowledge of how to build a Mastodon bot.
",python,2019
"Inheritance is among the first concepts we learn when studying object-oriented programming. But inheritance comes with some unhappy strings attached. Inheritance, by its very nature, tends to bind a subclass to its superclass. This means that modifying the behavior of a superclass might alter the behavior of all its subclasses, sometimes in unanticipated ways. Furthermore, it’s commonly accepted that inheritance actually breaks encapsulation. So, if inheritance has these issues, what alternative do we have? More than two decades ago, The Gang of Four (Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides) suggested in their famous _Design Patterns_ book  that we should favor object composition over class inheritance.

In this talk I will show some code examples in Python where inheritance goes astray and demonstrate how to correct them by using composition. My intention is not to demonize inheritance, but instead present how to use it wisely in order to improve the design of our object-oriented software so that it’s more flexible and easier to maintain.",python,2019
"Many developers struggle to find the balance between striving to improve existing code and letting good enough alone by accepting certain shortcomings. As a new developer to a team it can be difficult to understand existing strategies and patterns that are sometimes flat out bad (and often openly acknowledged as such). Often the result of tight deadlines or unclear specifications, even the best developers write code they later look back upon with shudders. So how do we decide when refactoring is worth it? Come learn strategies for refactoring with minimal impact, methods for working with bad code you can’t change, and strategies for knowing the difference between what is fixable and what is better left alone. ",python,2019
"The Zen of Python, accessed by running `import this`, is a list of nineteen aphorisms that have guided the development of the language. It has good advice for how to organize our code, but what does it have to say about how we organize ourselves? Plenty: the Zen of Python is not only a solid set of development principles, but the other easter egg is that it’s packed with wisdom about how to build healthy teams. 

In this talk I draw upon my time as an engineering manager of Python-focused engineering teams to tell stories of what the Zen of Python has to teach us about communication and conflict, building inclusive teams and transparent processes, and promoting psychological safety. Come ready to reflect on and feel inspired by a new interpretation of these principles, and bring what you learn back to your meetup, study group, open source project, or team. ",python,2019
"If you haven't tried multiprocessing or you are trying to move beyond `multiprocessing.map()`, you will likely find that using Python's `multiprocessing` module can get quite intricate and convoluted.  This talk focuses on a few techniques (starting, shutting down, data flow, blocking, etc) that will maximize `multiprocessing`’s efficiency, while also helping you through the complex issues related to coordinating startup and _especially_ shutdown of your multiprocess app.

",python,2019
"We will cover how we used Python to adapt to a large institutional processing setup.  We used Python to create the definitions, configuration files, and supplementary metadata for each of the weather radars we worked with.  We used a variety of custom tools to interface with existing systems and processes that would have been infeasible to work with otherwise.

We took advantage of one of Python’s greatest strengths: its flexibility.  We used it to perform the bulk of our data processing with NumPy, created custom utility functions to encourage code reuse, and created custom scripts for interfacing with the institutional data processing framework we worked within.
",python,2019
"Using the pandas python library requires a shift in thinking that is not always intuitive to those who use it. This talk will take a deep dive into the underlying data structure of pandas to explain why it performs the way it does under certain circumstances. It will explain why a MultiIndex DataFrame takes up less memory than it's simple counter part, why groupby should never be run on a non-MultiIndexed DataFrame, why the example documentation for the pandas apply function is an example of how not to use it, and how not taking the time to normalize data can affect performance.  
",python,2019
"Having libraries in your Python project properly locked to a specific version is a well known best practice. Dependency management tools in the Python ecosystem lock dependencies to the latest version available, but what if the latest version available is not the best fit for your application? Open source project Thoth is an advanced Python dependency resolver which recommends libraries for your project based on observations that are gathered for Python libraries for specific runtime environments. How these recommendations look like? How are different observations like performance characteristics of machine learning libraries for a particular hardware gathered?",python,2019
"One of the reasons why programming in Python is very straightforward and simple is that we do not have to worry about the lifetime of our objects. That is, once it ceases to be necessary, a variable disappears from the memory ""magically"". The fact that this happens automatically can erroneously lead us to believe that it is not required to worry about what happens behind the scenes.

Nothing is further from reality: knowing how Python manages memory is fundamental in specific scenarios, and not knowing what is happening can have consequences as significant as unpleasant. For example, if our programs manage a large amount of data at the same time or launch multiple processes in parallel, this ceases to be a theoretical issue and becomes something that we, logical minds, also care.

Although these concepts tend to be considered advanced and difficult to understand, we will see that this is not the case. This topic is not a purely theoretical matter nor is it difficult to find its practical applications. In this talk, we will explain why it is something that should matter to us, and we will talk about how to apply the knowledge we have gained to specific problems.",python,2019
"Why come to yet another talk about CPython's GIL? [1]  Sure, we'll spend a little time on what it is, who it affects (and doesn't), and how to work around it.  However, what you want to come hear is what the future holds for the GIL.

We'll take most of the time talking about life *after* the GIL!  Come see what recent developments and ongoing work will allow us to either circumvent the GIL and get rid of it, unlocking true multi-core capability in Python code.

[1] In case you don't know, the GIL is a global lock that prevents multi-core parallelism in pure Python code.  It has a controversial place in the community.  Look it up (or come to this talk)!",python,2019
"Type hinting for Python (as a linter tool) came out in September 2015 as part of Python 3.5 (and was championed by Guido himself). Since then, variable annotations (plus, more recently, protocols) improved its capabilities even further. Over the last two years, tools, such as mypy, could build on top of it. Slowly, these annotations have emerged from a proof of concept state (e.g., mypy API planning) to becoming a stable feature.

In this presentation, I'll tell my story of using type hints for both adding type hinting and checking type correctness for a library supporting both Python 2 and 3 and reusing this information to insert type data into the generated Sphinx documentation automatically.",python,2019
"Knowing your enemies is as important as knowing your friends. Understanding your debugger is a little of both. Have you ever wondered how Python debugging looks on the inside?
On our journey to building a Python debugger, we learned a lot about its internals, quirks and more.

During this session, we’ll share how debugging actually works in Python.
We’ll discuss the differences between CPython and PyPy interpreters, explain the underlying debugging mechanism and show you how to utilize this knowledge at work and up your watercooler talk game.",python,2019
"Walk into any factory and you will see a Programmable Logic Controller (PLC). It's the small box that has a memory card and an Ethernet cable on one side, and lots of colorful wires connected to the other end. Inside runs the logic that turns inputs from sensors into outputs to robots, conveyor belts and other machinery. PLCs evolved from relay banks in the 1970s and have ruled the world of industrial automation since then.

In the first half of this talk we will take a look at how they work, how to program them, and why a strange language called ""ladder logic"" is (still) the lingua franca for programming them. In a short on-stage demo I will write some PLC code to control a device on stage.

It's 2019 now and just running a PLC isn't quite enough anymore. Everyone is talking about the ""Industrial Internet of Things"" and they have connected their PLCs to the company network. The second half of the talk will look at how we can connect to PLCs to read data and influence the running program with Python. ",python,2019
"Everyone starts with the best intentions with their Python projects, ""this time it's going to be clean, simple and maintainable"". But code evolves over time, requirements change and codebases can get messy and complicated quickly.

In this talk, you will learn how to use `wily` to measure and graph how complicated your Python code is and a series of practical techniques to simplify it. `wily` will show you which parts of your projects are becoming or have become hard to maintain and need a refactor. Once you know where the skeletons are, you will learn practical techniques for refactoring ""complex"" code and some resources to use to take your refactoring to the next level.",python,2019
"Time zones are complicated, but they are a fact of engineering life. Time zones have [skipped entire days](http://www.bbc.com/news/world-asia-16351377) and repeated others. There are time zones that switch to [DST twice per year](https://www.timeanddate.com/time/zone/morocco/casablanca). But not necessarily every year.  In Python it's even possible to create datetimes with non-transitive equality (`a == b`, `b == c`, `a != c`).

In this talk you'll learn about Python's time zone model and other concepts critical to avoiding datetime troubles. Using `dateutil` and `pytz` as examples, this talk covers how to deal with ambiguous and imaginary times, datetime arithmetic around a Daylight Savings Time transition, and datetime's new `fold` attribute, introduced in Python 3.6 ([PEP 495](https://www.python.org/dev/peps/pep-0495/)).
",python,2019
"At some point every Python programmer sees Python bytecode files -- they're those '.pyc' files Python likes to leave behind after it runs. But have you ever wondered what's really going on in those files? Well, wonder no more! In this talk you'll learn what Python bytecode is and how it's used to execute your code, as well as how to decipher and read it, and how to reason about bytecode to understand the performance of your Python code.",python,2018
"Until very recently, Apache Spark has been a de facto standard choice of a framework for batch data processing. For Python developers, diving into Spark is challenging, because it requires learning the Java infrastructure, memory management, configuration management. The multiple layers of indirection also make it harder to debug things, especially when throwing the Pyspark wrapper into the equation.

With Dask emerging as a pure Python framework for parallel computing, Python developers might be looking at it with new hope, wondering if it might work for them in place of Spark. In this talk, I’m using a data aggregation example to highlight the important differences between the two frameworks, and make it clear how involved the switch may be.

Note: Just in case it's unclear, there's no Java of any kind in this talk. All the code / examples use Python (PySpark).
",python,2018
"In this talk, you’ll learn about a category of security issue known as side channel attacks. You’ll be amused to see how features like automatic data compression, short-circuit execution, and deterministic hashing can be abused to bypass security systems. No security background knowledge is required. The talk assumes at least intermediate Python experience.

We’ll take a tour of real side channel vulnerabilities in open source Python codebases, including the patches that fixed them. It also offers practical advice for avoiding these issues. My goal is to demystify this topic, even if you aren’t writing security-critical software.
",python,2018
"“So tell me,” my manager said, “what is an average?”

There’s probably nothing worse than that sinking feeling when you finish an analysis, email it to your manager or client to review, and they point out a mistake so basic you can’t even fathom how you missed it. 

This talk is about mine: how to take an average.

Averages are something we use everywhere - it’s a simple np.mean() in pandas or AVG() in SQL. But recently I’ve come to appreciate just how easy it is to calculate this statistic incorrectly. We learn once - in middle school no less - how to take an average, and never revisit it. Then, when we are faced with multidimensional datasets (ie. pretty much every dataset out there), we never reconsider whether we should be taking an average the same way.

In this talk, we follow my arduous and humbling journey of learning how to properly take an average with multidimensional data. We will cover how improperly calculating it can produce grossly incorrect figures, which can slip into publications, research analyses and management reports.",python,2018
"Recommender systems have become increasingly popular in recent years, and are used by some of the largest websites in the world to predict the likelihood of a user taking an action on an item. In the world of Netflix, this means recommending similar movies to the ones you have seen. In the world of dating, this means suggesting matches similar to people you already showed interest in!

My path to recommenders has been an unusual one: from a Software Engineer to working on matching algorithms at a dating company, with a little background on machine learning. With my knowledge of Python and the use of basic SVD (Singular Value Decomposition) frameworks, I was able to understand SVDs from a practical standpoint of what you can do with them, instead of focusing on the science.

In my talk, you will learn 2 practical ways of generating recommendations using SVDs: matrix factorization and item similarity. We will be learning the high-level components of SVD the ""doer way"": we will be implementing a simple movie recommendation engine with the help of Jupiter notebooks, the MovieLens database, and the Surprise recommendation package.
",python,2018
"Do we even need humans? Humans and data science are flawed on their own. Humans lack the ability to process large volumes of information. Machines lack intuition, empathy, and nuance. You'll learn how to guide users of expert-use systems by applying data science to their user experience. This allows us to take advantage of the human-touch while leveraging our large datasets. What is the relationship between human decisions and algorithms? Are we thinking about data science all wrong? In this talk, you'll learn the ways we balance human decisions and data science throughout our applications, the challenges we have faced along the way and the future of the relationship between humans and data.",python,2018
"Writing quality Python code can be both tough and tedious. On top of the general design, there are many code quality aspects that you need to watch out for when writing and reviewing code such as adherence to PEP8, docstring quality, test quality, etc. Furthermore, everyone is human. If you are catching these code quality issues by hand, there is a good chance that at some point you will miss an easy opportunity to improve code quality. If the quality check can be done by a machine, then why would you even try to catch the code quality issue by hand? In the end, the machine will be able to perform the quality check with much more speed, accuracy, and consistency than a person.

This talk will dive into how existing open source projects offload and automate many of these code quality checks resulting in:

- A higher quality and a more consistent codebase
- Maintainers being able to focus more on the higher level design and interfaces
  of a project.
- An improved contribution process and higher quality pull requests from
  external contributors

By diving into how these open source projects automate code quality checks, you will learn about:

- The available tooling related to checking code quality such as `flake8`,
  `pylint`, `coverage`, etc.
- How to automate code quality checks for both a development and team 
  setting.
- First-hand accounts of the benefits and lessons learned from automating
  code quality checks in real-life open source projects.

",python,2018
"Nowadays, there are many ways of building data science models using Python, including statistical and machine learning methods. I will introduce probabilistic models, which use Bayesian statistical methods to quantify all aspects of uncertainty relevant to your problem, and provide inferences in simple, interpretable terms using probabilities.  A particularly flexible form of probabilistic models uses Bayesian *non-parametric* methods, which allow models to vary in complexity depending on how much data are available. In doing so, they avoid the over-fitting that is common in machine learning and statistical modeling. I will demonstrate the basics of Bayesian non-parametric modeling in Python, using the PyMC3 package. Specifically, I will introduce two common types, Gaussian processes and Dirichlet processes, and show how they can be applied easily to real-world problems using two examples.",python,2018
"Behavior-Driven Development (BDD) is gaining popularity as an improved way to collaborate over product features and tests. In Python, **behave** is one of the leading BDD test frameworks. Using **behave**, teams write Gherkin behavior scenarios (e.g., tests) in plain language, and then programmers write Python code to automate the steps. BDD testing is great because tests are self-documenting and steps abide by the DRY principle. An example test could be:

> Given the DuckDuckGo home page is displayed
> When the user searches the phrase ""Python""
> Then search results for ""Python"" are shown

This talk will teach how to use **behave** to develop well-designed test scenarios and a robust automation framework. It will focus on the layers of the behave framework: feature files, step definitions, support classes, and config files. A full example project will be hosted on GitHub for audience members to reference after the talk.

",python,2018
"Scraping one web site for information is easy, scraping 10000 different sites is hard. Beyond page-specific scraping, how do you build a program than can extract the publication date of (almost) any news article online, no matter the web site?

We’ll cover when to use machine learning vs. humans or heuristics for data extraction, the different steps of how to phrase the problem in terms of machine learning, including feature selection on HTML documents, and issues that arise when turning research into production code.",python,2018
"You've used pytest and you've used mypy, but bugs are still slipping through your code. What's next? In this talk, we cover two simple but powerful tools for keeping your code problem-free. Property-based testing, provided by the [Hypothesis](https://hypothesis.readthedocs.io/en/latest/) library, lets you run hundreds of tests from a single template. Contracts, via [dpcontracts](https://github.com/deadpixi/contracts), make your program test itself. You'll learn how and why to use these tools and how to combine them with the rest of your testing suite.
",python,2018
"Big-O is a computer science technique for analyzing how code performs as data gets larger.  It's a very handy tool for the working programmer, but it's often shrouded in off-putting mathematics.

In this talk, I'll teach you what you need to know about Big-O, and how to use it to keep your programs running well.  Big-O helps you choose the data structures and algorithms that will let your code work efficiently even on large data sets.

You can understand Big-O even if you aren't a theoretical computer science math nerd. Big-O isn't as mystical as it appears. It's wrapped in mathematical trappings, but doesn't have to be more than a common-sense assessment of how your code will behave.
",python,2018
"In the past few years, the power of computer vision has exploded. In this talk, we'll apply a deep learning model to a bird feeder. We'll use that model to detect, identify, and record birds that come to a smart bird feeder.

Along the way, we'll learn about different platforms to deploy deep learning cameras on, from the lowly Raspberry PI all the way up to the powerful NVIDIA Jetson embedded computer with a built in GPU.",python,2018
"Facebook, Google, Uber, LinkedIn, and friends are the rarefied heights of software engineering. They encounter and solve problems at scales shared by few others, and as a result, their priorities in production engineering and architecture are just a bit different from the rest of us down here in the other 99% of services. Through deconstructing a few blog posts from these giants, we’ll evaluate just what is it that they’re thinking about when they build systems and whether any of their choices are relevant to those of us operating at high scale yet still something less than millions of requests per second.

This talk will go into depth on how to make technological decisions to meet your customers’ requirements without requiring a small army of engineers to answer 2 AM pages, and how to set realistic goals for your team around operations, uptime, communications, and disaster recovery.

With these guidelines in mind, you should be better equipped to say no (or yes!) the next time your team’s software hipster proposes moving everything to the Next Big Thing.",python,2018
"Have you ever wanted to write a GUI application you can run on your laptop? What about an app that you can run on your phone? Historically, these have been difficult to achieve with Python, and impossible to achieve without learning a different API for each platform. But no more.

BeeWare is a collection of tools and libraries that allows you to build cross-platform native GUI applications in pure Python, targeting desktop, mobile and web platforms. In this talk, you'll be introduced to the BeeWare suite of tools and libraries, and see how you can use them to develop, from scratch, a GUI ChatBot application that can be deployed as a standalone desktop application, a mobile phone application, and a single page webapp - without making any changes to the application's codebase.",python,2018
"It’s one thing to build a robust data pipeline process in python but a whole other challenge to find tooling and build out the framework that allows for testing a data process. In order to truly iterate and develop a codebase, one has to be able to confidently test during the development process and monitor the production system. 

In this talk, I hope to address the key components for building out end to end testing for data pipelines by borrowing concepts from how we test python web services. Just like how we want to check for healthy status codes from our API responses, we want to be able to check that a pipeline is working as expected given the correct inputs. We’ll talk about key features that allows a data pipeline to be easily testable and how to identify timeseries metrics that can be used to monitor the health of a data pipeline.",python,2018
"We build product and software as teams. And as anyone who as worked on a team knows, there’s often a lot more that goes into working together to build that product than actually just building the product itself. A highly functional team is not as elusive it may seem. Software engineering is a skill we’ve developed, but even more importantly software engineering on teams is another skill we’ve been practicing and improving on as an industry. Software engineering principles and best practices may seem to have very little to do with teamwork, but being able to thoughtfully apply some of what we’ve learned as engineers towards teamwork, we can help move towards creating such success with our teams.",python,2018
"Want to know about the latest trends in the Python community and see the the big picture of how things have changed over the last few years? Interested in the results of the latest official Python Developers Survey 2017 which was supported by the Python Software Foundation and gathered responses from more than 10.000 Python developers? Come learn about the most popular types of Python development, trending frameworks, libraries and tools, additional languages being used by Python developers, Python versions usage statistics and many other insights from the world of Python. All derived from the actual data and professional research such as the Python Developers Survey 2017 which collected responses from over 10.000 Python developers, organized in partnership between the Python Software Foundation and JetBrains, the Python Developers Survey 2016, 3rd party surveys and supplementary analytical research.",python,2018
"Python now offers static types! Companies like Dropbox and Facebook, and open-source projects like Zulip, use static types (with [PEP 484](https://www.python.org/dev/peps/pep-0484/) and [mypy](https://github.com/python/mypy)) to make Python more productive and fun to work with — in existing codebases from 40k lines to 4 million, in Python 2 and 3, and while preserving the conciseness and flexibility that make Python a great language in the first place. I’ll describe how.

Reading and understanding code is a huge part of what we do as software developers. If we make it easier to understand our codebases, we make everyone more productive, help each other write fewer bugs, and lower barriers for new contributors. That's why Python now features optional static types, and why Dropbox, [Facebook](https://engineering.instagram.com/let-your-code-type-hint-itself-introducing-open-source-monkeytype-a855c7284881), and [Zulip](https://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/) use them on part or all of their Python code.

In this talk, I’ll share lessons from Zulip’s and Dropbox’s experience — having led the mypy team at Dropbox and working now on the Zulip core team — for how you can start using static types in your own codebases, large or small. We’ll discuss how to make it a seamless part of your project’s tooling; what order to approach things in; and powerful new tools that make it even easier today to add static types to your Python codebase than ever before.
",python,2018
"As engineers, we care a lot about the reliability of our applications. When a website falls over, pagers go off, and engineers burst into action to bring a site back to life.  Postmortems are written, and teams develop strategies to prevent similar failures in the future.

But what about the reliability of our data? Would _you_ trust financial reports built on your data? 

If not, what can you do to improve data health? If you _would_ trust these reports, how can you prove to customers, investors, and auditors alike that they should too?

In this talk, you’ll learn to apply strategies from the world of dev-ops to data. You’ll learn about questions auditors ask that can help you pinpoint data problems. You’ll also learn some accounting-specific tools for accurate and timely record keeping that I’ve found fascinating and helpful!

",python,2018
"Code reviews don't have to be a time consuming, morale zapping, arduous tasks. Not only can they catch bugs and errors but they can contribute in positive ways to the individual developer, the team, management and company as a whole.  

Art critiques have existed in academia for hundreds of years. The methodology of the critique has evolved to be time sensitive and productive, while keeping the enthusiasm of the student artist intact.  

The purpose of the art critique is to get peers and mentors to look at the work and raise any problems they may see. It's also time where people with more experience could contribute their knowledge in a helpful way. This process is about producing the best work, quickly and in a productive and constructive way. 
 
These methods can be applied to code review.  

",python,2018
"In 2017, I was released from prison after serving 17 years. One of the most transformational experiences I had while incarcerated was learning to code, through a pioneering new program called Code.7370 — the first coding curriculum in a United States prison.

In this talk, I’d like to share my experiences learning to code in prison and getting a software engineering job after my release, with the goals of:

Inspiring new programmers to stick with it and be confident in their abilities

Inspiring educators to think about how to support new coders in a broad range of learning environments (there’s no internet in prison!)

Inspiring everyone to think about the potential for rehabilitation in prison in a new way
",python,2018
"Colossal Cave, also known as Adventure or ADVENT, is the original text adventure. It was written in FORTRAN IV and there is practically no way to run the original program without translating it. We'll explore software archeology to write a Python interpreter to run the FORTRAN code as-is, without translating it. Come learn about pre-ASCII and 36-bit integers and writing interpreters in Python!

And, we'll show how to use BeeWare's Batavia Python interpreter (in JavaScript) to execute the program. FORTRAN IV in Python in JavaScript in your browser!",python,2018
"Testing mobile applications is hard. Testing manually is nearly impossible.
That’s where automated testing shines. Just sit back and watch the machine go!
Python is a very powerful language for writing automated tests, but since Python is not installed on mobile platforms, we need to find a way to remotely control and monitor the device.
But how do we automate a device remotely? The answer is Appium.

In this talk I will go over the process of deploying and testing iOS (or Android) applications, and how to work with Appium to easily generate Python 3 code for testing your application.",python,2018
"Setting up application monitoring is often an afterthought, and in the speaker's opinion can be a bit overwhelming to get started with. What is a `metric`? What is a `gauge`? What is a `counter`? What's that `upper 90` metric you have up on your `dashboard`? And what *all* metrics should I monitor?

This talk aims to get you started on the monitoring journey in Python. In addition to clearing up some of the jargon, we will look at `statsd` and `prometheus` monitoring systems and how to integrate our applications with these.

Without the numbers, we are really flying blind!",python,2018
"![Logo][1]

[**Website**](https://cupy.chainer.org/) | [**Docs**](https://docs-cupy.chainer.org/en/stable/) | [**Install Guide**](https://docs-cupy.chainer.org/en/stable/install.html) | [**Tutorial**](https://docs-cupy.chainer.org/en/stable/tutorial/) | **Examples** ([Official](https://github.com/cupy/cupy/blob/master/examples)) | [**Forum**](https://groups.google.com/forum/#!forum/cupy)

CuPy is an open-source library with NumPy syntax that increases speed by doing matrix operations on NVIDIA GPUs. It is accelerated with the CUDA platform from NVIDIA and also uses CUDA-related libraries, including cuBLAS, cuDNN, cuRAND, cuSOLVER, cuSPARSE, and NCCL, to make full use of the GPU architecture. CuPy's interface is highly compatible with NumPy; in most cases it can be used as a drop-in replacement. CuPy supports various methods, data types, indexing, broadcasting, and more.

  [1]: https://raw.githubusercontent.com/cupy/cupy/master/docs/image/cupy_logo_1000px.png",python,2018
"The PEP 557 dataclasses module is available in starting in Python 3.7.   It will become an essential part of every Python programmer's toolkit.  This talk shows what problem the module solves, explains its key design decisions, and provides practical examples of how to put it to work.

Dataclasses are shown to be the next step in a progression of data aggregation tools: tuple, dict, simple class, bunch recipe, named tuples, records, attrs, and then dataclasses. Each builds upon the one that came before, adding expressiveness at the expense of complexity.

Dataclasses are unique in that they let you selectively turn-on or turn-off its various capabilities and it lets the user choose the underlying data store (either instance dictionary, instance slots, or an inherited base class).

Dataclasses and typing.NamedTuple both use variable annotations which were new in Python 3.6.







",python,2018
"Data Visualization charts are supposed to be our map to information. However, when making charts, customarily we are just re-sizing lines and circles based on metrics instead of creating data-driven version of reality.  The contemporary charting techniques have a few shortcomings (especially when dealing with high-dimensional dataset): 

* **Context Reduction**: in order to fit a high-dimensional dataset into a chart one needs to filter/ aggregate/ flatten data which results in reduction of full context of information.  Without context most of the charts show only a part of the story, that can potentially lead to data misinterpretation/misunderstanding. 
* **Numeric Thinking**: naturally humans have hard time perceiving big numbers. While data visualization is suppose to help us to conceptualize large volumes,  unless the dataset is carefully prepared, 2D charts rarely give us the intuitive grasp of magnitude. 
* **Perceptual de-humanization**: when examining charts it is easy to forget that we are dealing with activity in real world instead of lines/bars. 

Augmented/Mixed Reality can potentially solve all of the issues listed above by presenting an intuitive and interactive environment for data exploration. Three dimensional space provides conditions to create complex data stories with more “realistic assets” (beyond lines and bars). The talk would present the architecture required to create MR data visualization story with Python (70% of architecture), starting with drawing 3D assets in a data-driven way and finishing with deployment on MR devices. ",python,2018
"Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.

Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.

Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.

In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.

Debuggers are a wonderful tool, however when you have 100 computers the “wonder” can be a bit more like “pain”. This talk will look at how to connect remote debuggers, but also remind you that it’s probably not the easiest path forward.
",python,2018
"In 2011 I gave a talk about ""Killing Patents with Python"" - finding the right piece of prior art by using statistical natural language processing techniques on the US Patent Database. A number of unexpected benefits came out of that exploration, including the ability to describe large patent portfolios and businesses in a way that had not been done before.

Since then, the state of the art has advanced - and so has the ability to do strange and wonderful things by applying the latest neural network-based analysis to the nine million patents and patent applications that people have submitted to the USPTO. Not only can we learn new things about what people have invented, we might just be able to get the computer to do a little ""inventing"" itself.


",python,2018
"We use JupyterHub, XArray, Dask, and Kubernetes to build a cloud-based system to enable scientists to analyze and manage large datasets.  We use this in practice to serve a broad community of atmospheric and climate scientists.

Atmospheric and climate scientists analyze large volumes of observational and simulated data to better understand our planet.  They have historically used tools like NumPy and SciPy along with Jupyter notebooks to combine efficient computation with accessibility.  However, as datasets increase in size and collaboration extends to new populations of scientists these tools begin to feel their age.  In this talk we use more recent libraries to build a modern deployment for academic scientists.  In particular we use the following tools:

- **Dask:** to parallelize and scale NumPy computations
- **XArray**: as a self-discribing data model and tool kit for labeled and index arrays
- **JupyterLab:** to enable more APIs for users beyond the classic notebook
- **JupyterHub:** to manage users and maintain environments for a new population of cloud-friendly users
- **Kubernetes:** to manage everything and deploy easily on cloud hardware

This talk will focus less on how these libraries work and will instead be a case study of using them together in an operational setting.  During the talk we will build up and deploy a running system that the audience can then use to access distributed computing resources.
",python,2018
"One of the most challenging and important thing fors for Python developers learn is the unittest mock library. The patch function is in particular confusing- there are many different ways to use it. Should I use a context manager? Decorator? When would I use it manually? Improperly used patch functions can make unit tests useless, all the while making them look as if they are correctly testing code.Let’s learn how to wield patch with confidence!
",python,2018
"In the 1850s, Edward Orange Wildman Whitehouse was appointed the lead engineer of the first attempt to build a trans-Atlantic telegraph cable. With the entire population of two continents waiting for his go-live, their handlebar moustaches aquiver, he demonstrated in fine form just how spectacularly a big project can be a bigger disaster.

This is a tale of long-winded rants, spectacular sideburns, and gentlemen scientists behaving badly. It is also a lesson about the importance of honest reflection in technical teamwork. Lilly outlines some of the mistakes made during one of the biggest tech delivery projects in history, and how a constructive view of failure helped to turn it all around. Through the public meltdowns of Wildman Whitehouse you will learn the importance of feedback, how to handle complex tasks gracefully, and the best way to recover from having your pipeline eaten by a whale.",python,2018
"Want to have fun with Python? Do something visual? Get started today? Learn how to draw, animate, and use sprites for games with the [Python Arcade](http://arcade.academy/) library.

""Arcade"" is an easy-to-use Python library for creating 2D arcade games. We'll show you how to get started creating your own game, and find plenty of example code to get an idea of what you can do with this library. If you are familiar with PyGame, Arcade is easier, more powerful, and uses recent Python features like type hinting and decorators.

This talk is great for beginners, educators, and people who want to create their own arcade games.",python,2018
"Multithreading makes shared memory easy, but true parallelism next to impossible. Multiprocessing gives us true parallelism, but it makes sharing memory very difficult, and high overhead. In this talk, we'll explore techniques to share memory between processes efficiently, with a focus on sharing read-only massive data structures.",python,2018
"Logs are our best friend, especially on those late nights when we try to troubleshoot a problem in production that was written by a co-worker who is on vacation. Logs are the main way to know what is happening with an application at runtime, but we don’t realize how important they are until we actually need them. Unfortunately, they are usually an under-estimated part of the development process.

This talk aims to transmit the need for the logging module, briefly explains how to use it and how it is built, and dives into all the complexity that is hidden to us. This will help attendees not just understand all the magic that allows us to inspect our applications at runtime, but also to avoid mistakes and adapt the module to our needs for more esoteric scenarios.

The talk is structured to simplify the understanding of the logging module. Many people have read the documentation, but still struggle to fully understand what is happening under the hood. This talk aims to eliminate that barrier by presenting it in an easier-to-digest manner.
",python,2018
"Are you an intermediate python developer looking to level up? Luckily, python provides us with a unique set of tools to make our code more elegant and readable by providing language features that make your code more intuitive and cut down on repetition. In this talk, I’ll share practical pythonic solutions for supercharging your code. 

Specifically, I'll cover:

- What magic methods are, and show you how to use them in your own code.
- When and how to use partial methods.
- An explanation of ContextManagers and Decorators, as well as multiple techniques for implementing them.
- How to effectively use `NamedTuples`, and even subclass and extend them!

Lastly, I'll go over some example code that ties many of these techniques together in a cohesive way. You'll leave this talk feeling confident about using these tools and techniques in your next python project!",python,2018
"Anyone who is interested in deep learning has gotten their hands dirty playing around with Tensorflow, Google's open source deep learning framework. Tensorflow has its benefits like wide scale adoption, deployment on mobile, and support for distributed computing, but it also has a somewhat challenging learning curve, is difficult to debug, and hard to deploy in production. PyTorch is a new deep learning framework that solves a lot of those problems.

PyTorch is only in beta, but users are rapidly adopting this modular deep learning framework. PyTorch supports tensor computation and dynamic computation graphs that allow you to change how the network behaves on the fly unlike static graphs that are used in frameworks such as Tensorflow. PyTorch offers modularity which enhances the ability to debug or see within the network and for many, is more intuitive to learn than Tensorflow.

This talk will objectively look at PyTorch and why it might be the best fit for your deep learning use case and we'll look at use cases that will showcase why you might want consider using Tensorflow instead.


",python,2018
"At the end of 2017, there were seven states with ongoing redistricting litigation.  We will discuss a statistical model that the United States Supreme Court declared to be appropriate in cases of racial gerrymandering, and show how it can be implemented and used with the library `PyMC3`.  We will also discuss what the model tells us about racial gerrymandering in North Carolina.
",python,2018
"Today, services built on Python 3.6.3 are widely used at Facebook. But as recently as May of 2014 it was actually impossible at all to use Python 3 at Facebook. Come learn how we cut the Gordian Knot of dependencies and social aversion to the point where new services are now being written in Python 3 while older Python 2 projects are actively migrated to Python 3. All accomplished by a small group of individual contributors in their spare time. Learn to fight the good fight and upgrade your organization to Python 3 like we did at Facebook.",python,2018
"You maintain an Open Source project with great code? Yet your project isn’t succeeding in the ways you want? Maybe you’re struggling with funding or documentation? Or you just can’t find new contributors and you’re drowning in issues and pull requests?
Open Source is made up of many components and we are often better-trained in methods for writing good code, than in methods for succeeding in the other dimensions we want our project to grow. 
In this talk we’ll explore the different components of an Open Source project and how they work together. After this talk you’ll be well-equipped with a ideas and strategies for growing, cultivating, and nourishing your Open Source project. 

For your project to succeed, all of its non-code components must be well-maintained. What are these different components and what methods can we learn to maintain them?

* Build real relationships with your sponsors and determine ways how both sides can benefit from this relationship, don’t just ask people for money. 
* Establish a good communication system with your contributors: Keep them informed, listen to their feedback and input, make them feel heard. 
* Thank the people who worked on ticket triage or marketing, not just those who wrote code, in your release notes. 
* Make it easy for new contributors to get started: Write and maintain good documentation, answer questions in a friendly and timely manner. 
* Market and evangelize in the right places and at the right time: Give conference talks, organize sprints, keep your project’s Twitter account active, always curate new and interesting content on your blog or website.
* Implement a Code of Conduct and enforce it if needed: Make your project a safe space to contribute for everyone. 

With these methods and a half-dozen others, you’ll handle beautifully all the components your project needs to succeed.",python,2018
"Resources are files that live within Python packages.  Think test data files, certificates, templates, translation catalogs, and other static files you want to access from Python code.  Sometimes you put these static files in a package directory within your source tree, and then locate them by importing the package and using its `__file__` attribute.  But this doesn't work for zip files!

You could use `pkg_resources`, an API that comes with `setuptools` and hides the differences between files on the file system and files in a zip file.  This is great because you don't have to use `__file__`, but it's not so great because `pkg_resources` is a big library and can have potentially severe performance problems, even at import time.

Welcome to `importlib.resources`, a new module and API in Python 3.7 that is also available as a standalone library for older versions of Python.  `importlib.resources` is build on top of Python's existing import system, so it is very efficient.  It also defines an abstract base class which loaders can implement to provide their own resource access.  Python's built-in zipimporter uses this to provide efficient access to resources within a zip file.  Third party import hooks can do the same, so resources can come from anything that is importable by Python.

This talk will step through the motivations behind `importlib.resources`, the library's usage, its interfaces, and the hooks made available to third party packages.  It will also talk about the minor differences between the standalone version and the version in Python 3.7's standard library.  Hopefully audience members will come away with compelling reasons to port their code to this much more efficient library.",python,2018
"Have you ever considered how many relationships you have in your virtual life? Every friend or page liked on Facebook, each connection in LinkedIn or Twitter account followed is a new relationship not only between two people, but also between their data. In Brazil only, we have 160 millions Facebook users. How can we represent and manipulate all these relationships? Graph Databases are storage systems that use graph structure (nodes and edges) to represent and store data in a semantic way.

This talk will begin approaching the challenge in representing relationships in Relational Databases and introducing a more friendly solution using graph. The definition of Graph Database, its pros and cons and some available tools (Neo4J, OrientDB and TitanDB) will be shown during the presentation, as well as how these tools can be integrated with Python.
",python,2018
"During peak hours, Netflix video streams make up more than one third of internet traffic. Netflix must stream uninterrupted in the face of widespread network issues, bad code deploys, AWS service outages, and much more. Failovers make this possible.

Failover is the process of transferring all of our traffic from one region in AWS to another. While most of Netflix runs on Java, failovers are powered entirely by Python. Python's versatility and rich ecosystem means we can use it for everything from predicting our traffic patterns to orchestrating traffic movement, while dealing with the eventual consistency of AWS.

Today, we can shift all of our 100 million+ users in under seven minutes. A lot of engineering work went into making this possible. The issues we faced and solutions we created have broad application to availability strategies in the cloud or the datacenter.",python,2018
A function is a small chunk of code that does useful work. Your job when writing a function is to do it in a way that it easy to read. Based on over 15 years of code reviews here are some tips and guidelines I give again and again. ,python,2018
"The DevOps movement gave us many ways to put Python applications into production.  But should your *application* care?  Should it need to know whether it’s running on your notebook, on a server, in a Docker container, or in some cloud platform as a service?

It should not, because environment-agnostic applications are easier to **test**, easier to **deploy**, easier to **handle**, and easier to **scale**.

But how can you *practically* structure and configure your applications to make them indifferent to the environment they run in?  How do secrets fit into the picture?  And where do you put that log file?

By the end of this talk you’ll know the tools and techniques that enable you to write such Python applications and you’ll be ready for the next big change.",python,2018
"New conferences rarely have resources to run the sort of outreach and inclusion programs that big conferences have. It’s hard to guess how much money you’ll have to spend, how many attendees you’ll have, and what your new community will look like. With so many things to worry about, it’s no surprise that most events don’t prioritise outreach until they’ve got a few years under their belt, if at all.

It doesn’t have to be this way, and it can even be easier to build a new event around outreach and inclusion than it is to build it in later on!

This talk shares the story of North Bay Python’s inaugural conference, which we planned in under 6 months, ran on a $40,000 budget, and built a welcoming community to make it real. We made inclusivity a founding principle and did so without compromising our speaker lineup while still attracting great sponsorship and hosted an event that almost every attendee wants to return to.

In this talk, we’re going to share with you how we built a conference, from the ground up, to be as inclusive as we could make it. We’ll touch on early organisation, marketing, and on-the ground logistics. Throughout the talk, you’ll learn:

* How we designed a budget that let us prioritise outreach and inclusion activities
* How we built the community that we wanted before the conference even started
* How we ran an event that proved that we meant everything we said

You too can host a new conference with a great lineup on a shoestring budget and short timeline, and you can do it while being inclusive, welcoming, and putting attendee safety first. Find out how you can have your cake, eat it, and still have lots to share with your new community.
",python,2018
"Most software has a user. Depending on the software, the user may need to provide various details about themselves for proper operation -- their name, their date of birth, where they live. However, it is quite common for software systems such as these to ask the wrong questions, collect too much data, and when it comes down to it, serialise the parts of the user's identity wrongly. This talk will discuss common ways that real-world systems store identity wrong, what questions you shouldn't ask, and how you can fix it in your own projects.",python,2018
"Timezones are one of those things every programmer loves to hate. Most of us, at
least in the US, just try to ignore them and hope nobody notices. Then twice a
year, we fear with impending doom those 3 small words: Daylight Saving Time.

It doesn't have to be this way. Armed with some best practices and a little help
from supporting libraries, timezone-related bugs can be a thing of the past.

This talk explores standard library and 3rd party library timezone support, as
well as persistence and serialization techniques for timezone-aware datetimes.
By the end of the talk, the listener should feel confident in their ability to
correctly store, send, receive, and manipulate datetime objects in any timezone.",python,2018
"Questions and confusion about the Python packaging ecosystem abound. What is this `setup.py` file? What's the difference between wheels and eggs? Do I use setuptools or distutils? Why should I use twine? Do I put my projects dependencies in a `requirements.txt` or in `setup.py`? How do I just get my module up on PyPI? Wait, what is Warehouse?

This talk will identify the key tools one might encounter when trying to distribute Python software, what they are used for, why they exist, and their history (including where their weird names come from). In addition, we'll see how they all work together, what it takes to make them work, and what the future has in store for Python packaging.",python,2018
"Unless you work on pacemakers or at NASA, you've probably accepted the fact that you will make mistakes in your code, and those mistakes will creep into production. This talk will introduce you to post-mortems, and how to use them as a vehicle for improving your code and your process.",python,2018
"Imagine you have an appointment in a large building you do not know. Your host sent instructions describing how to reach their office. Though the instructions were fairly clear, in a few places, such as at the end, you had to infer what to do. How does a _robot (agent)_ interpret an instruction in the environment to infer the correct course of action? Enabling harmonious _Human - Robot Interaction_ is of primary importance if they are to work seamlessly alongside people.

Dealing with natural language instructions in hard because of two main reasons, first being, Humans - through their prior experience know how to interpret natural language but agents can’t, and second is overcoming the ambiguity that is inherently associated with natural language instructions. This talk is about how deep learning models were used to solve such complex and ambiguous problem of converting natural language instruction into its corresponding action sequence.

Following verbal route instructions requires knowledge of language, space, action and perception. In this talk I shall be presenting, a neural sequence-to-sequence model for direction following, a task that is essential to realize effective autonomous agents.

At a high level, a sequence-to- sequence model is an end-to-end model made up of two recurrent neural networks: 

 - **Encoder** - which takes the model’s input sequence as input and encodes it into a fixed-size context vector.
 - **Decoder** - which uses the context vector from above as a seed from which to generate an output sequence. 

For this reason, sequence-to-sequence models are often referred to as _encoder-decoder_ models. The alignment based encoder-decoder model would translate the natural language instructions into corresponding action sequences. This model does not assume any prior linguistic knowledge: syntactic, semantic or lexical. The model learns the meaning of every word, including object names, verbs, spatial relations as well as syntax and the compositional semantics of the language on its own.

In this talk, steps involved in pre-processing of data, training the model, testing the model and final simulation of the model in the virtual environment will be discussed. This talk will also cover some of the challenges and trade-offs made while designing the model.",python,2018
"Wrestling bugs can be one of the most frustrating parts of programming - but with the right framing, bugs can also be our best allies. I'll tell the tales of two of my favorite bugs, including the time I triggered a DDOS of a logging cluster, and explain why I love them. I'll also give you concrete strategies for approaching tricky bugs and making them easier and more fun.",python,2018
"Those of us who have worked in software development for longer than a few years probably feel we have an intuitive sense of what a great developer is. Some traits come more easily to mind than others when it comes to identifying a great developer. In this talk we will take a slightly different approach to evaluating software development best practices, and identify one underrated skill common to great software developers: empathy. I hope to demonstrate that cognitive and emotional empathy skills are critical to good software development. We will explore ways to cultivate this trait in order to become better developers, both for our own sakes and for the sake of the teams in which we work.",python,2018
"What do AWS, GitHub, Travis CI, DockerHub, Google, Stripe, New Relic, and the rest of the myriad of services that make our developer life easier have in common?
 They all give you secret keys to authenticate with. Did you ever commit one of these to source control by mistake? That happened to me more times than I'm willing to admit!

In this talk I'm going to go over the best practices to follow when writing Python applications that prevent this type of accident.",python,2018
"Python provides a powerful platform for working with data, but often the most straightforward data analysis can be painfully slow. When used effectively, though, Python can be as fast as even compiled languages like C. This talk presents an overview of how to effectively approach optimization of numerical code in Python, touching on tools like numpy, pandas, scipy, cython, numba, and more.",python,2018
"This talk is about the history of Python packaging, the tools that have been historically available for application deployment, the problems/constraints presented by them, and presents a holistic solution to many of these problems: Pipenv.

A live demo of the tool will be presented, as well as a Q&A session.",python,2018
"Each member of your project team uses something different to document
their work -- RestructuredText, Markdown, and Jupyter Notebooks. How do
you combine all of these into useful documentation for your project's users.
Sphinx and friends to the rescue!

Learn how to integrate documentation into your everyday development
workflow, apply best practices, and use modern development tools and
services, like Travis CI and ReadTheDocs, to create engaging and up-to-date
documentation which users and contributors will love.

",python,2018
"The genome of a typical microbe contains roughly 5 million base pairs of DNA including > 4000 genes, which provide the instructions for cellular replication, energy metabolism, and other biological processes. At Zymergen, we edit DNA to design microbes with improved ability to produce valuable materials and molecules. Microbes with these edits are built and tested in high throughput by our fleet of robots. Genomes are far too large for exhaustive search, so identifying which edits to make requires machine learning on non-standard features. Our task to extract information from trees, networks, and graphs of independently representable knowledge bases (metabolism, genomics, regulation), in ways that respect the strongly causal relationships between systems. In this talk, I will describe how we use Python’s biological packages (e.g. BioPython, CobraPy, Escher, goatools) and other packages (NetworkX, TensorFlow, PyStan, AirFlow) to extract machine learning features and predict which genetic edits will produce high-performance microbes.",python,2018
"If you’ve spent much time writing (or debugging) Python performance problems, you’ve probably had a hard time managing memory with its limited language support. 

In this talk, we venture deep into the belly of the Rust Language to uncover the secret incantations for building high performance and memory safe Python extensions using Rust. 

Rust has a lot to offer in terms of safety and performance for high-level programming languages such Python, Ruby, Js and more with its easy Foreign Function Interface capabilities which enables developers to easily develop bindings for foreign code. 
",python,2018
"The end of life for Python 2 is 2020. Python 3 is the future and you'll need to consider both your upgrade plan and what steps you'll take after upgrading to start leveraging Python 3 features.

During this talk we'll briefly discuss how to start **the process of upgrading your code to Python 3**. We'll then dive into some of **the most useful Python 3 features** that you'll be able to start embracing once you drop Python 2 support.

A number of the most powerful Python 3 features are syntactic features that are **Python 3 only**. You won't get any experience using these features until you fully upgrade. These features are an incentive to drop Python 2 support in existing 2 and 3 compatible code. You can consider this talk as a teaser of Python 3 features that you may have never used.

After this talk I hope you'll be inspired to fully upgrade your code to Python 3.",python,2018
"Looking back at Python evolutions over the last 10 years.

Python 3.0 was released ten years ago (December 2008). It's time to look back: analyze the migration from Python 2 to Python 3, see the progress we made on the language, list bugs by cannot be fixed in Python 2 because of the backward compatibility, and discuss if it's time or not to bury Python 2.

Python became the defacto language in the scientific world and the favorite programming language as the first language to learn programming.

",python,2018
"For 2 years, a family of three has traveled on a converted school bus from conference to conference, building tooling for the road in Python and visiting Python families in every corner of the country.",python,2018
"What do geiger counters, black holes, heart monitors, and volcanoes have in common?  They all can use sound to convey information! This talk will explore using python for sonification: the process of translating data into sound that could otherwise be represented visually. Have you ever wondered how to use python to represent data other than making charts and graphs?  Are you a musician looking for inspiration in the world around you?  This talk will go over how to use python to translate time series data to MIDI that can be played back in real time. We’ll sonically interpret light-curve data from the Kepler space telescope using pygame, MIDIUtil, and astropy, turning points on a graph into a musical masterpiece! Come learn about how data sonification is used to help people, to expand the reach of scientific research, and to create music from data.",python,2018
"Quantum computers are slowly turning in to reality more than 30 years after they were first theorized. The need for quantum computers have become clear as we reach the limits of Moore’s law and yet we need more computational power. We are at a very early stage of quantum computing. Yet Python is slowly becoming a defacto language for programming quantum computers. 

In this talk, we will discuss the difference a traditional computer and a quantum computer. We will learn about the two architectures namely Quantum annealing and Quantum gate. Finally, we will learn to program quantum computers using Python.
",python,2018
"Python 3 removes a lot of the confusion around Unicode handling in Python, but that by no means fixes everything. Different locales and writing systems have unique behaviours that can trip you up. Here’s some of the worst ones and how to handle them correctly.",python,2018
"Occasionally we’ll find that some bit of Python we’ve written doesn’t run as fast as we’d like, what can we do? Performance bottlenecks aren’t always intuitive or easy to spot by reading code so we need to collect data with [profiling](https://docs.python.org/3.6/library/profile.html). Once we’ve identified the bottleneck we’ll need to change our approach, but what options are faster than others?

This talk illustrates a Python performance investigation and improvements using an [Advent of Code](http://www.adventofcode.com/) programming challenge. I’ll walk through starting from a slow (but correct) solution, look at profiling data to investigate _why_ it’s slow, and explore multiple paths for improving performance, including more efficient algorithms and using third-party tools like [Cython](http://cython.org/). You’ll leave this talk with a recipe for analyzing Python performance and information about some options for improved performance.",python,2018
"There are many computational needs for randomness--from creating a game to building a simulation involving naturally occurring randomness similar to the physical world. For most purposes using the python math module to create random numbers within a specific range can be done with no further questions, but sometimes we require a more nuanced implementation. 

We will look at both pseudo-random number generators, which use statistically repeatable processes to generate seemingly random series and true random number generators, which inject physical processes like atmospheric noise to generate sequences of numbers. We will discuss the benefits and drawbacks of both approaches and common methods of implementing these two types of generators in python. 

Finally, we will look at several real applications for randomness and discuss the best method for generating “randomness” in each scenario. ",python,2018
"Web applications contains lots of database operations, network calls, nested callbacks and other computationally expensive tasks that might take a long time to complete or even block other threads until it's done, here is where ReactiveX enters, it doesn't only gives us the facility to convert almost anything to a stream; variables, properties, user inputs, caches, etc to manage it asynchronously. But it also gives us an easy way to handle errors which is a hard task within asynchronous programming. ReactiveX makes our code more flexible, readable, maintainable and easy to write.

We will be exploring how ReactiveX help us to make things easier with its operators toolbox that can be used to filter, create, transform or unify any of those streams. We will learn that in just a few lines of maintainable code, we can have multiple web sockets which recieves multiple requests all handled by an asynchronous process that serves a filtered output.

To do that I decided to explain an example of the use with an example by implementing observables, observers/subscribers and subjects. We will start by requesting our data stream from the Github API with a Tornado web socket and then filtering and processing it asynchrounosly.",python,2018
"Writing lexers and parsers is a complex problem that often involves the use of special tools and domain specific languages (e.g., the lex/yacc tools on Unix).  In 2001, I wrote Python versions of these tools which can be found in the PLY project.  PLY predates a huge number of modern Python features including the iteration protocol, generators, decorators, metaclasses, and more.  As such, it relied on a variety of clever hacks to layer a domain specific parser specification language on top of Python itself. 

In this talk, I discuss a modernization of the PLY project that abandons its past and freely abuses modern Python features including advanced metaclasses, guaranteed dictionary ordering, class decorators, type hints, and more.   The result of this work can be found in the SLY project.  However, this talk isn't so much about SLY as it is focused on how far you can push Python metaprogramming features to create domain-specific languages.   Prepare to be horrified--and to write code that will break your IDE. 
 ",python,2018
"The WSGI (Web Server Gateway Interface) specification for hosting Python web applications was created in 2003. Measured in Internet time, it is ancient. The oldest main stream implementation of the WSGI specification is mod_wsgi, for the Apache HTTPD server and it is over 10 years old.

WSGI is starting to be regarded as not up to the job, with technologies such as HTTP/2, web sockets and async dispatching being the way forward. Reality is that WSGI will be around for quite some time yet and for the majority of use cases is more than adequate.

The real problem is not that we need to move to these new technologies, but that we aren't using the current WSGI servers to their best advantage. Moving to a new set of technologies will not necessarily make things better and will only create a new set of problems you have to solve.

As one of the oldest WSGI server implementations, Apache and mod\_wsgi may be regarded as boring and not cool, but it is still the most stable option for hosting WSGI applications available. It also hasn't been sitting still, with a considerable amount of development work being done on mod\_wsgi in the last few years to make it even more robust and easier to use in a development environment as well as production, including in containerised environments.

In this talk you will learn about many features of mod\_wsgi which you probably didn't even know existed, features which can help towards ensuring your Python web application deployment performs to its best, is secure, and has a low maintenance burden.

Topics which will be covered include:

* Easy deployment of Python web applications using mod\_wsgi-express.
* Integration of mod_wsgi-express with a Django web application.
* Using mod\_wsgi-express in a development environment.
* How to make use of mod\_wsgi-express in a production environment.
* Using mod_wsgi-express in a containerised runtime environment.
* Ensuring consistency between development and production environments using warpdrive.
* Using mod\_wsgi-express to bootstrap a system Apache installation for hosting WSGI applications.
* Why you should be using daemon mode of mod\_wsgi and not embedded mode.
* How to properly associate mod\_wsgi with a Python virtual environment.
* Building a robust deployment that can recover from misbehaving application code, backend services, or request overloading.
* Using hooks provided by mod\_wsgi to monitor the performance of your Python web application.

If you are a beginner, come learn why mod\_wsgi is still a good option for deploying your Python web applications. If you are an old time user of mod\_wsgi, find out about all the features you probably didn't know existed, revisit your current Python web application deployment and make it even better.",python,2018
"When you think of an API, you’re probably thinking about a web service. But it’s important to think about your developer interface when designing a software library as well! I’ll talk about the scikit-learn package, and how its API makes it easy to construct complex models from simple building blocks, using three basic pieces: transformers, estimators, and meta-estimators. Then I’ll show how this interface enabled us to construct our own meta-estimator for model stacking. This will demonstrate how to implement new modeling techniques in a scikit-learn style, and more generally, the value of writing libraries with the developer interface in mind.",python,2018
"Stop writing crappy shell scripts—write crappy Python scripts instead!

Other talks will show you how to write clean, performant, robust Python.  But that's not always necessary.  When writing personal automation or solving one-shot problems, it can be safe (and fun!) to quickly hack something together.

This talk will show examples of problems suitable for this approach, scenarios where it's reasonable to cut corners, novel techniques that can help break a problem down, and shortcuts that can speed development.",python,2018
"At some point, we all find ourselves at a SQL prompt making edits to the production database. We know it's a bad practice and we always intend to put in place safer infrastructure before we need to do it again — what does a better system actually look like?

This talk progresses through 5 strategies for teams using a Python stack to do SQL writes against a database, to achieve increasing safety and auditability:

(1) Develop a process for raw SQL edits 
(2) Run scripts locally
(3) Deploy and run scripts on an existing server
(4) Use a task runner
(5) Build a Script Runner service

We’ll talk about the pros and cons of each strategy and help you determine which one is right for your specific needs.

By the end of this talk you’ll be ready to start upgrading your infrastructure for making changes to your production database safely!",python,2018
"Taking on leadership roles always includes new demands on your attention and time. Inevitably, your finite work week will conflict with the sheer amount of tasks you have to do. How can we as leaders keep stepping up to new responsibilities while balancing our pre-existing ones?

This talk will focus on strategies for managing a too-large workload without abandoning important tasks or doing a shoddy job. We’ll look at techniques to prioritize what work matters most, identify tasks we should be doing ourselves, and finally delegate the rest to build our team’s skills while reducing our own workload.",python,2018
"Done! Your shiny new application is functionally complete and ready to be deployed to production! But how exactly do you deploy properly on Linux? Wonder no more! In 30 minutes, this talk explains how you can harness the power of the init system and systemd to solve common deployment problems, including some that you didn't even know you had. Examples of things we will cover:

* How to secure your system by having: private /tmp for your process, read-only paths so that your process can not write to them, inaccessible paths, protect users home, network access, bin directories, etc.
* How to limit the resources you app can consume.
* How to interact directly with systemd, so it can start transient units, start/stop services, mount disks, resolve addresses.
* How to isolate your service without containers.
* How to isolate your service using containers (using systemd to spawn a namespace).

All this will be covered from a Python developer's perspective.
",python,2018
"The Django Channels project has taken a major turn with version 2.0, embracing Python's async functionality and building applications around an async event loop rather than worker processes.

Doing this, however, wasn't easy. We'll look through some of the techniques used to make Django coexist in this async world, including handing off between async and sync code, writing fully asynchronous HTTP and WebSocket handling, and what this means for the future of Django, and maybe Python web frameworks in general.",python,2018
"Get under the hood and learn about Python's beloved Abstract Syntax Tree. Ever wonder how Python code is run? Overheard people arguing about whether Python is interpreted or compiled? In this talk, we will delve into the lifecycle of a piece of Python code in order to understand the role that Python's Abstract Syntax Tree plays in shaping the runtime of your code. Utilizing your newfound knowledge of Python's AST, you'll get a taste of how you probably already rely on ASTs and how they can be used to build awesome tools.
",python,2018
"As web apps grow increasingly complex, distributing asynchronous work across multiple background workers is often a basic requirement of a performant app.  While there are a variety of tools that exist to solve this issue, one common feature among them is the need for a robust messaging platform.

[RabbitMQ][1] is a stable, full-featured, and mature solution that is usually found in the Python ecosystem backing [Celery][2] implementations.  While Celery's utilization of RabbitMQ works just fine out of the gate, users with complex workflows, unique constraints, or tight budgets can take advantage of the flexibility of RabbitMQ to streamline their data pipelines and get the most out of their infrastructure.

This talk will provide an overview of RabbitMQ, review its varied message-routing capabilities, and demonstrate some of the ways in which these features can be utilized in Python applications to solve common yet difficult use-cases.

  [1]: https://www.rabbitmq.com/
  [2]: http://www.celeryproject.org/
",python,2018
"Projects fail in droves. Systems hiccup and hours of downtime follows. Screws fall out all the time; the world is an imperfect place.

We talk a lot about building resilient systems, but all systems are (at least for now) built by humans. Humans who have been making the same types of mistakes for thousands of years. 

Just because failure happens doesn’t mean we can’t do our best to prevent it or—at the very least—to minimize the damage when it does. As a matter of fact, embracing failure can be one of the best things you do for your system. Failure is a vital part of evolution. By learning to love failure we learn how to take the next step forward. Ignoring or punishing failure leads to stagnation and wasted potential.

This talk distills 3000 pages of failure research into 40 minutes of knowledge about the human factors of failure, how it can be recognised, and how you can work around it to create more resilient systems.

By the end of this talk the audience will have an awareness of the most common psychological reasons for mistakes and failures and how to develop systems and processes to protect against them.",python,2018
"All the data in the world is useless if you cannot understand it. EDA and data visualization are the most crucial yet overlooked stage in analytics process. This is because they give insights on the most relevant features in a particular data set required to build an accurate model. It is often said that the more the data, the better the model but sometimes, this can be counter-productive as more data can be a disadvantage. EDA helps avoid that.

EDA is useful for professionals while data visualization is useful for end-users. 

For end-users: 
A good sketch is better than a long speech. The value of a machine learning model is not known unless it is used to make data driven decisions. It is therefore necessary for data scientists to master the act of telling a story for their work to stay relevant. This is where data visualization is extremely useful.  
We must remember that the end-users of the results are not professionals like us but people who know little or nothing about data analysis. For effective communication of our analysis, there is need for a detailed yet simple data visualization because the work of a data scientist is not done if data-driven insights and decisions are not made.

For professionals:
How do you ensure you are ready to use machine learning algorithms in a project? How do you choose the most suitable algorithms for your data set? How do you define the feature variables that can potentially be used for machine learning? Most data scientists ask these questions.  EDA answers these questions explicitly.
Also, EDA helps in understanding the data. Understanding the data brings familiarity with the data, giving insights on the best models that fit the data set, the features in the dataset that will be useful for building an accurate machine learning model, making feature engineering an easy process.

In this talk, I will give a detailed explanation on what EDA and data visualization are and why they are very helpful in building accurate machine learning models for analytics as well as enhancing productivity and better understanding for clients. I will also discuss the risks of not mastering EDA and data visualization as a data scientist.",python,2018
"Congratulations on finishing your first tutorials or classes in python! In the parlance of the hero’s journey myth, you’ve had your ‘threshold moment”: you’ve started down a path that could lead to a long and fulfilling career. But the road to this glorious future is frustratingly obscured by a lack of guidance in the present. You know enough to realize that you don’t have all the skills you need yet, but it’s hard to know how to learn those skills, or even articulate what they are. There are no easy solutions to this problem. There are, however, a few fundamental things to know and advice to keep in mind. Drawing from my own experience and with input from others, I’ve compiled some helpful hints about the skills, tools, and guiding questions that will get you to mastery.",python,2018
"Python's cyclic garbage collector wonderfully hides the complexity of memory management from the programmer. But we pay the price in performance. Ever wondered how that works? In this talk, you'll learn how garbage collection is designed in Python, what the tradeoffs are and how Instagram battled copy-on-write memory issues by disabling the garbage collector entirely.

You'll also learn why that isn't such a great idea after all and how we ended up extending the garbage collector API which allowed us to (mostly) re-enable garbage collection. We'll discuss our upstream contributions to the garbage collector that landed in Python 3.6 and 3.7.

This is an in-depth talk about memory management but no prior experience with CPython internals is necessary to follow it.",python,2018
"Have you ever written a small, elegant application that couldn't keep up with the growth of your data or user demand? Did your beautiful design end up buried in threads and locks? Did Python's very special Global Interpreter Lock make all of this an exercise in futility?

This talk is for you! With the combined powers of AsyncIO and multiprocessing, we'll redesign an old multithreaded application limited by the GIL into a modern solution that scales with the demand using only the standard library. No prior AsyncIO or multiprocessing experience required.
",python,2018
"Concurrent programs are super useful: think of web apps juggling lots of simultaneous downloads and websocket connections, chat bots tracking multiple concurrent conversations, or web spiders fetching pages in parallel. But *writing* concurrent programs is complicated, intimidating to newcomers, and often challenging even for experts.

Does it have to be? Python is famous for being simple and straightforward; can Python make concurrent programming simple and straightforward too? I think so. By carefully analyzing usability pitfalls in other libraries, and taking advantage of new Python 3 features, I've come up with a new set of primitives that make it dramatically easier to write correct concurrent programs, and implemented them in a new library called [Trio](https://trio.readthedocs.io). In this talk, I'll describe these primitives, and demonstrate how to use them to implement a basic algorithm for speeding up TCP connections. Compared to the best previous Python implementation, our version turns out to be easier to understand, more correct, and dramatically shorter.

This talk assumes basic familiarity with Python, but does *not* require any prior experience with concurrency, async/await, or networking.
",python,2018
"You've heard about Python type annotations, but wondered if they're useful in the real world? Worried you've got too much code and can't afford to annotate it?  Type-checked Python is here, it's for real, and it can help you catch bugs and make your code easier to understand. Come learn from our experience gradually typing a million-LOC production Python application!

Type checking solves real world problems in production Python systems. We'll cover the benefits, how type checking in Python works, how to introduce it gradually and sustainably in a production Python application, and how to measure success and avoid common pitfalls. We'll even demonstrate how modern Python typechecking goes hand-in-hand with duck-typing! Join us for a deep dive into type-checked Python in the real world.",python,2018
"Many projects already take advantage of static analysis tools like flake8, PyLint, and MyPy. Can we do better? In this talk, I'll discuss how to take a type checker, bolt on an interprocedural static analyzer, and delight your security team with high quality results.

Abstract 

It is incredibly challenging to build a halfway decent static analysis tool for a dynamic language like Python. Fortunately, it gets quite a bit easier with Python type annotations. To explain why, I'll present a tool that finds security vulnerabilities by tracking dangerous flows of information interprocedurally across an entire codebase. **Then,** I'll demonstrate how that tool is really just a slightly slower, more sophisticated, type checker.

",python,2018
"When we talk about Web API Design, we're usually driven to think in architecture, verbs, and nouns. But we often forget our user: the developer.

UX designers rely on many techniques to create great experiences. User research, User Testing, Personas, Usage Data Analysis and others. However when creating `invisible products` we’re not used to think in usability. So why don’t we take advantage of this background to improve our APIs experiences?

",python,2018
"![ryu-python](http://www.thesimplelogic.com/wordpress/wp-content/uploads/2017/12/ryu-python.png)

Hear the story of how we used Python to build an AI that plays Super StreetFighter II on the Super NES. We'll cover how Python provided the key glue between the SNES emulator and AI, and how the AI was built with `gym`, `keras-rl` and `tensorflow`. We'll show examples of game play and training, and talk about which bot beat which bot in the bot-v-bot tournament we ran. 

After this talk you'll know how easy it is to use Python and Python's machine learning libraries to teach a computer to play games.  You'll see a practical example of the same type of machine learning used by AlphaGo, and also get to find out which character in StreetFighter II is best to pick when playing your friends.

  [1]: https://lh3.googleusercontent.com/Mh9uzCm4JeevMN5w-SWJgzWabrqOClAVMsa4jJtMRm-il1dP6oVTsRstJSQlbgKf4qh3A08yMZ36pwezsITA=w3230-h1786
  [2]: http://www.thesimplelogic.com/wordpress/wp-content/uploads/2017/12/ryu-python.png
",python,2018
"Recently, a new LED strip specification, APA102, has been released which allows these strips to be driven by a general purpose CPU instead of a dedicated microcontroller. This allows us the luxury of controlling them with Python!

I'll teach you about how to get the the hardware, how to think about programming for lights and how to build anything from a psychedelic art installation to home lighting to an educational tool. 

Programming with lights is awesome because you can SEE bugs with your eyes. I think the use of these LED's have great potential as a teaching tool because of the immediacy of the feedback.

LIVE hardware demos!  See Quicksort in brilliant colors!",python,2018
"Know you should be doing testing but haven’t gotten over the hurdle to learn it? pytest is Python’s modern, friendly, and powerful testing framework. When paired with an IDE, testing gets a visual interface, making it much easier to get started.

In this talk we cover “visual testing”: starting, learning, using, and mastering test-driven development (TDD) with the help of a nice UI. We’ll show PyCharm Community Edition, a free and open-source Python IDE, as a productive TDD environment for pytest. Specifically, we’ll show a workflow using pytest and PyCharm that helps make tests speed up development, or at the very least help to make testing seem less ""in the way"" of other development activities",python,2018
"How do you become a Python core developer? How can I become one? What is it like to be a Python core developer?

These are the questions I often receive ever since I became a Python core developer a year ago. Contributing to Python is a long journey that does not end when one earns the commit privilege. There are responsibilities to bear and expectations to live up to.

In the past year, I've been learning more about what it really means to be a Python core developer. Let me share all of that with you.
",python,2018
"Many of us practice test driven development, and pride ourselves in our code coverage. This is relatively easy to do when you begin a new project, but what happens when you take over an existing code base with little to no tests? Where and how do you start writing tests? This task can be very intimidating and frustrating, but can be accomplished!

This talk will run through some common approaches and methodologies for adding test coverage to pre-existing code (that you might not even be familiar with at all). The next time you take over an untested monolith,  you will be able to do the right thing and start writing tests instead of hoping for the best!",python,2018
"RESTful has been the go-to choice of API world. Why another API approach? To support more data-driven applications, to provide more flexibility and ease unnecessary code and calls, to address a wide variety of large-scale development problems, **GraphQL** comes with HTTP, JSON, Versioning, Nullability, Pagination, and Server-side Batching & Caching in mind to make API ""Simple yet Powerful"".

By applying [Graphene-Python](http://graphene-python.org/), a library for building GraphQL APIs in Python easily, this talk will go through the background and challenges of applying GraphQL as the new API service in a restaurant POS (point of sale) system within complex cloud infrastructure in Python. Introduction, testing, and live demo is included for sure.",python,2018
"Knowing how to code and being able to teach it are two separate skills. When we have expertise in a subject, it's common to take for granted that we'll be able to effectively communicate our expertise to someone else. Come learn (or re-learn!) how to teach and discover practical examples you can put to work right away.

By sharpening your teaching skills, you'll be a more effective mentor, trainer, and team member.",python,2018
"There is huge potential for R to accelerate scientific research, since it not only provides powerful analytical tools that increase reproducibility but also creates a new frontier for communication and publishing when combined with the open web. However, a fundamental shift is needed in scientific culture so that we value and prioritize data science, collaboration, and open practices, and provide training and support for our emerging scientific leaders. I will discuss my work to help catalyze this shift in environmental science through the Ocean Health Index project. Over the past six years, our team has dramatically improved how we work by building an analytical workflow with R that emphasizes communication and training, which has enabled over 20 groups around the world to build from our science and code for ocean management. R has been so transformative for our science, and we shared our path to better science in less time (Lowndes et al. 2017) to encourage others in the scientific community to embrace data science and do the same. Building from this, as a Mozilla Fellow I recently launched Openscapes to engage and empower scientists and help ignite this change more broadly.
",r,2019
"The ""shinyEventLogger"" package is a logging framework dedicated to complex shiny apps. Its main goal is to help to develop, debug, and analyze usage of our apps. Multiple-line events logging can be done simultaneously, not only to R console, but also to your browser JavaScript console, so you can see logged events in the real-time, using your app already deployed to a server (shinyapps.io, rsconnect). Moreover, your events (together with unique sesssionID and timestamp) can be saved as a text file or into a remote database (currently MongoDB), and be ready for further analysis with the help of process-mining techniques from ""bupaR"" package. You can log different types of events, for example:
 *  `log_event(""Hello World!"")`
 * `log_value(input$variable)`
 * `log_output(str(mtcars))`
 * `log_test(testthat::expect_true(TRUE))`
 * and others (errors, warnings, messages). 
You can time nested events for performance analysis. If you are logging a value of an evaluated expression, not only the value will be logged but also the expression itself. Each event can be logged with a list of parameters that are event-specific or common for a group of events. 
CRAN: https://cran.r-project.org/package=shinyEventLogger 
Vignette: https://kalimu.github.io/shinyEventLogger/articles/shinyEventLogger.html  
",r,2019
"We present mwshiny, a package that extends Shiny applications across multiple windows. Shiny lets R users develop interactive applications, alleviating the need for web development languages. Increasingly, users have access to multi-monitor system configurations; further, with Shiny apps hosted online, the possibility of a remote controller driving a visualization-focused monitor provides another necessity for a multi-window environment. Using mwshiny, users set up the interface and functionality of these multi-window applications by using Shiny's familiar syntax and conventions. By elegantly utilizing Shiny's reactive structure, we break down app development into a simple workflow with three parts: user interface development, server computation, and server output. We demonstrate this workflow through three case studies, including the aforementioned multi-monitor and controller-driving situations, in which we focus on population dynamics and cultural awareness, respectively. We also show mwshiny as applied to an immersive healthcare visualization; the Rensselaer Campfire, a new interface designed for group interaction, consists of two connected monitors in the form of a cylindrical fire pit, as well as an outside controller. These case studies show the impact of mwshiny as we move into a future with ever more immersive applications and structures.
",r,2019
"R Shiny has become overwhelmingly popular and widespread in the R community. Shiny web applications provide companies and individuals with an excellent way of demoing and showcasing data analytics and visualization, and interactive R-based projects in general. Although products and services exist for exposing Shiny apps on the web (Shiny Server, RStudio Connect, Shinyapps.io, ShinyProxy), there is a desire for and a clear benefit of full integration with an existing website. This allows providers to retain full flexibility in terms of customization and the experience provided to users exploring their apps. In this talk, we cover the challenges and requirements for the seamless embedding and integration of Shiny apps into a custom gallery part of a larger website. We will show how we used Docker and Kubernetes as natural choices for deploying and running Shiny apps, and for customizing the way they are served and made accessible through the web. We explain how to set up an embeddable Shiny app, how to build, maintain and deploy the corresponding Docker image, and how to configure the relevant Kubernetes resources. Finally, we will demonstrate how all this can easily be combined into a GitHub Pages website based on Jekyll.
",r,2019
"Experts are often consulted to quantify uncertainty on subjects when insufficient data is available, e.g. the temperature in Toulouse next week. The goal is to find a consensus by combining all experts' knowledge. We have built an R Shiny application to automate this procedure for an international governmental organization. 
The administration module allows to create a survey, invite experts and control the elicitation phase. When elicitation is completed, a report can be built automatically. This helps to find a consensus by plotting e.g. the combined distribution of all experts' elicitations. The administrator can, at any time, download the available data for use outside the Shiny application.
In the elicitation module, the assigned experts can define values for the requested quantiles in a table. The corresponding distribution is shown in an interactive barplot. By dragging the breakpoints in the barplot, the provided elicitation values in the table are automatically updated and vice-versa. 
During this talk, we will share our experiences on deployment with ShinyProxy. This allowed us to easily restrict the administration module to authorized users only and to guarantee persistent data storage using a docker volume.
",r,2019
"The goal of the tidyr package is to help get your data into a ""tidy"" form, where variables are found in columns and observations are found in rows. tidyr has two main drawbacks:
* Many people (including me!) find it hard to remember exactly how spread() and gather() work, and most usage require re-reading the documentation. This suggests there are fundamental problems with their design.
* Many web APIs provide data in JSON, which turns into deeply nested lists when loaded into R. tidyr provides few tools for tidying or rectangling this sort of data.
In this talk, I will introduce new tools in tidyr 1.0.0 that aim to solve both problems, making it easier to tackle traditional rectangular reshaping, as well as making it easier to rectangle deeply nested lists into a convenient form.
",r,2019
"dplyr, which provides tools for data summary and transformation, is one of the key user-facing packages of the tidyverse. dplyr is so powerful because each function is small and does one thing well. But this can also make it hard to learn, because it's not obvious what you can do, and there are often non-obvious tricks that can make your life much easier. In this talk, I'll summarise() a set of n() cool tricks, arrange(desc(NEWS)) to highlight some of the recent changes, and give you glimpse() of some of our thinking about the future.
",r,2019
"R is blessed with the very best data munging tools such as dplyr and data.table. However, R requires data to be loaded to RAM in the form of a data.frame; and a corollary of this is that R cannot deal with larger-than-RAM data easily. This talk introduces the disk.frame package which is designed to manipulate ""medium"" data - those datasets that are too large to fit into RAM but can be manipulated on a single machine with the right tools. In a nutshell, disk.frame makes use of two simple ideas to make larger-than-RAM data manipulation feasible * split up a larger-than-RAM dataset into chunks and store each chunk in a separate file inside a folder and
* provide a convenient API to manipulate these chunks Furthermore, disk.frame optimises on-disk data manipulation * by using state-of-the-art data storage format provided by the fst package to efficiently read and write data to disk
* by parallelizing operations with the excellent future package
* by using the data.table package's fast algorithms for grouping and merging Finally, disk.frame supports many dplyr-verbs to make it accessible to useRs who are already familiar with dplyr.
",r,2019
"The formula-data interface is a critical advantage of R in order to describe models to be estimated. However, more enhanced formula and data are often usefull. The new gdata.frame package tackle the case when the data set is characterized by two indexes. Two examples are panel data, for which observations are defined by an individual and a time period, and random utility models, for which observations are defined by a choice situation and an alternative. Moreover, indexes may have a nesting structure: for example, for a panel data of countries, the individual (country) index can be nested in a continent index. With gdata.frame, the indexes are stored as an attribute of the data.frame and extracted series inherit from it, thanks to specific methods for generic extractor functions. Usual operations, like for example group means or deviations from group means can then be computed in a very natural way, ie without having to define each time the series which defines the structure of the data set. gdata.frame therefore provides a generic solution to deal with grouped data. More specific data structure can then be defined that inherit from it. This feature is illustrated using the plm (for panel data) and the mlogit (for discrete choice models) packages.
",r,2019
"Base R has write.table() and read.table() to work with data in plain text files. Factors are stored as their labels, making them indistinguishable from characters and losing information on the levels. The git2rdata packages provides write_vc() which stores a dataframe as two plain text files: a tab separated file with the data and a metadata file in YAML format. The metadata stores the class of each variable and all other relevant metadata, e.g. the factor levels and their order. read_vc() reads the raw data using read.table() and restores the variables based on their metadata. Storing the metadata also allows to optimize the file storage, e.g. by storing factor indices rather than factor labels in the data file. This optimization can be turned of in case human readable data is preferred over smaller files. Git stores changes as row wise diffs. Swapping the order of two variables results in changing every single row of the plain text file. write_vc() avoids this be reordering the variables based on the existing metadata. Sorting the observations along user defined variables result in a stable row order. Metadata can be overridden if needed to accommodate a change in variables. git2rdata is useful to store and retrieve data in a reproducible workflow. Installation instructions, documentation and vignettes are available at https://inbo.github.io/git2rdata/
",r,2019
"Regression splines are piecewise polynomials constrained to join smoothly at boundaries called knots. They are traditionally viewed as an alternative to other methods for modeling nonlinear relationships, such as transformations, polynomial regression, and nonparametric regression. Regression splines are parametric and are implemented by introducing a regression-spline basis into the model matrix for a linear or similar regression model. It is usual not to focus on the estimated parameters for a regression spline but instead to represent the model graphically, and traditional regression-spline bases, such as B-splines and natural splines, respectively implemented in the bs() and ns() functions in the R splines package, are selected for numerical stability rather than interpretability. The emphasis on graphical interpretation makes sense but also represents a missed opportunity. We introduce generalized regression splines, implemented in the gspline() in the carEx package, which support the specification of a much wider variety of regression-spline and piecewise-polynomial models using bases that are associated with interpretable parameters.
",r,2019
"The nominal response model is an Item Response Theory (IRT) for polytomous items model that does not require a predetermined order of the response categories. While providing a very flexible modeling approach, it involves the estimation of many parameters at the risk of numerical instability and overfitting. The lasso is a technique widely used to achieve model selection and regularization. In this talk, we propose the use of a fused lasso penalty to group response categories and perform regularization. An adaptive version of the penalty is also considered. Simulation studies show that the proposal is quite effective in grouping the response categories, thus leading to a more parsimonious model. A remarkable advantage of the procedure is the reduction of both bias and root mean square error in small samples, while no difference is observed in large samples. An application to TIMSS data will illustrate the method. The R package regIRT (available at https://github.com/micbtz/regIRT) implements the methods.  
",r,2019
"The rise in availability of electronic health record data raises both challenges and opportunities for new and complex analyses. The merlin package in R provides an extended framework for the analysis of a range of data types, encompassing any number of outcomes of any type, each of which could be repeatedly measured (longitudinal), with any number of levels and with any number of random effects at each level. Many standard distributions are described, as well as non-standard user-defined non-linear models. The extension focuses on a complex linear predictor for each outcome model, allowing sharing and linking between outcome models in a flexible way, either by linking random effects directly, or the expected value of one outcome (or function of it) within the linear predictor of another. Non-linear and time-dependent effects are also seamlessly incorporated to the linear predictor through the use of splines or fractional polynomials. merlin allows level-specific random effect distributions and numerical integration techniques to improve usability, relaxing the normally distributed random effects assumption to allow multivariate t-distributed random effects. We will take a single dataset of patients with primary biliary cirrhosis and attempt to show the full range of capabilities of merlin.
",r,2019
"The talk illustrates the R package likelihoodAsy, available on CRAN and implementing some tools for higher-order likelihood inference. The basic functionality of the package is the implementation of the modified directed deviance for inference on a scalar parameter of interest, that could be seen as a fast method to approximate the most accurate parametric bootstrap inferences for the same task. The usage of the package requires to provide code for the likelihood function, for generating a sample from the model, and for the formulation of the interest parameter. The latter is allowed to be a rather general function of the model parameters. The code includes some functions for computing the modified profile likelihood for a multidimensional parameter of interest, and it could also be used to approximate median-unbiased estimation in a parametric statistical model. The features of the package will be illustrated by means of some examples on survival data models, IRT models and mixed models. 
",r,2019
"General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to statistical modelling ideally suited for hypothesis testing, forecasting and counterfactual analysis. The implementation of GETS-modelling, however, puts a large programming-burden on the user, and may require substantial computing power. We develop a framework for GETS-modelling with user-specified estimators and models, and provide flexible and computationally efficient functions via the R package gets (Pretis, Reade and Sucarrat (2018), J. of Statistical Software). In addition, the framework permits the user-specified estimation to be implemented in an external language (e.g.\ C++, Python, STATA, EViews, MATLAB, etc.).
",r,2019
"Water Resources Simulator, an object-oriented open-source software package based on R language, is developed for simulation of water resources systems based on Standard Operation Policy. In spite of numerous commercially available software packages for modeling and simulation of water resources systems, only a limited number of free and open source tools are available in this regard. This acted as the initiative for the development of WRSS which enables water resources engineers to study and assess water resources projects by modeling and simulation. This package provides users a number of functions and methods to build a model, manipulate its components, simulate the scenarios and publish and visualize the results in a water resource system study. WRSS is capable to incorporate various components of a large and complex supply-demand system as well as hydropower analysis for reservoirs since they have not been available in any other R packages. In addition, a particular coding system, devised for WRSS, allows water resources component to interact together by transferring mass in terms of seepage, leakage, spillage, and return flow. In addition to its capabilities, the package was successfully applied to a case study of a water resources system composed of 5 reservoirs and 11 demand sites and the results proved the role of WRSS in the enhancement of the dam operation under the large-scale model.
",r,2019
"Eco-hydrological models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, hillslope or watershed scale. However, with increasing computing power and observations available, bigger and bigger amounts of raw data are produced. Therefore, the need to develop flexible and user-oriented interfaces to visualize and analyze multiple outputs, e.g. performing sensitivity analyses, comparing and optimizing against observations (for specific research) or extraction of information (for data science), emerges. We present here the R open-source package **geotopbricks** (https://CRAN.R-project.org/package=geotopbricks), which offers an I/0 interface and R visualization and optimization tools for the GEOtop hydrological distributed model (https://www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins are shown.
",r,2019
"A typical gas fired power plant is equipped with sensors and instrumentation devices to capture key operating parameters of power plants. With a large fleet of power plants operating around the globe over >= 10 years , the volume of sensor data being captured enters the realm of big data and provides an opportunity to apply big data analytic techniques to mine valuable information for optimizing engineering design of power plants.
The often used ""worst case approach"" usually leads to large design margins and increased cost. With the advent of big data technology, low cost computation and memory, it is now possible to mine the sensor measurements to either validate or dispute worst case assumptions made during design. Therefore a core processing R kernel was developed performing statistical and data analysis on time series data like:   KDEs, pdfs, correlations,Time series   probability estimations in regions    conditional probability analysis   violin/box plots   Event Detection    An Shiny application has been developed integrating the core processing kernel and providing data analytic functionality available for design engineers like automatic :
- shortlisting/specification of senor names
- generatiion of fitting list of plants & units of our fleet
- download and post processing of time series data 
- GUI for data analytic techniques/visualations
",r,2019
"Anomaly detection problems have many different facets that lead to wide variations in problem formulations. At present, there is a fairly rich variety of R software packages supporting anomaly detection tasks using different analytical techniques. Some of these use an approach to anomaly detection based on a forecast distribution. We locate over 75 R packages with anomaly detection capabilities via a comprehensive online search. We first present a structured and comprehensive discussion on the functionality and capability of these publicly available R packages for anomaly detection. Despite the large number of packages available, there are some anomaly detection challenges that are not supported with existing packages. We reduce this gap by introducing three new R packages for anomaly detection, `oddstream`, `oddwater` and `stray`, with special reference to their capabilities, competitive features and target applications. Package `oddstream` introduces a framework that provides early detection of anomalous behaviours within a large collection of streaming time series. This includes a novel approach that adapts to non-stationarity in the time series. Package `oddwater` provides a framework for early detection of outliers in water-quality data from in situ sensors caused by technical issues. Package `stray` provides a framework to detect anomalies in high dimensional data.
",r,2019
"Anomaly detection is a topic of considerable importance and has been subject to increasing attention in recent years. This is certainly true within the R community, as evidenced by the number of packages hosted by CRAN related to this area.
The majority of packages contain approaches aimed at detecting point anomalies i.e. single observations, which are anomalous with regards to their data context. We introduce the anomaly package, which contains novel methodology aimed at detecting not only point anomalies but also collective anomalies, i.e., anomalous data segments. Importantly, the main algorithm of the package is capable of simultaneously detecting and distinguishing between both anomaly types. The utility of this approach is demonstrated on both simulated data, and real astrophysical data from NASA's Kepler telescope, which is included within the package.
",r,2019
"Only 35% of high schools in the US offer computer science (CS). However, by 2020 50% of all new STEM jobs will require some knowledge in CS and coding. Developing computational thinking (CT) across all disciplines and educational levels have become a priority for scholars and agencies, urging to integrate CT across K-12 curricula. CT can be better learned when blended to primary subject areas. CodeR4MATH is an NSF-funded project working to provide a collection of learning and tutoring resources for math educators to collaborate in an R environment. Linking math to real-life problem solving, math modeling is an iterative process involving situation representation, math operations, interpretation, and validation. The modules are being developed to be used in high school math classes in Stats, Algebra I and II. We present here the results from the first module (named Lifehacking) implementation, showing students greatly benefit from investigating meaningful problems that appeal to their personal interests. It guides students to model in R to answer practical questions such as the costs of eating in college or the costs of owning a car. Tasks are delivered via R tutorials using learnr tools, with students using real data to create their models. They are then guided to move to RStudio exploring its capabilities. The 2nd and 3rd modules are under development, focusing on Earth Sciences and Engineering.
",r,2019
"Of the many coding puzzles on the web, few focus on the programming skills needed for handling untidy data. During my summer internship at RStudio, I worked with Jenny Bryan to develop a series of data science puzzles known as the ""Tidies of March."" These puzzles isolate data wrangling tasks into bite-sized pieces to nurture core data science skills such as importing, reshaping, and summarizing data. We also provide access to puzzles and puzzle data directly in R through an accompanying Tidies of March package. I will show how this package models best practices for both data wrangling and project management.
",r,2019
"The University of Western Australia's Centre for Applied Statistics has for many years offered short courses for higher degree research students and industry-based professionals. In 2019 we have released stage one of our new programme of courses, all of which are based in R using the tidyverse approach. This presentation will describe our experience of moving from courses in base R to a tidyverse approach, highlighting advantages and disadvantages for us as educators and our students as learners. We will also describe our current programme, planned future courses and the pathways for participants based on their statistical background.
",r,2019
"In this talk we will present the details of the ghclass package which we have developed as a tool for managing statistical and data science courses with a computation component. The package is designed to automate the process of distributing and collecting assignments via git repositories hosted on GitHub. As part of this talk will discuss best practices for how these tools can be deployed in a variety of classroom settings from first year introductory courses through graduate level courses. Finally we will discuss how these tools fit into the larger context of modern statistical / data science pedagogy with a particular emphasis on the role of reproducibility in training new researchers.
",r,2019
"Data Science in a Box (datasciencebox.org) is an open-source project that aims to equip educators with concrete information on content and infrastructure for designing and painlessly running a semester-long modern introductory data science course with R. In this talk we outline five guiding pedagogical priniples that underlie the choice of topics and concepts introduced in the course as well as their ordering, highlight a sample of examples and assignments that demonstrate how the pedagogy is put into action, introduce `dsbox` -- the companion R package for datasets used in the course, and share sample student work and feedback. We will also walk through a quick start guide for faculty interested in using all or some of these resources in their teaching.
",r,2019
"The clustering of datasets is of paramount interest in multivariate data analysis. In presence of several datasets which pertain to the same individuals but not necessarily the same variables, CLUSTATIS method (Llobell, Cariou, Vigneau, Labenne & Qannari, 2018) operates a cluster analysis of these datasets. This method stands as the core of ClustBlock package. CLUSTATIS strategy consists of a hierarchical algorithm followed by a partitioning algorithm, and yields graphical displays and indices to assess the quality of the solution. A noise cluster option can be activated, with the aim of setting aside atypical datasets. ClustBlock includes specifics functions to perform CLUSTATIS with data from Free Sorting task. An adaptation of CLUSTATIS to the data from a Check All That Apply task, called CLUSCATA (Llobell, Cariou, Vigneau, Labenne & Qannari, 2019), is also available. References Llobell, F., Cariou, V., Vigneau, E., Labenne, A. & Qannari, E. M. (2018). Analysis and clustering of multiblock datasets by means of the STATIS and CLUSTATIS methods. Application to sensometrics. Food Quality and Preference. Llobell, F., Cariou, V., Vigneau, E., Labenne, A., & Qannari, E. M. (2019). A new approach for the analysis of data and the clustering of subjects in a CATA experiment. Food Quality and Preference, 72, 31-39.
",r,2019
"Ordinal data are a specific kind of categorical data occurring when the levels are ordered. They are used in a lot of domains, specifically when measurements are collected from persons by observations, testings, or questionnaires. ordinalClust is an R package that proposes efficient tools for modeling, clustering, co-clustering, and classification for ordinal data. The data are modeled through the BOS distribution, a gaussian-like distribution parameterized with a position and a precision parameter. On one hand, the co-clustering framework uses the Latent Block Model (LBM) and an SEM-Gibbs algorithm for the parameters inference. On the other hand, the clustering and the classification methods follow on from simplified versions of this algorithm. 
",r,2019
"The emergence of numerical sensors in many aspects of everyday life leads to the collection of high frequency data. For example in sports, athletes wear devices that collect simultaneously several variables during their training to follow their physical constants. This kind of data can be classified as multivariate functional data. To ease the understanding of those data, there is an increasing need of methods to analyze multivariate functional data. This work presents an R package that provides a clustering technique (Schmutz et al, 2018) that eases the modeling and comprehension of those multivariate functional data. This method, named funHDDC, is based on a functional latent mixture model which allows the partition of data into homogeneous clusters, and an EM algorithm for model inference. In addition to clustering algorithm, the package provides model selection criteria for choosing the number of clusters, and allows the execution of principal component analysis for multivariate functional data. The package usage will be shown on several practical examples whose an original example of horse speed prediction.
",r,2019
"The curse of dimensionality is a commonly encountered problem in statistics and data analysis. Variable sensitivity analysis methods are a well studied and established set of tools designed to overcome these sorts of problems. In most cases, those methods require a functional or computer code producing the output variable. Also, they fail to capture relevant features and patterns hidden within the geometry of the enveloping manifold projected onto a variable. In this talk we propose an index that captures, reflects and correlates the relevance of distinct variables within a model by focusing on the geometry of their projections. We first construct the 2-simplices of a Vietoris-Rips complex and then estimate the area of those objects from a data-set cloud. Afterwards, through affine transformations we propose our geometric sensitivity index. As a side result, we could also estimate the regression curve of the data-set generate solely by the estimated manifold. We compile all the analysis in an original package called TopSA (https://github.com/maikol-solis/TopSA), short for Topological Sensitivity Analysis. The package exploits the facility of the package `simple features` (sf) to handle all the geometric operations. The package returns the estimated indexes and has implemented a plot function using `ggplot` together with `geom_sf`.
",r,2019
"This talk reviews our recent work in the development of visualization methods in R for understanding and interpreting multivariate linear models (MLMs).
We begin with a description and examples of the HE plots framework, utilizing the heplots package, wherein multivariate tests can be visualized via ellipsoids in 2D, 3D. HE plots provide visual tests of significance: a term is significant by Roy's test if and only if its H ellipsoid projects somewhere outside the E ellipsoid. 
These graphical methods apply equally to all multivariate designs: MMRA, MANOVA, and MANCOVA, and the test of any linear hypothesis can also be displayed in an HE plot. 
When the rank of the hypothesis matrix for a term is 2 or more, these effects can also be visualized in a reduced-rank canonical space via the candisc package, which also provides new data plots for canonical correlation problems. These plots quite often provide a very simple description of differences in means among groups in a MANOVA setting, by displaying the projections of response variables as vectors in canonical space. We also describe some recent extensions of these ideas:
* We extend the visualization methods to robust MLMs.
* Influence measures and diagnostic plots for MLMs have now been implemented in the mvinfluence package.
* We develop visual tests of equality of covariance matrices for MANOVA.
",r,2019
"Shiny has established itself as an essential communication tool in the datascience field, and has gone beyond the scope of simple R users. Its ease of use makes it possible to produce ""quick and dirty"" applications. However, as soon as we try to go beyond the proof-of-concept and produce applications with professional vocation, some development and deployment aspects must be taken into account. Designing a shiny application that is maintainable over time, scalable, tested, robust and deployable is not an easy task. And like everything R-related, there are more wrong ways than right ways to do so.
With the {golem} package, we suggest a development template that relies on a R package. It contains a series of tools that allows you to add functionalities to your application for both development and deployment. For example `golem::add_module()` allows to add a Shiny module, `golem::use_favicon()` allows to add a favicon to the application. In addition, functions such as `golem::add_rconnect_file()` or `golem::gen_dockerfile()` allows you to easily deploy your application on RSconnect or shinyproxy. 
{golem} is an opinionated template designed to help easy development, deployment and maintainance inside a documented R package.
",r,2019
"Creating one-off shiny applications is easy to do, but what happens when you need to maintain an application over a longer period of time? How do you introduce new application features to an active user-base without disrupting their experience? These can be difficult questions to answer for data scientists who are unfamiliar with the basic principles of software and web application deployments and release cycles. I'll introduce a few core concepts used in the software development world: environment-based and application-based release patterns, and talk about how we can map those techniques to simpler Shiny application maintenance. Finally, I'll demo a Shiny application that leverages `session$user` data to mimic feature toggles for the roll-out of a new feature to a set of users.
",r,2019
"Shiny, alongside packages like dplyr and ggplot2, offers an unparalleled developer experience for creating self-service analytics dashboards that empower teams to make data-driven decisions. However, out of the box, Shiny is not well-suited to deployment in a multi-user environment. As part of our mission to establish a data culture in a game development studio, we wanted to deploy a suite of Shiny dashboards such that exploring player behaviour became part of every team's workflow. In this talk, we will discuss the architecture of the supporting cloud infrastructure, including packaging, service orchestration, and authentication. Also, we will show how we've adapted Shiny to a multi-user environment using its new support for promises in combination with the future package. Integrating Shiny into this production-grade architecture allows for a streamlined data science workflow that enables data scientists to focus on creating dashboard content with a built-in code review process, and also to deploy changes to production in a button click. We hope to demonstrate how any data-driven organisation can augment their team-wide workflow by leveraging this end-to-end Shiny pipeline.
",r,2019
"Shiny is conquering the enterprise world. Shiny apps are now built for hundreds of users who need reliability and functionality.
Having built and scaled a number of Shiny applications in production, we've learned what's important for a Shiny app to become a stunning success. We'd like to share our experiences with you.
This presentation will give you a deep dive into the best practices for building Shiny production apps, including:
* How to design app architecture for optimal maintainability and development velocity.
* Approaches for avoiding errors in production.
* Test pyramid: why it pays off to have automated end-to-end tests and unit tests, and how to minimize manual testing.
* How to scale Shiny apps to hundreds of users and set up automated performance testing.
* Deployment strategies and the path from deployment, involving many manual steps, to full automation.
Shiny allows you to rapidly iterate on functionality and build impressive dashboards, but it's not easy to do a great job getting that through to production. In this talk, we will discuss what specific steps you can take to achieve reliability without sacrificing speed of development.
",r,2019
"At Plant and Food Research we constantly aim to improve to quality and health properties of various fruit and vegetables. Genomic prediction is the science that aims to predict certain fruit and vegetable characteristics based on its DNA. For example, can we use the DNA of an apple to predict if it will taste sweet or has a high vitamin C concentration? Ultimately, can we use the DNA to predict if a consumer might say ""this apple is tasty"" or ""I like the beneficial health effects""? To tackle this daunting problem of linking a plant's DNA to a consumer's spoken word, we need to know everything about the plant, the consumer, and all the steps in between. In this presentation I will discuss the challenges we have to overcome and how R and AI has helped us with genomic prediction by collecting vital information at higher volumes and accuracy that was not possible before. Recent examples are an app we developed that automates sentiment analysis of consumer data, image recognition tools to monitor plants and deep learning methods to evaluate fruit performance in storage rooms.
",r,2019
"Introduction Hospitals face widely varying care demands intensifying nurses' work, since the number of admissions, discharges and transfers within and between units is fluctuating throughout the day. The objective was to describe patients' movements for a large Swiss University Hospital.    Methods Data from 2015 to 2017 were used containing patients' movements time and dates. With padr package, continuous time was extracted between each movement of the patient for an interval of 30 minutes. The first and last movements were recognized as admission and discharge, respectively. For the transfers, the use of the functions lead() and lag() from dplyr package were used. The goal was to identify for each unit if the patient was coming in or out. Finally, the amount of entries (admissions and transfers in) was plotted against exits (discharges and transfers out) on a pyramid bar chart using ggplot2 and scales packages.   Results Ten departments and 77 units were analyzed with 85,706 patients' observations. The final data comprised of more than 26 million data points. Patient flow varies considerably. While for some units entries and exits occur at the same time, other units have volatile periods with different times of the day where entries and exits occur leading to substantially variation in patient load.   Conclusion It is the first analysis of patient movements conducted on this level of granularity.
",r,2019
"Agent-based modelling (ABM) is a bottom-up simulation for simulating actions and interactions between agents. This approach it is particularly useful when modelling human-environment interactions, such as pedestrian's cumulative exposure levels related to travel patterns. Until recently, possible ways to report results were either to bring the text file after simulating the model that is tedious and time-consuming, or to use packages where the codes can only access the ABM platform, which slows the simulation speed (especially for geospatial models). Here, we use a newly released nlrx package that improves the modelling speed, iterates multiple jobs, and provides algorithms for temporal sequence plotting. From our pre-built pedestrian exposure model of Gangnam, Seoul, we demonstrate sequences of pedestrian's health status in every 100 ticks using gganimate, and plot risk population change in geographical hierarchies. Finally, we will discuss our efforts to encourage geographers and modellers to use R as a platform to conduct ABM studies.
",r,2019
"Recently, machine learning algorithms are used popularly in the engineering field, and simulation of the specific situation becomes one of the important processes. In this talk, we propose an environment for simulation of the physical movement in robots with R. We carried out the optimization of robot gait which is a major issue in the robotics area. As an optimization algorithm, GA (Genetic Algorithm) was used. In our physical simulation, we considered two options. The first option is about position and velocity when forces are applied to the body. We solved this dynamic equation of a robot's body placed in 2-dimensional space using the 3rd order Runge-Kutta method. The second option is for constraining the position and velocity when a body contacts the ground. The sequences of the robot's body position were visualized with animation package in R. This virtual robot has a body, and five parameters determine its walking trajectory. GA package in R was used to optimize these parameters. We successfully get some values that enable robots to walk steady and fast. Through this study, we expect simulation in robotics engineering area can be conducted with R as well.
",r,2019
"Open-ended interviews are common approaches in social sciences for catching the respondents' worldviews, feeling and knowledge. Their popularity is growing in natural science, particularily for studying social-ecological systems like farms, forests or fisheries. In these systems, decision-making processes and practices are no easily taken into account by classical experimental approaches.  
We developed an original approach that aims to visualize and analyse perceptions, knowledge and practices of managers in social-ecological systems. Based on the usage of R, we contributed to and developed two Shiny Applications that aims to (i) qualitatively code textual documents, like transcribed open-ended interviews ('qcoder') and to (ii) treat the outputs of the qualitative coding in order to draw cognitive maps ('cogmapr'). Cognitive maps are digraphs of variables that can be used as a semi-qualitative model of interviewees' worldviews. The ""cogmapr"" tool contains major functions from our ""Cognitive Mapping Approach for Analysing Actors' Systems of Practices"" : drawing Individual Cognitive Maps, computing Social Cognitive Maps and main indicators from the graph theory.  
The presentation will be supported by case studies from various agroecological systems : grassland, forest in Belgium and Romania, crop diversification in Europe, soil management in Québec.
",r,2019
"We present choicetools, a new package to work with data from conjoint analysis and best-worst (aka MaxDiff) stated choice surveys. Users may supply their own choice data, or import data from two popular survey platforms, Qualtrics (best-worst surveys) and Sawtooth Software Lighthouse Studio (conjoint and best-worst). choicetools estimates classical and hierarchical Bayesian models, performs preference share market simulation, and plots aggregate and individual-level estimates. It is also useful to teach conjoint analysis; it provides functions to create and field a survey in CSV format to be completed in a spreadsheet application in a classroom setting, and to estimate preferences from the responses. In our UseR! talk, we demonstrate a vignette style choice-based conjoint project using simulated responses, including survey specification, spreadsheet-style presentation, estimation, and plotting. We briefly discuss experimental features for attribute importance, and answer audience questions. This talk will be of interest to research practitioners in marketing, transportation, and other fields, and to academics who teach discrete choice and conjoint analysis methods.
",r,2019
"Mediation analysis is one of the most widely used statistical techniques in the social and behavioral sciences. In its simplest form, a mediation model allows to study how an independent variable (X) affects a dependent variable (Y) through an intervening variable that is called a mediator (M). Such an analysis is often carried out via a pair of regression models, in which case the indirect effect of X on Y through M can be computed as a product of coefficients from those regression models. The standard test for this indirect effect is a bootstrap test based on ordinary least squares (OLS) regressions. However, this test is very sensitive, e.g., to outliers or heavy tails, which poses a serious threat to empirical testing of theory about mediation mechanisms. The R package robmed implements a robust test for mediation analysis based on the fast and robust bootstrap methodology for robust regression estimators. This procedure yields reliable results for estimating the effect size and assessing its significance, even when the data deviate from the usual normality assumptions. In addition to simple mediation models, the package also provides functionality for mediation models with multiple mediators as well as control variables. Furthermore, the standard bootstrap test and other proposals are included in the package. We will demonstrate the use of package robmed in an example from management research.
",r,2019
"Process mining is an innovative research field aimed at extracting useful information about business processes from event data. An important task herein is process discovery; the discovery of process models from data. The results of process discovery are mainly deterministic process models, which do not convey a notion of probability or uncertainty. In this paper, Bayesian inference and Markov Chain Monte Carlo is used to build a statistical model on top of a process model using event data, which is able to generate probability distributions for choices in a process' control-flow. A generic algorithm to build such a model is presented, and it is shown how the resulting statistical model can be used to test different kinds of hypotheses, such as non-deterministic dependencies between different choices in the model. The algorithm is implemented in a new R package, named propro, for probabilistic process models. In this paper, it is shown how propro can be used in real-life process analysis scenarios and leads to valuable information about the processes under consideration, which go beyond the discovery of static control-flow. As a result, propro supports the enhancement of discovered process models by exposing probabilistic dependencies, and allows to compare the quality among different models, each of which provides important advancements in the field of process mining.
",r,2019
"The advent of biologging devices has led to the collection of ever-increasing quantities of tracking data from animals and humans. In parallel, sophisticated tools to process, visualize and analyze tracking data have been developed in abundance. Within the R software, we listed 57 packages focused on these tasks, called here tracking packages. Here we review and describe each package based on a workflow centered around tracking data, broken down in three stages: pre-processing, post-processing and analysis (data visualization, track description, path reconstruction, behavioral pattern identification, space use characterization, trajectory simulation and others). Links between packages are assessed through a network graph analysis and show that one third of the packages worked on isolation, reflecting a fragmentation in the R movement-ecology programming community. Finally, we provide recommendations for users to choose packages and for developers to maximize the usefulness of their contribution and strengthen the links between the programming community.
",r,2019
"Object tracking has been recently considered drastically to get insight into the behavior of moving objects within different contexts such as eye tracking, animal tracking, traffic analysis, city management, sports, etc. This, however, results in a huge amount of irregular data which are usually not well structured, depending on the tracking methods and the aim of the study. In this work, we review different classes and methods from R package trajectories with the aim of structuring, handling and summarizing movement data. Simulation techniques and model fitting are also studied. We further proceed with some statistical characteristics based on spatial point processes such as first- and second-order summary statistics to analyze the behavior of objects over time in terms of distribution and pairwise interaction. We finally demonstrate our findings of a taxi movement data from Beijing, China.
",r,2019
"We present an R implementation of spatial interaction models. Flow data represent movements of people or goods between two spatial locations, for example in migration, international trade, transportation. Gravity models, which have been extensively used for this purpose, include a function of distance between origin and destination among the explanatory variables to account for the spatial dimension. To further eliminate the spatial structure present in this type of data, we implement two fitting methods for spatial autoregressive models accounting for spatial dependence between flows: a Bayesian approach and a three stage least squares approach. We discuss impacts measures evaluation as well and extend these computations to the case of different characteristics at origin and destination. Contrary to existing programs, our implementation with the R language also allows for a different list for origins and destinations. We illustrate with an application to air transportation data.   LeSage, J. P., and Thomas-Agnan, C. (2015). Interpreting spatial econometricorigin-destination flow models. Journal of Regional Science, 55(2),188-208. Margaretic, P., Thomas-Agnan, C., & Doucet, R. (2017). Spatial dependence in (origin-destination) air passenger flows. Papers in Regional Science, 96(2), 357-380.
",r,2019
"Since the first release of R on CRAN, in 1997, its use in many fields has grown rapidly. Lai et al. (2019), for example, suggest that more than 50% of research articles published in Ecology use R in some way. Much like many ecological datasets, transport data tend to be large, diverse and have spatial and temporal attributes. Unlike Ecology, Transport Planning has been a slow adopter of R, with a much lower percentage of papers using the language. This raises the question: why? After exploring this question, in relation to dominant transport planning software products, the talk will sketch of what an open source transport planning 'ecosystem' could look like. Based on my own experience, of developing the stplanr package and teaching practitioners, the talk will discuss the importance of building ""communities of practice"", for transport planners making the switch to R. These observations relate to others promoting R in new environments, and link to the wider question of how to advocate for open source software in wider society. Lai, J., Lortie, C.J., Muenchen, R.A., Yang, J., Ma, K., 2019. Evaluating the popularity of R in ecology. Ecosphere 10.
",r,2019
"The flextable package provides an interface to generate tables for publication and corporate reporting. Originally designed to work with the package officer, it has evolved this year to get compatible with the R Markdown format.
The package enables simple and complex tabular reporting composition thanks to a user-friendly grammar. It supports output to HTML, Word, PowerPoint and recently PDF through the pagedown package.
In this talk I will introduce the main concepts of the package and demonstrate them with simples examples. I will show how to manage the layouts, how to format the content and mix images with text. Finally, I will expose a concrete implementation inside an R Markdown report and a Shiny application.
",r,2019
"Although R Markdown can render documents to Microsoft Word, R/R Markdown users must sometimes transcribe statistical content in to separate Microsoft Word documents (e.g., documents drafted by colleagues in Word, or documents that must be prepared in Word), a process that is error prone, irreproducible, and inefficient. We will present StatTag (www.stattag.org), an open source, free, and user-friendly program we developed to address this problem. StatTag establishes a bi-directional link between R/R Markdown files and a Word document, and supports a reproducible pipeline even when: (1) statistical results must be included and updated in Word documents that were never generated from Markdown;  and (2) text in Word files generated from R/R Markdown has departed substantially from original Markdown content, for example through tracked changes or comments. We will demonstrate how to use StatTag to connect R/R Markdown and Word files so that all files can be edited separately, but statistical content – values, tables, figures, and verbatim output – can be updated automatically in Word. Using practical examples, we will also illustrate how to use StatTag to view, edit, and rerun R/R Markdown code directly from Word. 
",r,2019
"Analysis, R programs or packages are easier to use when correctly documented. However, documentation is perceived time consuming and is the poor child of development projects. We assume that any kind of R project (data analysis, R package, shiny applications) can embrace the literate programming paradigm: explanation of the program logic in natural langage interlaced with code snippets. Hence, Rmarkdown files are ideal candidates for reproducible projects and workflows. The ""Rmd first"" approach proposes a project in four parts: (1) Prototype: keep track of data explorations, graphs and tables using Rmd files. (2) Package: regularly transform code chunks into functions to simplify and clarify reading. Extracted functions are documented and tested. The Rmd file ends as the vignette of the package. (3) Modularize: separate projects into multiple sub-parts, hence Rmd files. It simplifies participation of multiple developers by reducing potential files conflicts. (4) Deploy: use available tools to share your work, confidentially or publicly. In a vignette documented package, automated report may be built with {bookdown}, a website may present the full documentation with {pkgdown}. This approach allows developers to explain their process during development. At low cost, documentation is already available, the analysis is directly useable and reproducible, the report already written.
",r,2019
"Is there a crisis in reproducible research? Some studies such as Ioannidis et. el (2009) have estimated that over fifty percent of published papers in some fields of research are not reproducible. In statistical and data analysis projects, this appears to be due to lack of training and poor tools rather than scientific fraud.  
In the R community, there is a move towards Don't Repeat Yourself (DRY) approaches to reproducible research and reporting. Employing computing tools like GNU Make to regenerate output when syntax or data files change, GNU git for version control, writing GNU R functions or packages for repetitive tasks and R Markdown for reporting can greatly improve reproducibility.  
The gnumaker package is ideal for statisticians and data analysts with little experience of Make. By specifying the relationships between syntax and data files gnumaker makes it easy to create and plot GNU Makefiles as DAGs. Gnumaker employs Make pattern rules for R, Sweave, R Markdown, Stata, SAS, etc outlined in P Baker (2019) Using GNU Make to Manage the Workflow of Data Analysis Projects, Journal of Statistical Software (Accepted). See https://github.com/petebaker
",r,2019
"The paradigm of the current revolution in biomedical studies, and life-sciences in general, is to focus on a better understanding of biological processes. In the early stages of drug development, different types of information on the compounds are collected: the chemical structures of the molecules, the predicted targets (target predictions), various bioassays, the toxicity and more. An analysis of each data set could reveal interesting yet disjoint information. An important task is the integration of the data sets from the experiments to understand the working mechanism of the drug. Multi-source clustering methods aim to discover groups of objects that are consistently similar across data sets. We introduce a new multi-source clustering method, M-ABC, which applies a consensus function generating a clustering of clusters. This can improve the quality of individual clustering algorithms. The proposed method is implemented and publicly available in the R package IntClust which is a wrapper package for a multitude of ensemble clustering methods. In addition, visualization for a comparison of clustering results and cluster specific secondary analyses such as differential gene expression and pathway analysis are available.
",r,2019
"We describe a new algorithm and R package for peak detection in genomic data sets using constrained optimal changepoint algorithms. These detect changes from background to peak regions by imposing the constraint that the mean should alternately increase then decrease. An existing algorithm for this problem exists, and gives state-of-the-art accuracy results, but it is computationally expensive when the number of changes is large. We propose an algorithm with empirical O(N log N) time complexity that jointly estimates the number of peaks and their locations by globally minimizing a non-convex penalized cost function. We also propose a sequential search algorithm that finds the best solution with K segments in O(N log(N) log(K)) time, which is much faster than the previous O(K N log N) algorithm. Our empirical results show that our disk-based implementation in the PeakSegDisk R package can be used to quickly compute constrained optimal models with many changepoints, which are needed to analyze typical genomic data sets that have tens of millions of observations. 
",r,2019
"The Roadmap Epigenomics consortium has gathered and produced abundant epigenomic data in human cell lines and tissues to build reference epigenetic maps. The data is freely available, and is exploitable through web browsers. It has been used by the consortium to classify chromatin into ""states"" that can be more easily compared across cell type. Here we systematically assessed the links between epigenetic marks and transcription by generating ""stacked profiles"" of epigenetic marks at transcription start sites (TSS), transcription end sites (TES) and middle exons, sorted according to expression levels or exon inclusion ratio, for each cell type and tissue in the dataset. The thousands of attractive visualisations generated are made easily browsable through a web application, www.perepigenomics.roslin.ed.ac.uk. We also investigated how change of epigenetic marks across cells were correlated to change of expression level / inclusion ratio, allowing us to build a comprehensive understanding of the associations between epigenetic marks at TSS, TES and middle exons and expression levels or exon inclusion ratio.
",r,2019
"Dose-response modeling is a crucial step in drug discovery and safety assessment. R packages like ""drc"" and ""DoseFinding"" provide useful tools to fit dose-response models and estimate parameters such as effective doses. When it comes to modeling dose-response patterns of several compounds (e.g., in high content screening studies), one may face with three different challenges:  1. the dose-response relationship may be non-existent (flat patterns),  2. no single dose-response model fits the data well enough,  3. due to the use of often complex non-linear models the estimation could become computationally intensive (especially in a high-throughput setting).  To address these issues, we have developed an R package called 'clustDRM` that provides a unified platform to analyze dose-response relationships in a high-throughput setting. The package uses a two-stage approach. First, it filters out the compounds with a flat dose-response pattern and identifies the patterns of the non-flat ones. Next, it fits a set of appropriate dose-response models (accounting for the previously identified patterns) and uses model-averaging to estimate effective doses and their standard errors. Parallel computations are used to address the third issue. Furthermore, a Shiny app accompanies the package to make its use easier for scientists from various fields without deep knowledge of R.
",r,2019
"In many application settings, the data have missing features which make data analysis challenging. An abundant literature addresses missing data as well as more than 150 R packages. Funded by the R consortium, we have created the R-miss-tastic plateform along with a dedicated task view which aims at giving an overview of main references, contributors, tutorials to offer users keys to analyse their data. This plateform highlights that this is an active field of work and that as usual different problems requires designing dedicated methods.
In this presentation, I will share my experience on the topic. I will start by the inferential framework, where the aim is to estimate at best the parameters and their variance in the presence of missing data. Last multiple imputation methods have focused on taking into account the heterogeneity of the data (multi-sources with variables of different natures, etc.). Then I will present recent results in a supervised-learning setting. A striking one is that the widely-used method of imputing with the mean prior to learning can be consistent. That such a simple approach can be relevant may have important consequences in practice.
",r,2019
"Since its introduction in 2012, Shiny has become a mainstay of the R ecosystem, providing a solid foundation for building interactive artifacts of all kinds. But that interactivity has always come at a significant cost to reproducibility, as actions performed in a Shiny app are not easily captured for later analysis and replication. In 2016, Shiny gained a ""bookmarkable state"" feature that makes it possible to snapshot and restore application state via URL. But this feature, though useful, doesn't completely solve the reproducibility problem, as the actual program logic is still locked behind a user interface.
In this talk, I'll discuss some of the approaches that app authors have taken to achieve these ends, along with some surprising and exciting approaches that have recently emerged. These new approaches usefully decrease the implementation effort and code duplication, and may eventually become essential tools for those who wish to combine interactivity with reproducibility.
",r,2019
"Reproducible documentation and the use of RMarkdown is becoming more prevalent in the academic world. Packages built on RMarkdown such as posterdown, xaringan, pagedown, and rticles allow for most aspects of a typical research project to be produced; however, newcomers to RMarkdown can be put off or discouraged by the inconsistencies in which each type of document needs to be formatted in the '.Rmd' file. This talk will provide insight into the benefits and downfalls of fully customizable RMarkdown packages with emphasis on (1) the importance of a timely workflow for the short duration of an average graduate student program; (2) introducing students or supervisors who are new to RMarkdown, and (3) show the importance of reproducibility in every stage of a good research project. Concepts from this presentation intend to spark conversation and show the importance of reproducible document generation in all stages of a research project.  
",r,2019
"Large research groups commonly employ a biostatistician to work across their portfolio of research projects, however, this is not feasible for many research active clinicians and ""low-profile/establishing"" research groups who often struggle to access biostatistical support. Our group offers ""low-barrier"" initial consultations for analysis, data management and database development on a ""per-project"" basis that is attractive to the local health research community as it makes biostatistical expertise affordable and easily accessible. To facilitate our workflow, we have developed and refined a template project (skeleton) in R (using the ""ProjectTemplate"" package) that, along with version control systems, conforms to the principles of reproducible research. We have also developed R markdown templates to produce documents following our Institute's style guide. This approach allows us to streamline our workflow to expeditiously initiate projects and produce professional looking reports in multiple formats directly from the analysis package without wasting time on the non-analytical aspects of our projects; this approach is identical and successful for both simple and large-scale projects.
",r,2019
"Applying security best practices is essential not only for developers or sensitive data storage but also for the everyday R user installing R packages, contributing to open source, working with APIs or remote servers. However, keeping up-to-date with security best practices and applying them meticulously requires significant effort and is difficult without expert knowledge.    The goal of the R package ropsec (github.com/ropenscilabs/ropsec) is to bring some of the best practices closer to all R users and enabling them to add a few more layers of security to their personal workstation and shared work.    In this talk I will focus on signing commits: why you should do it and how ropsec helps you do it the right way with the lowest possible risk of making a mess of your settings. I will also highlight how can you reliably test an R package whose core functionality is changing settings outside your R project.   Work on ropsec started at the 2018 ropensci unconf (unconf18.ropensci.org) and the package continuously improved since then. ropsec leverages gpg (on CRAN) that provides low-level functions for signing commits; the value added comes from the collection of high-level functions, the thorough documentation and the intuitive workflow that help R users to solve end-to-end use cases. Its aim is to mitigate the risk of doing something the user does not intend to do due to the complexity of the low level operations.
",r,2019
"Version control systems provide an important environment for controlled code and software development. In the case of R and RStudio however, the integration of version control tools is still significantly behind what is desirable. This has become a common typical concern for the whole community, especially with the uprise of Github and git for open-source R package development, where issues are being tackled through separate branches and contributors are more numerous and heterogenous.
Command-line git interaction can be an additional barrier and so individuals and organizations have sought different ways to deal with the shortcomings. In this talk, we propose a flexible light-weight combination with Meld (http://meldmerge.org/), an open-source visual diff and merge tool, and demonstrate ""compareWith"" (https://github.com/miraisolutions/compareWith), an R package that allows users to interact with Meld from within RStudio.
compareWith provides user-friendly addins that enable and improve tasks that are otherwise difficult or impossible without any custom extension. Examples include: i) compare differences prior to commit, for single active files or the whole project; ii) resolve and merge conflicts via three-way comparison; iii) compare 2 distinct files with each other. Even simple tasks benefit from the improved diff viewing tool compared to what is built into RStudio.
",r,2019
"Building an R package is a great way of encapsulating code, documentation and data, in a single testable and easily distributable unit. Whether you work by yourself or with others, the goal is always to keep your code easily maintainable and bug-free. R CMD check offers a set of checks on the source code of a package to ensure a quality standard required for packages on CRAN. However, it does not cover other aspects of writing good quality software such as code complexity (1) and does not require testing. The goodpractice package leverages several R packages addressing these aspects and, in one place, calculates code coverage (via covr) and cyclomatic complexity (via cyclocomp), runs linters (via lintr), includes all checks from R CMD check (via rcmdcheck) and gives further advice on good practices for R packages, e.g., to include a URL for a bug tracker. The package currently contains 230 checks to help package developers write high quality packages to a common standard. It is both configurable and extensible, so you can use it with your custom set of checks.
(1) TJ McCabe (1976) A Complexity Measure. IEEE Transactions on Software Engineering (SE-2:4).
",r,2019
"rt - R tools for the command line - is an R package containing a collection CLI programs to simplify R package development, daily routines and managing the R user library. It runs on Unix-alikes, macOS and Windows. rt packs many basic operations in handy command line instructions, effectively isolating tasks in a separate R process, to not stand in the way of your coding experience. Developing R packages often is a tedious matter involving many repetitive tasks, which mainly are testing and checking. Reducing the effort for those tasks sums up to a more efficient workflow and more available time for coding. rt allows you to test, check, build and spellcheck packages, upload them to the winbuilder and rhub from the command line.  Simple routines like installing packages from CRAN or GitHub, knitting documents, starting shiny apps and updating the package library can be run directly from the shell. For users that maintain R installations on multiple machines, a dotfile configuration allows them to keep the package libraries synchronous.   Project Page: https://github.com/rdatsci/rt
",r,2019
"Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This lightning talk with introduce {polite} package – a collection of functions for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice.
",r,2019
"We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Driven by the need to unlock the potential of textual data, sentiment analysis is increasingly used to capture its information value. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from The Wall Street Journal and The Washington Post to create a selection of interesting aggregated text-based indices, and use these to forecast expected stock market volatility.
",r,2019
"The number of scientific articles is constantly increasing. It is sometimes impossible to read all the articles in certain areas. Among this great diversity of articles, some may be more interesting than others. It is difficult to select which articles are essential in a field. The contemporary way to judge the scientific quality of an article is to use the impact factor or the number of citations. However, these parameters may lead to a lack of certain articles that are not very well cited but are very innovative. It is therefore essential to ask the question of what makes an article fundamental in a field. Using the ""fulltext"" package in our Shiny web application we show how the analysis of a bibliography using a network is a good way to visualize the state of the art in a field.  We  searched for different parameters to judge scientific quality using data science approaches. Recent research has shown that the work of small research teams can lead to scientific innovations. In this sense, the analysis of scientific articles by global techniques could play an important role in the discovery of these advances.  
",r,2019
"Word clouds provide a nice mechanism to visualize a list of weighted words. R has already two dedicated packages, wordcloud and wordcloud2, that produce respectively a base plot and a html widget. ggwordcloud has been introduced last fall to propose an alternative for the ggplot2 ecosystem. It consists mainly in a new geometry geom_text_wordcloud which places words on the ggplot2 canvas using an algorithm very similar to the one used in wordcloud. It provides some extensions: a better word scaling, the use of masks as in wordcloud2 and works with ggplot2 facet system. The code has been inspired by ggrepel and relies on RCpp so that the rendering is fast. In the lightning talk, we will present the package and some of its uses. For more details, see https://lepennec.github.io/ggwordcloud/
",r,2019
"One of the big challenges of learning German is determining the grammatical gender of German Nouns (Genus / grammatisches Geschlecht, e.g. der Löffel, die Gabel, das Messer). For light-hearted, the rules seem to be pretty arbitrary. In this talk, I am going to show how I created a quite (or not quite) accurate model to predict the grammatical gender of German nouns using the R package keras. During the development process, I have encountered some interesting problems about the German Language and machine learning in general.
",r,2019
"What is the biggest challenge in data science? Some say it is messy data, others say company politics, but for us at CorrelAid one of the biggest challenges is its unused potential. Larger companies, universities or governmental organizations can afford professional data scientists, but what about civil society or NPOs? Currently, the resources to generate and use data science insights are highly imbalanced. Using R, we show how to leverage this unused potential by applying a cutting-edge multi-language classification approach.  
In collaboration with Minor – a German organisation offering legal advisory for marginalised groups – we present a data for good use case. We use NLP techniques such as multi-language word embeddings (word2vec), unsupervised classification (e1071, caret) and topic modeling (stm) to enable Minor volunteers to better allocate their time. We demonstrate the implementation of an interactive Shiny Dashboard app for classifying and filtering multilingual Facebook comments, including English, Bulgarian and Arabic. The filter mechanisms first identify and filter out comments in the Facebook groups which Minor monitors. The topic model then allocates comments to the respective volunteer at Minor. As a result, volunteers can spend more time on actually helping their clients on Facebook instead of sifting through unstructured comments to find the relevant cases.
",r,2019
"Care trajectory analysis from medical administrative information systems data requires integrating multiple medical data sources (hospital care, drug consumption...) stored in various specialized codifications (ICD10, CIP...), more or less detailed and not always intelligible at first glance by epidemiologist researchers. Tools and methods are needed to facilitate the annotation and the integration such codes to decipher and compare care trajectories. Because medical data are often codified according to international nomenclatures, they can be linked to knowledge representations from medical and pharmacological domains. Linked Data initiatives and Semantic Web technologies have led to the spread of knowledge representations through ontologies, thesauri, taxonomies and nomenclatures. However, the use of these standards, technologies and knowledge representations is still hesitant for non-computer scientists as non-trivial to organize and query for pharmaco-epidemiologist researchers.The R package queryMed (https://github.com/yannrivault/queryMed) provides user-friendly methods to access biomedical knowledge resources from the Linked Data. It proposes methods for the enrichment of health care data to facilitate care trajectory analysis (e.g., pharmaco-surveillance, pharmaco-vigilance), making the most of ontologies or nomenclatures properties.
",r,2019
"Aircraft trajectories are becoming more available both publicly via ADS-B data crowd-sourced by aviation enthusiasts and non-profit organisations such as OpensSky Network and ADSBexchange, and also commercially via platforms such as FlightRadar24 and Flightaware. The trrrj R package supports import, export and analysis of aircraft trajectories (4D: 3D plus time) in the various phases of a commercial flight. The package also provides support for spatial analysis and plotting horizontal and vertical profiles. We present a real use case of trrrj application for arrival flights to several European airports by dealing with the assessment of inefficiencies (time, distance flown, fuel/CO2 emission) related to holdings.
",r,2019
"Data on night-time light intensity is increasingly used by social science researchers as a proxy for economic development. Calculated from weather satellite recordings, it provides annual data for the whole globe in gridded format with pixels covering less than one square kilometer. This allows researchers to aggregate these data at the level of subnational units and analyze it together with other socio-economic indicators. Satellite images are freely available as large raster files. The aim of this presentation is to show how to analyze these data step by step in R – starting from importing the data to R, then correctly imposing a map of a selected area (e.g. shapefile) on the raster object, limiting the satellite image to the selected spatial extent and finally aggregating the data for the analyzed territorial units and visualizing the result. In addition, correlation between night-time light intensity and selected socio-economic indicators (e.g. population, GDP) will be analyzed for world countries, US states and UE regions (NUTS2 and NUTS3). R packages: dplyr, sf, raster, ggplot2, leaflet, WDI, eurostat
",r,2019
"In ecology or in epidemiology, understanding how landscape structures the spatial distributions of species and populations is crucial to determine management rules for sustainable systems in agriculture and conservation biology. However, identifying landscape effects remains difficult, especially since no easy-to-use tools are available for this type of analysis. Here we present 'SILand', an R package. It is the first user-friendly tool that permits a complete and complex analysis of landscape effects, using few functions based on a ""classic"" syntax similar to well-known packages (such as stat and lme4). It can be used for the following: (i) quickly import GIS files in R; (ii) infer the spatial influence of landscape variables and the area of significant influence of each landscape variable and (iii) provides highly informative visualizations of the results (prediction maps).
",r,2019
"Diabrotica or more commonly known as the cucumber beetle or corn rootworm is a species of beetle. It is a major a pest and is known to cause major economic damage to corn growers. Diabrotica was previously only found in the United States and some parts of South America but is believed to have been introduced to Europe during the Yugoslav wars. Due to its potential devastating economic repurcussions it is important to understand the population life cycle of the beetles to ensure adequate controls and measures are in place to minimise impact. Through the use of the Gompertz curve and Bayesian Hierarchical Models in R, we can model the observed dynamics of the beetles emergence to infer when emergence starts and how space, time, and climactic factors affect the dynamics.  
",r,2019
"Geospatial data is of increasing importance in resource management, impact assessment and decision-making. However, the variety of requirements and user experience when working with spatial data - from developing data visualisations through graphical interfaces to undertaking analyses with customized coding - necessitates tailored solutions to maximize accessibility of data and analysis tools for different user groups. The Marine Stewardship Council (MSC) is a non-profit organization providing an internationally recognized sustainable seafood ecolabel and fishery and traceability certification programs. The nature of such an organization means that multiple departments have different requirements and coding proficiency, requiring that the MSC's spatial data management and analysis strategy accommodates a variety of access and analysis tools. Here, we discuss the implementation of spatial data storage, organisation and analysis at the MSC in light of maximizing accessibility, workflow efficiency and reproducibility of results. Our implementation combines open-source analysis tools such as R, Python and qGIS together with PostgreSQL for spatial data management. R and Python scripts combined under the rPython package allow connecting to the spatial database via R or qGIS and automating repeated processes such as specific queries and analyses.
",r,2019
"The administrative divisions of countries change over time, making it tricky to combine territorial databases from different dates. I will present two packages which help to solve this problem:
1) COGugaison [1], which provides functions for converting a spatial dataset from one year to what it would be (or have been) another year. The package handles when the territories gather and split.
2) CARTElette [2], which contains geographical layers corresponding to the annual division of French territories that can be loaded directly into R's popular {sf} format. Thanks to these two packages, it is for example possible to look at the evolution of the number of women in France in each French department since 1975 taking into account that some of the territories have changed during this period [3]. At this time, those packages only concern France and are therefore only documented in French. By presenting this problem and my current work internationally, I hope to inspire future extensions to other countries and collaborations with international spatial analysts.
[1] https://github.com/antuki/COGugaison
[2] https://github.com/antuki/CARTElette
[3] https://antuki.github.io/slides/180306_RLadies_COGugaison_carto/180306_RLadies_COGugaison_carto.html#51
",r,2019
"The R-package persephone is developed to enable easy processing during the production of seasonally adjusted estimates. It builds on top of RJDmetra and provides analytical tools such as interactive plots to support the SA expert. The package should make it easy to construct personalized dashboards containing selected plots and diagnostics. Furthermore it will support hierarchical time series and tackle the issue of direct vs indirect adjustment.
",r,2019
"Amidst growing recognition of irreproducibility in the scientific literature, we face concerns about the credibility of research and the reliability of decisions they inform. Open science practices, such as writing open-access software, encourage research practices that are both transparent and repeatable. Although open-source R packages for data analysis are increasingly common, proprietary software and black-box approaches are still common for many data collection and processing procedures. In this talk I will briefly present ""mixchar"", a R package alternative to financially restricted ""point-and-click"" methods to estimate carbon in plant litter, and discuss software development for open science in research more generally.
",r,2019
"In this talk we will discuss a workshop taught to social scientists and econometricians at the Center for Spatial Data Science at the University of Chicago with little to no background in spatial data or programming. Unlike a conventional spatial statistics or analysis course, the workshop integrated learning to code in R with learning to think spatially. Researchers learned how to explore, manipulate, and visualize spatial data using recently-developed spatial packages in R, while at the same time learning habits for project management and reproducible research. This talk discusses pitfalls and success stories from the workshop, along with considerations when putting together a spatial data curriculum in R. It describes how teaching researchers led to increased programming literacy among researchers, as well as contributions to open source spatial packages. Finally, it puts forth suggestions for a spatial data curriculum that can be shared openly to teach spatial thinking in R worldwide.
",r,2019
"I'm bad at math; algorithms look like a foreign language to me; and until recently, I thought of R as a tool just for statistics. All of that changed when I discovered generative art. Almost overnight I went from being afraid of math to dreaming of the Mandelbrot set and reading papers on Wang tiling algorithms. I desperately wanted to make my own art, but I had just become comfortable with R, and the idea of learning Processing or Javascript was daunting. So I barrelled forward with R, transforming my attitude towards coding and what is possible with R. In this talk I will take useRs along my journey from math-phobe to algorithm evangelizer through my ""12 Months of aRt"" project (which they can read about on my blog williamrchase.com). I will discuss how the beauty of generative art engaged me more than any math class, and I will inspire useRs to do something fun with R and learn in the process. Along the way, we will learn how R is a fully capable creative coding environment, and how you can leverage a wide breadth of tools such as the tidyverse, spatial libraries, and even Rcpp to turn your creative visions into reality.   
",r,2019
"R-Ladies is an worldwide organization that promotes gender diversity in the R community. Brazil is a developing country in Latin America, that currently has 9 R-Ladies chapters, and the one in Sao Paulo was created in August 2018. By February 2019, it had almost 300 members, showing an impressive growth in such a small period of time. The chapter already held 7 events (four 3 hour meetups, one datathon and 2 whole day workshops), and other meetings are being planned. All of the content presented is made available on github, so anyone can access it. The group provides a safe environment for people interested in R, and all of the activities are free to attend. The gratuity of the activities is very relevant in the Brazilian context, since the few R courses available in Portuguese are often expensive, and there is no other active R group in Sao Paulo. The chapter also collaborates with other projects, through lectures and workshops, also always open to the community. The increasing popularity of the group shows us how important it is to support it, what can be the key to motivate the creation of other chapters in Brazil and increase the strength of the Brazilian R community.
",r,2019
"The strength of R comes from its community. It's easy to get involved as a member into community (if it exists). However, building a new community or making a community active is not easy. Here, I'll share my experience (as a student) on building first R community in Nepal (now 350+ members). What were the shared struggles of my community? How I managed to overcome challenges and made welcoming & active community. Besides that, I'll also share how our community helps student learn R in academics & research. So, come and join me to know tips on building active & engaging community at your place.
",r,2019
"In this talk, I intend to unfold the challenges local useR communities are facing, backing up my research with the following data: How many R user groups and R ladies chapters have been founded? How many are still in existence? How many couldn't survive in Africa and how many were revived? These data will be analysed to profer a lasting solution to these challenges and help nurture existing useR communities as well as new ones. All this will be capped with advice on how I have worked with more than 7 growing communities in the last 5 years.  
",r,2019
"In France, over half of processed food consumed on daily basis is made by food industry. Dietary behaviors play a key role in development of chronic diseases. In order to help people make healthier choices, French government, among others actions, implemented Nutri-Score label, on a voluntary basis. This scoring adapted from Food Standard Agency score ranges from A (green, best score) to E (red, worst score), taking into account contents of 7 favorable and unfavorable common components in food. We developed a web-based Shiny app to help agribusiness professionals calculate and improve the Nutri-Score of their products. A first feature consists in loading user's product nutritional composition table and automatically calculating score for all products. Using ggplot2 and ggiraph, the user visualizes score repartition in all her/his products database, filtering according to her/his own variables. Officer allows user to pick up desired graphs for automatic report. Then, from a product and score target (from A to E) selected by user, all combinations of components (expressed as contents ranges) complying with target are generated. Only the closest combinations to the initial nutritional composition are displayed to user. She/he can also choose varying components. In this way, user gets conceivable solutions to improve nutritional quality of her/him products by saving her/him valuable time.
",r,2019
"The NHS in England is under considerable pressure during winter. Within the context of existing funding pressures, demand for hospital care increasingly exceeds the capacity of emergency departments. In recent years, performance targets have consistently been missed on a national level with potentially worrying consequences for care quality and safety. This trend has also received growing media and political attention. Throughout winter the NHS regularly releases provider-level data on performance indicators, such as A&E waiting times and hospital bed occupancy, which are key to understanding quality of care and to inform future planning efforts. However, partly due to inconvenient formatting of the spreadsheets, it takes considerable analytical skill and effort to routinely produce aggregate metrics, examine trends and assess regional variation. We have developed a Shiny app as an interface for visualisation and comparison of a range of NHS performance indicators over winter, aimed at the public, the media and NHS analysts (to be released before useR). The app also shows historical context and the option to aggregate indicators within local areas. With this we aim to provide a convenient, consistent and consolidated way of tracking NHS winter performance indicators. We also want use it as a case study and learning resource to promote the use of R within the NHS via the NHS-R community.
",r,2019
"Antibacterial agents have made modern medicine possible. However, the dramatic increase of resistant and multiresistant bacteria is now recognized as a global challenge for human health. Phenotypic resistance can be measured in growth experiments where bacterial isolates are cultivated under drug exposure. This can be done in liquid media on multiwell plates to identify minimum inhibitory concentrations (MIC), or as diffusion test on an agar dish, where the diameter of the inhibition zone (ZD) is recorded. This is repeated with a large number of strains, because environmental populations are composed of different geno- and phenotypes. The MIC or ZD values form multi-modal distribution mixtures.
Package antibioticR (https://github.com/tpetzoldt/antibioticR) implements methods to separate sub-populations from environmental samples and to estimate distribution parameters and quantiles. It provides:
1. Kernel density smoothing to estimate location parameters and an initial guess of variance,
2. A web-based (shiny) implementation of the ECOFFinder algorithm (Turnidge et al, 2006), 
3. Maximum likelihood estimation of multi-modal normal and exponential-normal mixtures.
The package analyzes sensitivity, tolerance and resistance on a sub-acute level to compare populations of different origin. The package contains also visualization tools and interactive web-applications.
",r,2019
"Mendelian randomization (MR) is a powerful approach to study causality by using germline genetic variants associated with a risk factor (exposure) as instrumental variables for the risk factor itself. The growing availability of results from large genome-wide association studies, not only for clinical outcomes but also for lifestyle exposures, makes MR analyses based on summary genetic data relevant for many exposure-outcome relationships. Properly conducting MR studies in R requires several steps that involve packages for both estimation and data visualization. We will present how to best exploit R to perform and present two-sample MR analyses with an example of our work on the role of obesity-related factors in cancer risk. The approach include the harmonization of the genetic information for the exposure and outcome, the estimation of the causal effect of the exposure on the outcome using different estimators, and a number of complementary analyses for the evaluation of potential pleiotropy, heterogeneity, assumption violations, and bias. The wide range of techniques required to conduct a robust MR analysis is reflected in the use of both largely used and MR-specific packages.
",r,2019
"In vivo studies are crucial to the development of novel therapies. In vivo data is important for proof-of-concept validation, FDA applications and clinical trials. Appropriate data analysis and interpretation are essential in providing the knowledge about the drug efficacy and safety within a living organism. With drug discovery science moving forward at an ever-accelerating rate analyses software not always capable to offer appropriate analysis suit. In vivo scientists at Janssen R&D needed comprehensive analysis tool to conduct appropriate and efficient analyses of in vivo data to insure quality and speed of decision-making. INVIVOLDA shiny application was developed to fulfill the gap. INVIVOLDA offers: powerful Linear Mixed Effect modeling for evaluating differences between treatments over time; Buckley-James method is used to infer about mean treatment differences when response is non-linear in the presence of censoring; survival analysis enables evaluation of time-to-event data. Furthermore, interactive and animated graphics allow users to conduct independent and thorough data explorations. INVIVOLDA allows streamlining complex statistical analyses of in-vivo longitudinal data by utilizing modern graphics, appropriate modelling techniques and report generation and insures efficient, traceable and reproducible in vivo research and data-driven decision making.
",r,2019
"Spatial and spatio-temporal analyses of count data are crucial in epidemiology and other fields to provide accurate estimates of mortality and/or incidence risks, and unveil the underlying spatial and spatio-temporal patterns. However, fitting spatial and spatio-temporal models is not easy for non-expert users. Here, we present the interactive web application SSTCDapp for the analysis of spatial and spatio-temporal mortality (or incidence) data, which is addressed at https://emi-sstcdapp.unavarra.es/. The web application is designed to perform descriptive analyses in space and time of mortality risks or rates, and to fit an extensive range of fairly complex spatio-temporal models commonly used in disease mapping. The application is built with the R package shiny and relies on the well founded integrated nested Laplace (INLA) approximation technique for model fitting and inference. Unlike other software used in disease mapping, SSTCDapp provides an user-friendly interface that facilitates the fit of complex statistical models to non-experts users without the need of installing any software in their own computers, since all the analyses and computations are made in a powerful remote server. In addition, a desktop version is also available to run the application locally if needed, which avoids uploading the data to the online application fully guaranteeing data confidentiality.
",r,2019
"There is no doubt that R is the swiss knife for modeling activities. Variety of families of models and implementations in numerous packages talks for itself. Interested R users will read the Machine Learning task view and discover (some of) those packages. This talk is not about models themselves but about frameworks. Some meta-packages indeed do not at all contain modeling functions but act as wrappers around existing assets and provide high level functionalities (cross-validation, hyperparameters grid search, staking...). Objective is to have consistent syntax with a limited set of 'verbs' as well as providing a way to implement a modeling process. We will introduce the reasons why such packages are so interesting for Data Scientists and present 3 solutions: caret, mlr and SuperLearner. In addition, as modeling pipe also requires data preparation, we will also talk about vtreat, recipes (+embed), mlCPO, sl3 and their possible integration with those frameworks. We will present some modeling flows as example and also introduce a tidy approach for modeling .
",r,2019
"This talk will provide a brief overview of the field of Automatic Machine Learning (AutoML), with a focus on software and benchmarking. The term ""AutoML"" refers to automated methods for model selection and/or hyperparameter optimization and includes such techniques as automated stacking (ensembles), neural architecture search, pipeline optimization and feature engineering. AutoML tools are designed to maximize ease-of-use by simplifying the API. We will discuss the common AutoML software design patterns and take a detailed look at the AutoML algorithm inside of the ""h2o"" R package.
An important part of the development process to evolve and improve an AutoML system is a comprehensive benchmarking strategy. Benchmarking AutoML software across a wide variety of datasets allows the algorithm designer to identify weaknesses in the algorithm and software. This enables tool designers to make incremental, measurable improvements to their system over time. We will present a new open source platform for benchmarking AutoML systems which is part of the OpenML.org ecosystem for reproducible research in machine learning. The system is extensible, so anyone can write a wrapper for their software in order to benchmark it against the most popular open source AutoML systems. We will also present benchmarking results for H2O AutoML against a variety of (mostly Python-based) AutoML systems.
",r,2019
"The package mlr (Machine Learning with R) was released to CRAN in 2013, and its core design dates back even further. The new mlr3 package is its modular, from-scratch reimplementation in R6. Data is stored primarily as data.tables. mlr3 relies heavily on the reference semantics of R6 and data.table, which enable efficient and elegant programming on the provided machine learning building blocks. The package is geared towards scalability and larger datasets by natively supporting parallelization and out-of-memory data-backends like databases. With a clear object-oriented design, mlr3 focuses on core computational operations, while add-on packages allow seamless integration of extended functionality. For example, mlr3survival implements tasks and learners for survival analysis, and mlr3pipelines extends mlr3 with graphs of (pre)-processing operations, which can be jointly tuned with the mlr3tuning package. Project page: https://mlr3.mlr-org.com
",r,2019
"mlr3pipelines is an object-oriented dataflow programming toolkit for machine learning in R6. It provides an expressive and intuitive language to define ML workflows as directed acyclic graphs that represent data flows between computational units, e.g., preprocessing, model fitting and model combination. This chains data and model manipulation steps in a modular way to form powerful data processing pipelines. Many complex ML concepts, for which special purpose packages are usually provided, can now be expressed in few lines of graph definition code: e.g., unions of feature views, bagging, stacking and hurdle models. Resulting pipelines are parameterized, so all components can jointly be tuned to obtain an optimal configuration. Graphs can contain ""branching"" nodes which allow selective, conditional processing of execution paths. The tuning of such tasks allows complex model selection. The modular, object-oriented concept of mlr3pipelines facilitates convenient extension with custom operations, while the compatibility with mlr3 allows convenient tuning, benchmarking, nested resampling and more. Project page: https://github.com/mlr-org/mlr3pipelines
",r,2019
"Many R users request data from the web in their scripts and packages. This talk introduces a modern suite of packages for managing HTTP requests. The crul package is a modern HTTP request library, including asynchronous requests, automatic handling of pagination, and more. Importantly, crul provides an R6 based object system that makes it easier to program with relative to other tools. The webmockr package mocks HTTP requests, returning user specified mocked responses matching the format of the real thing. The vcr package leverages webmockr to cache HTTP requests and responses. Both webmockr and vcr support many HTTP libraries. Last, httpcode provides information on all HTTP codes, and fauxpas provides proper HTTP error classes for use in most HTTP R libraries. These tools together provide a modern way for R programmers to manage HTTP requests.
",r,2019
"Data science using R is increasing performed in the cloud or over a network. But how secure is this process? In this talk, we won't look at complex hacking but instead, focus on the relatively easy hacks that can be performed to access systems. We'll use three R related examples of how it is possible to access a users system.  In the first example, we'll investigate domain squatting on the Bioconductor website. By registering only thirteen domains, we had the potential to run arbitrary on hundreds of users. In the second example, we'll look at techniques for guessing passwords on RStudio server instances. Lastly, we'll highlight how users can be a little too trusting when running R code from blogs.     
",r,2019
"Usethis is one of the packages created in the recent ""conscious uncoupling"" of the devtools package. Devtools is an established package that facilitates various aspects of package development. Never fear: devtools is alive and well and remains the public face of this functionality, but it has recently been split into a handful of more focused packages, under the hood. Usethis now holds functionality related to package and project setup. I'll explain the ""conscious uncoupling"" of devtools and describe the current features of usethis specifically. The DRY concept -- don't repeat yourself -- is well accepted as a best practice for code and it's an equally effective way to approach your development workflow. The usethis package offers functions that enact key steps of the package development process in a programmatic and documented way. This is an attractive alternative to doing everything by hand or, more realistically, copying and modifying files from one of your other packages. Usethis helps with initial setup and also with the sequential addition of features, such as specific dependencies (e.g. Rcpp, the pipe, the tidy eval toolkit) or practices (e.g. version control, testing, continuous integration).
",r,2019
"In 2017 the tidyverse grammars were reimplemented on top of tidy evaluation, a metaprogramming framework from the rlang package. Tidy eval makes it possible to program flexibly and robustly with data masking functions from packages like dplyr or ggplot2. However, possible does not mean easy. Tidy eval has the major downside of requiring to learn new programming concepts and tools. In this talk, we'll focus on easier techniques to reuse code from tidyverse pipelines without such a steep learning curve: mapping columns, using fixed column names, passing dots, subsetting the `.data` pronoun, and interpolating expressions. These techniques will help you write functions around tidyverse pipelines and reduce code duplication in your scripts.
",r,2019
"Within the tidyverse, the core structure that powers many packages is the tibble, a modern reimagining of the data frame. Unfortunately, with the large focus on data frames, the array has been left behind. The rray package is an attempt to change that. By borrowing ideas from tibble, rray hopes to create ""simpler arrays"" that are more predictable to use and program around. To accomplish this, rray provides the following new infrastructure:   An rray class, which never drops dimensions while subsetting, and consistently retains dimension names where possible.    Broadcasting semantics, using the xtensor library. rray implements the wildly popular idea of broadcasting, originally found in the Python library, numpy, to allow more intuitive and powerful operations between multiple rray objects. This opens up a much more complete set of operations than is currently possible with base R.    A consistent toolkit for common array manipulation tasks, such as computing sums and products along any axis. Each function retains dimensionality by default, making it easy to link operations together through broadcasting. Importantly, this toolkit works with base R arrays as well as with the new rray objects.  https://davisvaughan.github.io/rray/ https://github.com/DavisVaughan/rray
",r,2019
"Hierarchical Bayesian models are increasingly used for applied statistics. Parameters of such models can be estimated through Gibbs sampling Markov chain Monte Carlo using a variety of algorithms (conjugated priors, Metropolis-Hastings, Hamiltonian Monte Carlo). These algorithms approximate the parameter posterior distributions through iterative simulations and are computationally intensive. Using C/C++ language to code such algorithms make computations faster. In our presentation, we will show how Rcpp* R packages (Rcpp, RcppGSL and RcppArmadillo), can be easily used to (i) call Gibbs samplers written in C++ from within R, (ii) reduce computation time through efficient random draws, and (iii) facilitate vector and matrix operations. We will illustrate this approach with the new jSDM R package for fitting joint species distribution models.
",r,2019
"Many processes have space-time non-separable dynamics (e.g. disease spread and species distribution) which should be accounted for during modeling. A non-separable stochastic partial differential approach (SPDE) can be used to consider the realistic space-time evolution of the process in which the spatial and temporal autocorrelation in the latent field are linked (Krainski 2018). Observations of these processes are often measured as point-referenced locations in time, i.e. space-time point patterns. The log-Gaussian Cox process model is a popular class to model point patterns. However, it can be difficult fit in practice because the likelihood depends on an integral over the spatial and temporal domains. Implement these models in R-INLA is challenging because it involves several steps. We provide a step-by-step approach to constructing a space-time model in R-INLA using fox rabies as a case study. We discuss several useful updates to the R-INLA package (e.g. inlabru), including improvements to the integration methods in space and time and options to improve computational performance. We also discuss several practical considerations for users to consider in model construction including mesh generation, model implementation, model checking and extensions to the basic model. The goal of this work is to help users avoid common pitfalls when constructing and interpreting these models.
",r,2019
"Model selection with high-dimensional data becomes an important issue in the last two decades. With the presence of missing data, only a few methods are available to select a model, and their performances are limited. We propose a novel approach -- Adaptive Bayesian SLOPE, as an extension of sorted $l_1$ regularization but in Bayesian framework, to perform parameter estimation and variable selection simultaneously in high-dimensional setting. This methodology in particular aims at controlling the False Discovery Rate (FDR). Meanwhile, we tackle the problem of missing data with a stochastic approximation EM algorithm. The proposed methodology is further illustrated by comprehensive simulation studies, in terms of power, FDR and bias of estimation.
",r,2019
"Endogeneity becomes a challenge when aiming to uncover causal relationships in empirical research. Reasons are manifold, i.e. omitted variables, measurement error or simultaneity. These might lead to the unwanted correlation between the independent variables and the error term of a statistical model. While external instrumental variables methods can be used to control for endogeneity, these approaches require additional information which is usually difficult to obtain.  
Internal instrumental variable (IIV) methods address this issue by treating endogeneity without the need of additional variables, taking advantage of the structure of the data. Implementations of IIV are rare. Thereby, we propose the R package ""REndo"" that implements five instrument-free methods: the latent instrumental variables approach (Ebbes et al. 2005), the higher moments estimation (Lewbel 1997), the heteroskedastic error approach (Lewbel 2012), the joint estimation using copula (Park and Gupta 2012) and the multilevel GMM (Kim and Frees 2007).   
This talk will focus on both, the theory behind each of the five methods proposed as well as on the practical implementation using real and simulated data.
",r,2019
"Enormous amounts of observational data are being produced every day from internet users, health care providers and satellites alike. This opens up a lot of new possibilities for what observational data may be used for. But if the subject is causality, it is still common to solely rely on externally proposed, ""hypothesis-driven"" models, which limits the range of causal inquiries. However, in some cases it is possible to construct a causal model from the data using structure learning. This is not only relevant for those interested in inferring causality. Even when prediction is the goal, knowledge of causal structures is useful for helping domain adaption because the mechanistic nature of causal structures make them more stable. A myriad of packages for structure learning have been developed in R, including pcalg, bnstruct, bnlearn, deal, catnet and stablespec, each of them dedicated to a certain class of causal models (e.g. linear), a specific algorithmic approach (e.g. constraint-based), or a combination of both. In this presentation, I provide an overview of existing R packages for structure learning, focusing on overlaps and differences in functionality, interface and possibilities to include external information. I also discuss how the packages may be integrated into a joint tool, thereby facilitating structure learning without settling on a model class or learning approach a priori.
",r,2019
"The fable ecosystem provides a tidy interface for time series modelling and forecasting, leveraging the data structure from the tsibble package to support a more natural analysis of modern time series. fable is designed to forecast collections of related (possibly multivariate) time series, and to provide tools for working with multiple models. It emphasises density forecasting, whilst continuing to provide a simple user-interface for point forecasting.  Existing implementations of time series models work well in isolation, however it has long-been known that ensembles of forecasts improve forecast accuracy. Hybrid forecasting (separately forecasting components of a time series) is another useful forecasting method. Both ensemble and hybrid forecasts can be expressed as forecast combinations. Recent enhancements to the fable framework now provide a flexible approach to easily combine and evaluate the forecasts from multiple models. The fable package is designed for extensibility, allowing for easier creation of new forecasting models and tools. Without any further implementation, extension models can leverage essential functionality including plotting, accuracy evaluation, model combination and diagnostics. This talk will feature recent developments to the fable framework for combining forecasts, and the performance gain will be evaluated using a set of related time series.
",r,2019
"This work presents two feature-based forecasting algorithms for large-scale time series forecasting. The algorithms involve computing a range of features of the time series which are then used to select the forecasting model. The forecasting model selection process is carried out using a pre-trained classifier. In our first algorithm we use a random forest algorithm to train the classifier. We call this framework FFORMS (Feature-based FORecast Model Selection). The second algorithm use efficient Bayesian multivariate surface regression approach to estimate forecast error for each method, and then using the minimum predicted error to select a forecasting model. Both algorithms have been evaluated using time series from the M4 competition, and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches in the time series forecasting literature. The methods are made available in the seer and fformpp packages in R.
",r,2019
"Random forests were introduced in 2001 by Breiman and have since become a popular learning algorithm, for both regression and classification. However, when dealing with time series, random forests do not integrate the time-dependent structure and treat each instant as an independent observation. In this study, we propose the rangerts, an extended version of the ranger package for time series.  
Goehry (2018) proved under right hypotheses on parameters and the time series that random forests are consistent. In practice, the idea is to replace the IID bootstrapping with dependent bootstrapping to subsample time series during the tree construction phase to take time dependency into account.   
We tested our package both on numerical simulations and the real world applications on electricity load and show our method improves forests' accuracy in some cases. We discuss also how to make a good choice of the key parameters. References Breiman L. Random forests, Machine Learning, vol. 45, no.1, pp. 5-32, 2001. Goehry B. Random forests for time-dependent processes, preprint, Available: https://hal.archives-ouvertes.fr/hal-01955331, 2018. Wright M.N. & Ziegler A. ranger: A fast implementation of random forests for high dimensional data in C++ and R, J Stat Softw 77:1-17, 2017.
",r,2019
"There are several packages in R that implement forecasting using state space models, and only one that relies on a single source of error state space model (""forecast"" package). Unfortunately, the forecasting functions in that package are not flexible enough for different research purposes. For example, exponential smoothing, implemented in ets() function does not allow using explanatory variables, setting the initial values of the states vector and does not allow fitting models to the data with periodicity higher than 24. This motivated the original development of the package back in 2015, with the main aim of making the research in forecasting area ""smooth"". Four years later, the smooth package has a handful of flexible forecasting functions useful for different research purposes, implementing ETS, ARIMA, vector exponential smoothing, simulation functions and more. In this presentation we will discuss the main functions of the package, show their advantages and disadvantages, and show how they can be applied for the solution of the real world forecasting problems and complement ""forecast"" and other widely used packages.
",r,2019
"Introducing the R package ForecastComb. The aim is to provide researchers and practitioners with a comprehensive implementation of the most common ways in which forecasts can be combined. The package in its current version covers 15 popular estimation methods for creating a combined forecasts – including simple methods, regression-based methods, and eigenvector-based methods. It also includes useful tools to deal with common challenges of forecast combination (e.g., missing values in component forecasts, or multicollinearity), and to rationalize and visualize the combination results.
",r,2019
"The R for Data Science (R4DS) Online Learning Community was started by Jesse Mostipak in August of 2017, with the goal of creating a supportive and responsive online space for learners and mentors to come together to provide support and guidance in learning R in a beginner-friendly, safe and supportive environment. However, the main aim of the R for Data Science community was to move through the *R for Data Science* text by Garrett Grolemund and Hadley Wickham, which walks readers through the major features of the tidyverse in the context of data science. We also aim to help users learn R and expand their R knowledge. Over time R4DS science community has been developing projects intended to help connect mentors and learners. One of the first projects born out of this collaboration is #TidyTuesday, a weekly social data project focused on using tidyverse packages to clean, wrangle, tidy, and plot a new dataset every Tuesday. Over the years, the community have been experienced a steep growth with over 2000 members from various countries in the world in our slack channel. At the R4DS community, we encourage diversity and contribution from everyone. We believe that no question is silly and no one is an island of knowledge. In this talk, we will be sharing the positive changes made since the transfer of leadership, our challenges, the lesson we have learnt and provide room for suggestion and opinions.
",r,2019
"This talk summarizes some of the recent work that put Latin America (LatAm) in the international R user map. In January 2017, R-Ladies Buenos Aires (BA) was founded. R-Ladies BA encouraged the foundation of other R-Ladies groups in the region (e.g., Santa Rosa, Santiago). In two years, LatAm went from one to 29 R-Ladies active groups. English-only materials can be a barrier to R. Hence, LatAm R-Ladies led efforts to translate into Spanish: R for Data Science (es.r4ds.hadley.nz), R-Ladies code of conduct and policies (bit.ly/2UwP79x), and R Consortium 2017 (bit.ly/2Tpt8RR) and RStudio 2018 surveys. The creation of new regional conferences was another R-Ladies-led effort. Thanks to the R-Ladies network, LatinR (latin-r.com) got rapidly started and LatAm had its first SatRDay. Among other achievements, these events impulsed the regional community in the form of new and diverse R user groups (e.g., Rosario, Montevideo). In a nutshell, in the last two years, the LatAm R user community has seen fast growth and regrouping, mainly spearheaded by R-Ladies. This presentation details key aspects of this process and seeks to inspire other R regional communities worldwide.
",r,2019
"Africa R is a consortium of passionate Africa R user groups and users innovating with R every day and are inspired to share their experience as well as communicate their findings to a global audience. This consortium was birth from the underrepresentation and minority involvement of African population in every role and area of participation, whether as R developers, conference speakers, educators, users, researchers, leaders and package maintainers.    As a community, our mission is to achieve improved representation by encouraging, inspiring, and empowering African population of all genders who are underrepresented in the global R community.    With a primary objective of supporting already existing R Users across Africa and R enthusiasts to embrace the full potential of R programming, through fostering a collaborative continental network of R gurus, mentors, learners, developers and leaders to help facilitate individual and collective progress worldwide.   Africa R talk includes a presentation of our work plan, collaborators, partners and mentors. We will also be using this opportunity to show statistics of members, R user group in our network and launching our website.    #AfricaRusers - Twitter handle.  
",r,2019
"9 satRday events were organised since UseR! 2018. Join us for a panel discussoin with a diverse group of organisers from across the world. You will learn about why they chose to volunteer their time, how they approached satRday's mission, what they have learnt from organising the event, how satRday impacted their local R community, and more!
",r,2019
"9 satRday events were organised since UseR! 2018. Join us for a panel discussoin with a diverse group of organisers from across the world. You will learn about why they chose to volunteer their time, how they approached satRday's mission, what they have learnt from organising the event, how satRday impacted their local R community, and more!
",r,2019
"The response to emerging disease outbreaks and health emergencies are increasingly data-driven, integrating various sources of information to improve situational awareness in real time. Outbreak analytics face many of the modern data science challenges, and additional difficulties pertaining to the emergency, low-resource settings characterising some of these outbreaks. In this presentation, I will outline some of these challenges, and a range of solutions developed by the R Epidemics Consortium (RECON), an NGO dedicated to developing free analytics resources for health crises. In particular, we will discuss different aspects relating to the deployment of robust, reliable reporting infrastructures in the 2019 Ebola outbreak in North Kivu, DRC. We will showcase features of new R packages dedicated to data standardisation and cleaning (linelist), automated reporting (reportfactory), and offline alternatives to online repositories such as CRAN and github for deploying R-based analysis environments (RECON deployer). I will conclude on how R can help strengthening outbreak response capacities, and more generally humanitarian work, in low and middle income countries.
",r,2019
"Data analysis is integral to informing operational elements of humanitarian medical responses. Field epidemiologists play a central role in informing such responses as they aim to rapidly collect, analyse and disseminate results to support Médecins Sans Frontières (MSF) and partners with timely and targeted intervention strategies. However, a lack of standardised analytical methods within MSF challenges this process. The R4epis project group consists of 18 professionals with expertise in: R programming, field epidemiology, data science, health information systems, geographic information systems, and public health. Between October 2018 and April 2019, R scripts were developed to address all aspects of data cleaning, data analysis, and automatic reporting for outbreaks (measles, meningitis, cholera and acute jaundice) and surveys (retrospective mortality, malnutrition and vaccination coverage). Analyses and outputs were piloted and validated by epidemiologists using historical data. The resulting templates were made available to field epidemiologists for field testing, which was conducted between February and April 2019. R4epis will contribute to the improvement of the quality, timeliness, consistency of data analyses and standardisation of outputs from field epidemiologists during emergency response.
",r,2019
"Multiple imputation is a common strategy to overcome the missing data issue. Several MI methods have been proposed in the literature to impute multilevel data with classical sporadically missing values only. However, methods for dealing with more complex missing data are needed. Indeed, the multilevel structure is often due to data merging (because of the heterogeneity between collected datasets), but variables often vary according to the dataset, leading to systematically missing variables. micemd is an addon for the mice package to perform multiple imputation using chained equations with two-level data. It includes imputation methods specifically handling sporadically and systematically missing values (Resche-Rigon et al. 2013, Audigier, V. et al, 2018). micemd offers a complete solution for the analysis: the choice of the imputation model for each variable can be automatically tuned according to the data structure (Audigier, V. et al, 2018), it gathers tools for checking model fitting (Blackwell, M. et al, 2015) and allows parallel calculation. The talk is motivated by a meta-analysis in cardiovascular disease consisting of 28 observational cohorts in which systematically missing and sporadically missing data (GREAT data). Then, based on a simulation study, advantages and drawbacks of each multiple imputation method are discussed. Finally, methods are compared on the GREAT data.
",r,2019
"Health policy models are increasingly being used by clinicians, analysts and policy makers to predict patients' long-term outcomes, how these outcomes are affected by interventions, and whether the interventions are (cost-)effective and should be recommended for use. Usability of models as well as reliability and transparency of methods and results is therefore vital. Even when the code is freely available, laws preventing the sharing of sensitive individual patient data mean that the published results are still not fully reproducible. Additionally, model users may want to change input parameters, and therefore need to possess skills, and time, to understand the underlying code. We present a Shiny-based user-friendly web interface for a health policy model predicting progression of chronic kidney disease and cardiovascular complications. The interface is freely available and users can change different parameters using drop-down menus or .csv files; the output is a detailed downloadable .csv file, and a userguide is provided together with a range of templates. We discuss how, in addition to aid with the usability, such interface may help with debugging and transparency, and what the key considerations during the development are.
",r,2019
"Ancestry informative markers (AIMs) are genetic markers that give information about the genogeographic ancestry of individuals. They are for example used to predict the genogeographic origin of individuals related to forensic crime and identification cases. A likelihood ratio test (LRT) approach is derived in order to prevent erroneous conclusions due to e.g. no relevant populations in a database of reference populations. The LRT is a measure of absolute concordance between a profile and a population, rather than a relative measure of the profile's likelihood in two populations by a likelihood ratio (LR). The LRT is similar to Fisher's exact test and constitutes an outlier test analogous to studentized residuals. Varying sample sizes of the reference populations in the database are explicitly included in the LRT with fewer assumptions than the LR. The LRT is extended to handle admixed profiles (parents of different genogeographic origin). The methodology has been implemented in the R package genogeographer with an optional shiny front-end, that enables forensic geneticists to make explorative analyses, produce various graphical outputs together with evidential weight computations.
",r,2019
"The Carpentries is a non-profit organization that organizes a global community of scientists that runs workshops where researchers are taught foundational computational and data science skills. Since 2012, The Carpentries has taught 2,000+ workshops in 46 countries reaching 38,000+ learners.    Here, I will present how The Carpentries uses R in its daily operations. From analyzing our survey results, to sending personalized certificates of workshop attendance to learners, from creating live-updating visualizations for our workshop instructors to understand their audience before the workshops, to our lesson templates, I will go through some examples of how R has become a central part of how we set up our systems to manage and automate our workflows within our organization.   The combination of literate programming to generate reports, web application development with shiny, and the availability of packages to interact with the web API for many of the online tools and services we use, has allowed us to develop custom workflows. We can iterate quickly and deploy using continuous integration services and Docker.   This talk will be of interest to organizations looking to automate their operations to demonstrate how R can successfully be used in production.
",r,2019
"Continuous integration is a software development practice that advocates for members of a team to merge their work frequently in a shared repository. Each integration is verified through a process of automated building and testing. This process can be facilitated through the use of a build server. A popular choice is Jenkins: a free and open source automation server. Continuous delivery is an extension of the continuous integration principle that asks that every successfully built version of the software is put into a staging environment from where it can easily be deployed to a production setting. The deployment process is still manual but it can be simplified through the use of self-service operations. Rundeck is an open source management platform that allows to define such self-service operations. We propose two new R packages: rjenkins and rrundeck. These packages interact with the Web APIs offered by Jenkins and Rundeck to easily create, trigger and monitor jobs and operations. This provides R users with an intuitive interface to these tools which can be used in an interactive or scripted way.
",r,2019
"We know that adopting git or code version control mechanisms are important for creating a culture of reproducibility in data science. But once you've established the basic best practices in your daily workflow, what comes next? This talk will be an introduction to continuous integration and continuous delivery tools (CI/CD). I'll cover reasons why CI/CD tools can enhance reproducibility for R and data science, showcase practical examples like automated testing and push-based application deployment, and point to simple resources for getting started with these tools in a git and GitHub based environment.  The target user base for advanced Git and GitHub integration tooling remains focused on software engineers and IT professionals. As data scientists lead the scientific community as a whole toward better, reproducible research practices, we need to be aware of the vast ecosystem of technology solutions that can benefit this mission. The specific tools I'll aim to cover in this short presentation are: GitHub webhooks, Jenkins, and Travis, all framed in terms of their use with R code and data products.  
",r,2019
"Continuous integration and delivery (CI/CD) has evolved as a software development best practice, and it also strengthens reproducibility in (data) science. GitHub actions is a new workflow automation feature of the popular code repository host GitHub. It is a convenient service layer on top of the popular container standard docker, and is itself partly open source, thus limiting vendor lock-in. GitHub actions may offer better CI/CD for the R community, but most importantly, it is simple to reason about if things go wrong. The ghactions project presented here offers three avenues to bring GitHub actions to the R community:
1. Developing and curating actions to run R-specific jobs on GitHub, including arbitrary R code or deploying to shinyapps.io.
2. Furnishing users with some out-of-the-box workflows for different kinds of R projects.
3. Documenting experiences and evolving best practices for how to make the most of GitHub actions for R. More information on the ghactions package and project can be found at: http://maxheld.de/ghactions/.
",r,2019
"Writing and maintaining packages is an essential contribution to the R community. Despite a number of formal requirements on packages, most of the internal details of the language can be inspected and modified through reflective features and a rich C API, giving a lot of freedom to package developers. This probably contributed to the popularity of R, but poses a risk when not used responsibly.
R needs to adapt to the changing needs of its users and to changes in the software/hardware environments in which it is used. As of today, almost any change in the R runtime, however minute, breaks some packages. The causes are hard to find especially when the change is to undocumented R behavior or when it ""wakes up"" an old bug. Tests using all CRAN/BIOC packages are run routinely, requiring expensive hardware. Debugging requires skill, experience, knowledge of R internals and typically much more time than the implementation of the change that caused the bug. It is done by R Core and adds to the workload of repository maintainers.
This talk is an inspiration for package authors who want to develop packages responsibly, without unnecessarily increasing the cost of maintenance and evolution of R. It will include advice and examples based on my work on the R runtime (and debugging of packages) related to PROTECT bugs, in-place modification of immutable values, memory management at the C/C++ boundary, and parse data fixes.
",r,2019
"For a long time now, programming langages have been divided in two categories, dynamically typed ones and statically typed ones.Both sides tend to argue that their system has more inherent benefits than drawbacks according to their needs. On one hand it is convenient to have a system that allows writing non well typed programs that you know to be correct. On the other hand, no programmer is ever safe from a silly mistake taking ages to be fixed, if ever detected, in his code base. Thus, with the ever growing usage of dynamically typed langages such as R or javascript, it has become increasingly important to detect mistakes as early as possible in the deveopment process. By adapting some approaches inherited from the strongly statically typed langages community we have developped a typing system for a fragment of the R programming language. We argue that it does not restrict the expresiveness of the R language beyond what is actually widely used. Moreover we have embedded a type checker in a state of the art integrated development environment leveraging the graphical interface to report useful errors to the user.
",r,2019
"Many package developers boost performance by coding key steps in C++, using R's C headers and/or Rcpp. The nimble package, predecessor to nCompiler, includes a system for automatic generation of C++ for a core subset of R's math and distribution functions. nimble implements vectorized and recycling-rule operations by code-generating to the Eigen C++ library and automatic differentiation via the CppAD library (in development versions). It includes basic flow control and static data types. However, as a general R programming tool, nimble has design limitations. nCompiler is a new package, designed to be a more general programming tool, with some refactored components from nimble. nCompiler allows definition of classes that mix R and C++ (code-generated or embedded) data and methods as well as pure functions. Much numerical work becomes C++ without coding any C++ by hand. nCompiler plays well with Rcpp and harnesses its compilation tools; harnesses Eigen more deeply, including its Tensor features; supports automatic differentiation via CppAD; and is designed for embedding code in packages, parallelizing, serializing C++ objects, and providing a natural workflow.
",r,2019
"Interactive debuggers are one of the most useful tools to aid software development. The two most used approaches for interactive debugging in the R ecosystem, the built-in debugger and R Studio, do not support interactive debugging of both R and C code of R packages at the same time and in one tool. FastR is an open source alternative R implementation that, apart from being compatible with GNU-R, puts emphasis on the performance of R execution and tooling support. FastR is part of the multilingual virtual machine (GraalVM) that, among other things, provides language agnostic support for interactive debugging. One of the other projects built on top of GraalVM is a C/C++ interpreter. FastR can be configured to run the native code of selected R packages using this C/C++ interpreter, which should yield the same behavior, but since both languages are now running in one system, it opens up many exciting possibilities including seamless cross-language debugging. In the talk, we will demonstrate how to configure Visual Studio Code and FastR for cross-language interactive debugging and how to debug a sample R package with native code.
",r,2019
"Monte Carlo simulation studies are computer experiments that involve generating data by pseudo-random sampling; they provide an invaluable tool for statistical and biostatistical research. Consequently, dissemination of results plays a focal role to drive adoption and further development of new methods. However, simulation studies are often poorly designed, analysed, reported. One of the aspects often poorly reported - often not reported at all - is the Monte Carlo error of summary statistics, defined as the standard deviation of the estimated quantity over replications. Monte Carlo errors play a crucial role in understanding how results are affected by chance.
In order to aid researchers interested in running and analysing simulation studies, we developed rsimsum and INTEREST. rsimsum is an R package for analysing results from simulation studies: it computes the most common summary statistics and Monte Carlo errors are reported by default. INTEREST is a shiny app providing an interface to rsimsum, offering tools to analyse simulation studies interactively and export plots and tables of summary statistics for later use. rsimsum and INTEREST can aid investigating results from simulation studies and supplement the reporting of their results to a great extent, allowing researches to share the full results of their simulation study and readers to explore them freely and in a more engaging way.
",r,2019
"Algorithmic Differentiation (AD) has been available as a programming tool for statisticians for a number of decades. However, its adoption as an alternative to symbolic and numeric methods does not seem to be very common. One possible reason for this is the difficulty that is typically encountered when attempting to integrate AD functionality into existing statistical computing environments. The RcppEigenAD package attempts to mitigate these difficulties when employing AD within R by combining the facilities of the Rcpp package for extending R with C++, the AD library cppAD, and the Eigen linear algebra library, into a single R package. The resulting package, (RcppEigenAD), allows the user to define matrix valued functions of matrix arguments in c++ and seamlessly integrate them into an R session in a way that allows not only for computing the function but also their Jacobian and Hessian. The package also includes an implementation of Faa di Bruno's formula for calculating the partial derivatives of the composition of functions defined by the user. The package has application in the areas of optimisation, sensitivity analysis and calculating covariances via the delta method, which are illustrated with examples.
",r,2019
"Solving differential equations in R presents a challenge because one must choose between implementations that are either expressive but slow to compute, or cumbersome to write but faster. I present a new package ""odin"" for removing this tradeoff by creating a domain specific language (DSL) hosted in R that compiles a subset of R to C for efficiently expressing and solving differential equations (JavaScript and R are compilation targets under current development). By treating the set of differential equations as a directed acyclic graph, the DSL is declarative rather than imperative. ""odin"" leverages the well established ""deSolve"" package to interface with a number of well understood solvers. I present applications of ""odin"" both in a research context for implementing epidemiological models with 10's of thousands of equations and in teaching contexts where we are using the introspection built into the DSL to automatically generate shiny interfaces.
",r,2019
"The amount of data to manage is increasing across institutions. Metadata plays a key role to make this data findable, accessible, interoperable and re-usable, and becomes a pillar through legal frames (eg INSPIRE) or with the emergence of data management plans (DMPs). Data managers have thus to deal with these requirements by applying standards to manage (meta)data formats and access protocols. It is especially the case for spatial information which is ruled by ISO/OGC standards. The use of R has been spreading worldwide as preferred tool for data managers and scientists. In this context, some projects were initiated to support metadata handling for specific domains (eg EML), while the capacity to produce standardized ISO/OGC geographic metadata with R was limited. The geometa and ows4R packages aim to fill this gap by providing functions to write and read ISO/OGC metadata, and interfaces to the OGC Web Services. We explain their functioning, including recent features provided with the support of the R Consortium. We present then how they contribute to several national and international information systems in different domains such as fisheries, marine monitoring, ecology and earth observation. Based on these packages, the geoflow initiative as orchestrator for spatial data management in R will be introduced to demonstrate how R can be used for managing spatial data infrastructures.
",r,2019
"Voronoi estimators are non-parametric and adaptive estimators of the intensity of a point process. The intensity estimate at a given location is equal to the reciprocal of the size of the Voronoi/Dirichlet cell containing that location. Their major drawback is that they tend to paradoxically under-smooth the data in regions where the point density of the observed point pattern is high, and over-smooth where the point density is low. To remedy this behaviour, we propose to apply an additional smoothing operation to the Voronoi estimator, based on resampling the point pattern by independent random thinning. Through a simulation study we show that our resample-smoothing technique improves the estimation substantially. The proposed intensity estimation scheme is also applied to two datasets: locations of pine saplings (planar point pattern) and motor vehicle traffic accidents (linear network point pattern). Everything is implemented in R and released in the `spatstat` package available on CRAN. The oral presentation will explain the basic concepts, which are very simple, and leave out the mathematical details. Instead, focus is on the relevant objects and classes in the R implementation and how e.g. the Voronoi/Dirichlet tesselation is handled on a linear network such as a road map.
",r,2019
"The R spatial ecosystem is blooming and dealing with spatial objects and spatial computations has never been so easy. In this context, the cartography package aim is to create thematic maps with the visual quality of those designed with classical mapping or GIS tools. The package helps to design cartographic representations such as proportional symbols, choropleth, typology, flows or discontinuities maps. It also offers several features that improve the graphic presentation of maps, for instance, map palettes, layout elements (scale, north arrow, title...), labels or legends. cartography is a mature package (first release in 2015), it has already been reviewed in both software and cartography focused journals (Giraud, Lambert 2016 & Giraud, Lambert 2017). It follows current good practices by using continuous integration and a test suite. A vignette, a cheat sheet and a companion website help new users to start using the package. In this presentation we will firstly give an overview of the package main features. Then we will develop examples of use of the package along with other spatial related packages.
Giraud, T., & Lambert, N. (2016). cartography: Create and Integrate Maps in your R Workflow. The Journal of Open Source Software, 1(4), 1-2.
Giraud, T., & Lambert, N. (2017, July). Reproducible cartography. In International Cartographic Conference (pp. 173-183). Springer, Cham.
",r,2019
"R allows for creating beautiful maps and many examples can be found. Cartography is an indispensable tool in analyzing spatial data and communicating regional patterns to your target audience. Current data sources often contain location data making maps a natural visualisation choice. While these detailed location data are fine for analytic purposes, derived statistiscs have the risk of disclosing information of individual persons. For example if one plots the spatial distribution of income or getting social welfare, sparsely populated areas may be very revealing. Statistical procedures to control disclosure have been readily available (sdcTable, sdcMicro), but those focus on protecting tabulated or microdata, not on protecting spatial data as such. R package sdcSpatial (https://github.com/edwindj/sdcSpatial) allows for creating maps that show spatial patterns but at the same time protect the privacy of the target population. The package also contains methods for assessing the associated risk for a given data set.
",r,2019
"The R package ""colorspace"" (http://colorspace.R-Forge.R-project.org/) provides a flexible toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in statistical graphics and data visualizations. In particular, the package provides a broad range of color palettes based on the HCL (Hue-Chroma-Luminance) color space. The three HCL dimensions have been shown to match those of the human visual system very well, thus facilitating intuitive selection of color palettes through trajectories in this space.
Namely, general strategies for three types of palettes are provided: (1) Qualitative for coding categorical information, i.e., where no particular ordering of categories is available and every color should receive the same perceptual weight. (2) Sequential for coding ordered/numeric information, i.e., going from high to low (or vice versa). (3) Diverging for coding ordered/numeric information around a central neutral value, i.e., where colors diverge from neutral to two extremes.
To aid selection and application of these palettes the package provides scales for use with ggplot2; shiny (and tcltk) apps for interactive exploration (see also http://hclwizard.org/); visualizations of palette properties; accompanying manipulation utilities (like desaturation and lighten/darken), and emulation of color vision deficiencies.
",r,2019
"Vega-Lite, alongside Vega, is a JavaScript implementation of an interactive grammar-of-graphics, developed by the Interactive Data Lab at the University of Washington. You build chart specifications using JSON; Vega(-Lite) renders your specifications as charts in your browser.  The vegawidget package (on CRAN) is an htmlwidgets interface to Vega(-Lite), letting you compose specifications using R lists. The package offers functions to help build chart specifications and to render them as htmlwidgets. It also offers functions to define interactivity between Shiny and Vega(-Lite) via datasets, events, and signals (reactive variables). You can also define interactivity using JavaScript in an R Markdown document.  Although Vega-lite offers an interactive grammar-of-graphics, this package offers a low-level interface for composing chart-specifications. As a result, vegawidget is designed to be extensible, making it easier to develop higher-level, user-friendly packages to build specific types of charts, or even to build a general ggplot2-like framework, using vegawidget as a foundation. Package website: https://vegawidget.github.io/vegawidget
",r,2019
"The tour is a tool for the visualisation of multi-dimensional structures by means of dynamic projections, implemented the R package tourr (Wickham et al., 2011). Availability in R means that we can readily extend it with features from other packages. In this talk I will show how we can use Shiny and plotly to create a graphical interface, enhancing usability for non-experts and allowing for interactive features like linked brushing in the tour display, stopping and restarting with new settings, or hover text information on data points. In addition I will show how index functions scoring the ""interestingness"" of 2D projections, available in various R packages, can be combined with the guided tour, steering projections towards more interesting views of the data.
",r,2019
"R is the lingua franca of statistical computing, but it lacks a strong API for interactive, dynamic graphics. Shiny and ggvis are excellent, but they do not support dynamic actions (brushing, geodesic rotations, etc.). XLISP-STAT and R were the dominant computing and graphics platforms in the nineties and early 2000s, but it became clear that only a single open source platform was viable and the community chose R. However, it is now common to use multiple platforms, e.g., R and Spark or R and Python. xstatR in an environment that combines R and XLISP-STAT within a single Docker container (https://github.com/jharner/xstatR). R and XLISP-STAT run as separate Linux processes in a container with a bridge between them, which allows saved R objects to be translated into Lisp objects (or vice versa). Models typically are built in R and exploratory and diagnostic dynamic plots are created in XLISP-STAT. Since XLISP-STAT also supports windows, menus, etc., a graphical interface has been developed which obviates the need for users to learn Lisp. Prototype XLISP-STAT packages have been built for model-based interactive diagnostic plots, multivariate visualizations, and GGobi-type dynamic graphs. The xstatR container can be deployed locally or to any cloud service, e.g., AWS. We are refactoring xstatR to run XLISP-STAT as a subprocess of R, which will allow XLISP-STAT to more-directly access R objects.
",r,2019
"Hail is a Spark-based framework for genomic computing at scale. We have explored the application of deferred evaluation to the construction of an interface between R and Hail. The interface implements standard base R and Bioconductor APIs on top of Hail by constructing expressions in Hail's interface language and evaluating them using sparklyr. Users require no special knowledge of Hail or Spark. We will describe the design of the interface and demonstrate the manipulation of a Hail-backed SummarizedExperiment object, the core abstraction for genomic data in Bioconductor.
",r,2019
"Data exploration is crucial in the comprehension of large biological datasets, generated by high-throughput assays such as sequencing, with interactivity as key aspect to generate insightful outputs. Most existing tools for intuitive and interactive visualization are limited to specific assays or analyses, and lack support for reproducible analysis. Sparked from a Bioconductor community-driven effort, we have built a general-purpose tool, iSEE - Interactive SummarizedExperiment Explorer, designed for interactive exploration of any experimental data which can be stored in a SummarizedExperiment object, i.e. an integrative data container for storing matrices of assays and tables of associated metadata. iSEE (https://bioconductor.org/packages/iSEE/) is implemented in R and Shiny, and is compatible with many existing R/Bioconductor packages for high-throughput biological data. Essential features include: - A highly customizable interface with different panel types, simultaneously viewing and linking panels to each other
- Automatic tracking of the exact R code generating all visible plots for full reproducibility
- Interactive tours to showcase datasets and findings
- Extendable analyses with custom panel types
- Seamless deployment as an online companion browser for collaborations and publications.
",r,2019
"Similarly to other high-throughput technologies, metabolomics usually faces a data mining challenge to provide an understandable and useful output to advance in biomarker discovery and precision medicine. Biological interpretation of the results is one of the hard points and several bioinformatics tools have emerged to simplify and improve this step. However, sometimes these tools accept only very simplistic data structures and, for example, they do not even accept data with several covariates. POMA is a free, friendly and fast Shiny interface for analysing and visualization data after an analytical targeted metabolomics process and its hosted on https://polcastellano.shinyapps.io/POMA/. POMA allows the user to go from the raw data to statistical analysis. The analysis is organized in three blocks: ""Load Data"" (where user can upload metabolite data and a covariates file), ""Pre-processing"" (value imputation and normalization) and ""Statistical analysis"" (univariate and multivariate methods, limma, correlation analysis, feature selection methods, random forest, etc.). These steps include multiple types of interactive data visualization integrated in an intuitive user interface that requires no programming skills. Finally, POMA also generates different automatic statistical and exploratory reports to facilitate the analysis and interpretation of the results.
",r,2019
"The Bioconductor project has had profound influence on the statistical analysis and comprehension of high-throughput genomic data, while contributing many innovations to the R language and community. Bioconductor started in 2002 and has grown to more than 1700 packages downloaded to ½ million unique IP addresses annually; Bioconductor has more than 30,000 citations in the scientific literature, and positively impacts many scientific careers. The desire for open, reproducible science contributes to many aspects of Bioconductor, including literate programming vignettes, multi-package workflows, teaching courses and online material, extended package checks, use of formal (S4) classes, reusable ‘infrastructure’ packages for robust and interoperable code, centralized version control and support, nightly cross-platform builds, and a distinctive release strategy that enables developer innovation while providing user stability. Contrasts between Bioconductor and R provide rich opportunities for reflection on establishing open source communities, how users translate software into science, and software development best practices. The ever-changing environment of scientific computing, especially the emergence of cloud-based computation and very large and heterogeneous public data resources, point to areas where Bioconductor, and R, will continue to innovate.
",r,2019
"Model-based clustering aims at partitioning observations into groups based on either finite or infinite mixture models. The mixture models used differ with respect to their clustering kernel, i.e., the statistical model used for each of the groups. The choice of a suitable clustering kernel allows to adapt the model to the available data structure as well as clustering purpose. We first give an overview on available estimation and inference methods for mixture models as well as their implementations available in R and highlight common aspects regardless of the clustering kernel.
Then, the design of the R package flexmix is discussed pointing out how it provides a common infrastructure for fitting different finite mixture models with the EM algorithm. The package thus allows for rapid prototyping and the quick assessment of different model specifications. However, only a specific estimation method and models which share specific characteristics are covered. We thus conclude by highlighting the need for general infrastructure packages to allow for joint use of different estimation and inference methods and post-processing tools.
",r,2019
"The Chrome web browser can be fully controlled in headless mode. A headless browser is a convenient tool for web scraping and automated testing of web sites: one can program all the actions performed in a web browser thanks to the chrome devtools protocol. It is also useful for creating a PDF or taking a screenshot of a web page. Headless Chrome automation is widely used in the node.js ecosystem mainly through the Puppeteer library.
In this talk, we will show you how to take the full control of Chrome directly from R without any dependency on another language or tool (node.js or Java are not required). We will be looking at the principles that make it possible and at some working examples like the chromeprint() function in pagedown package. We will also illustrate what is possible with some examples using the in-development crrri package (https://github.com/RLesur/crrri) that offers a low level access to the chrome devtools protocol.
",r,2019
"Shiny has become a must in the R environment. In any sector of activity, {shiny} is no longer only used for prototypes but also for applications in production, to create reports, dashboards, tools... Shiny offers a basic foundation for application development and has a very rich API that allows developers to write their own extensions. During our journey with shiny we were able to write several packages to include new features and take advantage of its hidden gems. We will therefore present you our ecosystem of packages around Shiny: - Make the interface more attractive with {shinyWidgets} - Build Proxy for htmlwidgets for smooth integration into Shiny, an example with {billboarder} - Minimal busy indicator with {shinybusy} - Monitor app usage with {shinylogs}
",r,2019
"auth0 is a freemium service used to add authentication in web applications. We present the auth0 R package, which implements solutions to use auth0 with Shiny apps. With a simple interface, it is possible to add authentication interfaces to shiny apps, including social (Google / Facebook / Twitter / Linkedin), enterprise (Active Directory / LDAP), app creator's own database and many others. The auth0 R package needs only two things to work: create a configuration YML file with auth0's API keys, and replace the shiny::shinyApp() function with auth0::shinyAuth0App(). The package also includes i) helpers to collect and operate the logged user information inside the app and ii) logout buttons. The auth0 service can be used offline (localhost), in a shiny-server or even with RStudio's shinyapps.io service. In this lightning talk, we will show how auth0 R package works and, if possible, a live demonstration of an app running with auth0.
",r,2019
"Shiny applications are ubiquitous nowadays and traditionally created as a collection of R script files (optionally with external resources, such as images, css files etc). When the applications grow larger it may become difficult to manage these files.
I will advocate the use of R packages for creation of shiny applications. This approach allows to leverage all the advantages of the R packaging ecosystem, including managing namespaces and dependencies, versioning, documentation and tests. These are especially important when deploying and supporting shiny applications in production. In particular I will talk about using functions to define both ""server"" and ""ui"" components, and separating logical application pieces into shiny modules for easier structuring and sharing.
The aim of this talk is to encourage people to try this approach by discussing the differences with the traditional (file-based) approach, advantages of having an R package and things to watch out for. Finally I will mention how a packaged shiny application can be deployed in an enterprise context using ShinyProxy.
",r,2019
"Have you ever wanted to deliver your model and Shiny application to someone that can't allow their data to leave their computer, or to deliver your Shiny application as a quick prototype that a user could use as a simple Desktop application without deploying over the Internet? Last year at useR! 2018 Katie Sasso showed how to do so by building an Electron-Shiny executable, which allows you to deliver your Shiny app as a Desktop application (https://www.youtube.com/watch?v=ARrbbviGvjc).  This year Abbas Rizvi is going to take it a step further and show a super easy was to use a newly available RStudio Add-In that makes the process super simple and convenient. RStudio Add-Ins allow the IDE to be extended and customized, creating automated and faster work streams. (https://www.rstudio.com/resources/webinars/understanding-add-ins/). After this talk you will be able to create a Windows or Mac Desktop Shiny App with a few simple clicks of the RStudio Add-in.
",r,2019
"When using huge amounts of fleet data data scientists like me often use Shiny in combination with Leaflet to further investigate such data and furthermore to present the user a first prototype of a future data product. When using Leaflet, you can quickly reach limits on the amount of data. As a rule of thumb, a maximum of 10,000 data points for visualization seem possible here. The solution is often clustering, but sometimes you want to be able to see all the data. Therefore a tile layer based approach is used for the visualization. With this technique, there is virtually no limit to the amount of data. In this Talk I will show how R can be used with the packages Leaflet and Plumber to accomplish this. As an example record, millions of taxi pickups in NYC are used. The data is here persisted in a NoSQL database like Cassandra.
",r,2019
"Researchers and practitioners nowadays have access to large longitudinal datasets. These datasets often need to be aggregated by groups in order to be utilized in statistical analyses or machine learning models, and to enable better understanding by users. Especially for large datasets, this process can be difficult and is often prone to errors.  We present the package ""fxtract"", which provides an easy to use API to facilitate the extraction of user-defined features from grouped data. Similar to the summarize-functionality of the dplyr package, our package helps reducing multiple rows of data into one row for each group. fxtract offers easy manageability of large datasets, since data for each group is only read into memory when needed. The package is written in R6 and parallelization is supported by using the R-Package future.   Project page: https://github.com/QuayAu/fxtract
",r,2019
"Open Source Routing Machine (OSRM) is a high-performance routing engine for calculating the shortest paths through a road network. These calculations are available via Google Maps. However, queries against a local OSRM server are orders of magnitude faster than Google Maps. The osrm package for R exposes these routing data to a wide range of potential applications. In this talk I'll show how to easily spin up and provision an OSRM server and use it to solve some interesting spatial optimisation problems in R.
",r,2019
"Internet business brings huge amount of data and daily challenges. For us at trivago it is very important to correctly detect nonhuman traffic or detect anomalies. We are going to share our real example of an obvious anomaly with business impact: 
 - How was the anomaly detected? 
 - What caused the anomaly? Are we able to answer this confidently? 
 - What action should we take? 
 - How can, and can't R help us?
",r,2019
"Of the existing R packages that help analyze baseball statistics, few are geared toward fantasy baseball, which benefits from projection data for drafting a team, and updated data for daily analysis to set game day rosters and maintain a fantasy team. This talk describes how to use the tidyverse package suite to bring in baseball projected and actualized data, analyze baseball statistics relevant to fantasy baseball, draft a competitive fantasy baseball team, and maintain this team throughout the course of a season. By applying the tidyverse package suite of data cleaning and visualization tools to baseball statistical analysis, R users already familiar with this popular suite of packages can learn how to play and succeed at fantasy baseball. Users not already familiar with fantasy sports can expect a quick primer on fantasy baseball and statistics, sourcing baseball data for analysis, and analytic approaches to apply to a league of their own. 
",r,2019
"Sleeping child is an everyday dream come true of every young parent. Unwilling to fall asleep child can change one's life into a nightmare and those, who calmly sleep long hours used to be the object of desire. There are many functions, that the parents try to optimize - some simply want their children to sleep as much as possible, some want to wake up later and enjoy their kids in the evenings or the opposite (you cannot have it all), in our case we also wanted to synchronize sleeping times of our two children. To reach the goal, they try to find some patterns in children’s behavior – they impose some daily routines, do not let children nap too long during the daytime or try to put them in bed earlier, hoping that slower pace will lull them. However intuitions may be misleading, and that is why after a few months of observing my children sleeping patterns I decided to hire statistical tools. I wanted to verify the hypothesis if we should wake them up earlier in the morning (which in case of our kids turned out to be true), but the framework allowed me to detect also some more useful patterns and draw hints. After first attempts with linear regression, I decided to introduce my kids to the neural network and let it learn. My further idea is to create a web app, where one can upload a data file with observed sleeping times and habits, which would help draw reliable conclusions. 
",r,2019
"Selecting submissions for a conference can be viewed as a measurement problem: in principle, organisers aim to accept the ""best"" submissions, but the precise manner in which this is achieved can vary considerably. It is also common to involve multiple reviewers in the process, and it may not always be the case that all reviewers manage to rate all submissions. Hence, there is a chance that some particularly harsh reviewers may rate the same submission (and put it at a disadvantage), or some more lenient reviewers may happen to rate the same submission and propel it higher in the ranking. A solution to this issue is offered by multi-faceted Rasch models, which view submission scores as a function of not just the quality of the submission in itself, but also reviewer severity. This allows to adjust submission scores accordingly, providing a fairer measurement process. Conveniently, the R package `TAM` allows to estimate this type of model. In this talk, I will walk you through an example of how `TAM` was used on data collected as part of the review process for a data science conference in Scotland.
",r,2019
"We propose a new methodology to study multivariate linear models. It consists in estimating the covariance matrix of the responses beforehand and plugging this estimator into a penalized regression model to perform variable selection. Depending on the application field and the data at hand, different penalizations are preferred. For example, recoursing on a l1-norm penalty performs a ""simple"" selection in the regression coefficients. In certain situations, we need to select the same variable for all the responses or a group of variables all together. This can be done by means of a penalty similar to the sparse group Lasso (Yuan and Lin (2006)). Another example is the fused-Lasso penalty, where one want to influence variables to have the same coefficient (Hoefling (2010)). We study an example in genomic for study water-use efficiency (WE) of vine. The covariables are the number of replicates of the different alleles of different genes and the responses characterize the WE of the different vines. The aim is to select genes or alleles that have an effect on this WE. We consider several ways to study the problem: either by selecting an allele of a gene for a specific response or by influencing it to have the same coefficients for all the responses. It can also be interesting to select all the alleles of the same gene together. The presentation will show how to do all of this with the VariSel package.
",r,2019
"Maximum spacing estimation (MSE) introduced by [1] is a method for estimating the parameters based on the probability differences of order statistics. More precisely, the method consists in maximizing of the geometric mean of spacings in the data, which are the differences between the values of the distribution function at sorted observations. MSE has been proved to be reliable method both for heavy-tailed models by [2] and lighted-tailed models by [1]. Currently, only the BMT package provides the MSE for a single distribution. In this talk, we study the development and integration of this method in the fitdistrplus package [3]. This latter package provides maximum likelihood and other methods for any distribution characterized by d, p, q functions. Using the S3 class and generic methods, this estimation method nicely fits the package framework. An uncertainty analysis is carried out on simulated datasets from both light and heavy tailed models. Finally, we illustrate the relevancy of this method to model insurance losses on real actuarial datasets.
[1] Ranneby, Bo (1984). The maximum spacing method. An estimation method related to the maximum likelihood method. 
[2] Wong, T.S.T; Li, W.K. (2006). A note on the estimation of extreme value distributions using maximum product of spacings. 
[3] M. L. Delignette-Muller, C. Dutang (2015). fitdistrplus: An R Package for Fitting Distributions.
",r,2019
"Agent-based models (ABM) are discrete-event computer models focusing on the interactions between autonomous entities. The downsides of a high flexibility are that (i) ABM must be defined algorithmically (requiring programming) and (ii) ABM are black-box models, the study of which resort to large numbers of simulations. The GAMA modeling platform offers a user-friendly language (GAML) and environment to develop and efficiently run ABM, potentially in an explicit spatial environment. It lacks, however, the statistical tools to exploit ABM simulations. The rama package is an R interface to GAMA intended to fill this gap. The package defines the S3 class experiment that inherits from the tbl_df class and that is tidyverse compatible. An experiment object is linked to an ABM model defined in a GAML (text) file and contains, for this model, a number of simulations (in rows) that differ by their seed and/or parameters values (in columns). Methods are available to intuitively subset and combine experiment objects, as well as to generate experimental designs for experiment objects. Among them, run_experiment() seamlessly calls the GAMA engine to run the simulations of an experiment object and returns it augmented with a list-column of data frames of time series of the simulations outputs. Simulations outputs can then be analysed in R for model calibration, sensitivity analysis and hypotheses testing.
",r,2019
"The set cover problem is of fundamental importance in the field of approximation algorithms: Given a collection of sets, find the smallest sub-collection, so that all elements from a universe are covered. A diverse range of real world problems can be represented as set cover instance such as location-allocation, shift planning or virus detection. An optimal solution to the problem via linear programming is available. Due to the computational costs involved, this is not a feasible solution for large problems. A quick approximation is given by the greedy algorithm. This talk introduces RcppGreeySetCover, an implementation of the greedy set cover algorithm using Rcpp. The implementation is fast due to the reliance on efficient data structures made available by the Boost headers and the C++ 11 standard library. Preprocessing of input data is done efficiently via the data.table package. Input and output data is a tidy two column data.frame.  As a case study, we apply the package on a large scale (>= 100 million rows) hospital placement problem, which initially motivated the creation of the package.
",r,2019
"The GPareto package provides multi-objective optimization algorithms for expensive black-box functions and an ensemble of dedicated uncertainty quantification methods. Popular methods such as efficient global optimization in the mono-objective case rely on Gaussian processes or Kriging to build surrogate models. Several infill criteria have also been proposed in a multi-objective setup to select new points sequentially and efficiently cope with severely limited evaluation budgets. They are implemented, in addition to Pareto front estimation and uncertainty quantification visualization in the design and objective spaces. Rather than estimating the entire Pareto front, the GPGame package focus on finding equilibrium solutions (e.g., Nash equilibrium) that are better suited for larger number of objectives.
",r,2019
"Presenter: Peter Balazi Focus: Historical and future usage of R in credit risk modeling and its application Abstract: The highly regulated commercial banking industry has been slow and in general averse to change. To blame is the system legacy, processes but also managerial unwillingness. Historically and to these days, the commercial banking industry, especially risk management area, has been dominated by large software companies, SAS and alike. However this is changing rapidly, mainly led by the inner initiative of software end users. Now also increasingly recognized by management, of a need to change and adapt to the demand of end-users by realizing its potential and align to market place to attract talent. This presentation will briefly discuss the historical, current and future direction of a transition from typical conventional commercial type of statistical software (SAS) to R. Practical industry examples will be provided/shown where the choice of the statistical language is crucial and/or preferred in achieving the desired output giving the existing constrains. Brief view into this industry and best practice will be provided and encouragement given for R user community that better days lay ahead. Welcome for cooperation and sharing of knowledge will be also offered for those interested more details provided during the break in 1:1 session while approach in person.
",r,2019
"R++ is a new Graphical User Interface for R. It is intended for those who are not statisticians (doctors, psychologists, salespeople,...) but who nevertheless need high-level professional statistics. R++ is a result of collaborations with different computer-science labs specialised in Human-Computer Interaction. Through twenty or so meetings with statisticians, we have identified the tasks that are considered to be arduous, annoying or of low added value. 3 aspects have arisen: reading data (encoding problems, column separators, odd formats, distant data bases), data managment (outliers' detection, modalities' merging,...) and exporting results (text or graph). R ++ offers an easy-to-use interface simplifying processing these tasks. In reading the data, a preview is displayed allowing checking of the data and thus to proceed to various settings. For processing the data, several tools are available. For example, it is possible to instantly graphicaly represent all the variables of a database or to correct the types using a colour system. For exporting the data, interface allows adjustments to graphical settings, then a simple drag&drop can export graphs. Naturally, for the sake of reproducibility of analyses, each action generates the corresponding R code. A code editor and an R console are also available for more advanced uses. R++ is free for academics, students, and associations.
",r,2019
"The pharmaceutical industry has historically, overwhelmingly made use of one software package, SAS. Almost every submission made to regulatory agencies such as the FDA and the EMA have been made using SAS code, and SAS data formats. For those of us who are enthusiastic about promoting usage of R, this can lead to an uphill struggle, but things are changing. R has become more and more popular, and appetite for R usage is growing. In this talk I will discuss some of the efforts to promote R within Roche, with a focus on two particular methods: - Targeted training, focused on problems programmers have to solve every day - Tidyverse training, and a focus on functional programming - In house data and as close to real examples as possible, to show direct applications for work in R - New R packages. Multiple different packages have been developed to support different activities. In this talk I will focus primarily on diffdf, a package that is available on CRAN, which used allows detailed dataset comparison, meeting an unmet need
",r,2019
"We introduce Kasa AI, a community driven initiative for open research and software development in insurance and actuarial analytics. Open source software has been credited for recent rapid advances in machine learning and its applications in various industries. The insurance industry, being a heavily regulated industry, has been slower to embrace open source, but recent trends indicate that actuaries are shifting their workflows to adapt to new technologies. We discuss motivations for the community, current projects, which span both life and nonlife insurance, and how tooling in the R ecosystem has enabled reproducible research at scale.
",r,2019
"The annual conference on the use of R in official statistics is since 2018 enriched by a side event similar to the rOpenSci Unconfs. Similar to the idea of the awesome official statistics software list, this should complement an abstract top-down approach such as the common statistical production architecture with fast bottom-up solutions. One of the implemented ideas 2018 was to provide an easy way to generate Voronoi treemaps in R. The motivation came from examples of the ""Price Kaleidocscope"" from the federal statistical office of Germany. The newly released CRAN package voronoiTreemaps provides the functionality to create Voronoi treemaps with the JavaScript library d3-voronoi-treemap and to integrate them into a shiny app.
",r,2019
"The UK Office for National Statistics (ONS) uses a model-based conditional ratio estimator to estimate the product by industry sales of products produced by the UK manufacturing sector (the PRODCOM survey) and provided by the UK services sector (the Annual Survey of Goods and Services - ASGS). As ASGS is a recently developed survey by the ONS, R was chosen to be part of the production system over other software packages because of its abilities to handle large datasets (approximately 2000 product by 300 industries by 40000 businesses) and complex calculations efficiently and in a timely manner. R also had its advantages in that the results can be easily visualised as part of the processing. Using the recent set of tidyverse based packages this was able to be done in such a way for the methodology to be implemented concisely. This talk will briefly describe the methodology, the pipeline of functions created and a comparison of the outputs to the design-based Horvitz-Thompson (expansion) estimator. It is the author's hope that this pipeline will be packaged up and be made available for other National Statistics Offices and interested researchers who have surveys which require a similar estimation scheme. 
",r,2019
"NGS-based assays for cancer testing have been increasingly adopted by clinical laboratories, especially targeted gene panels which are now commonly used to guide diagnostic and therapeutic decisions. While there are many databases for the annotation and curation of individual variants, there is a lack of off-the-shelf software for the mining of those databases.   The Pathology Department at Peter MacCallum Cancer Centre conducts variant curation using in-house software, PathOS. This database contains variant information from thousands of clinical tumour samples, from a variety of NGS assays (mostly gene panels), and a range of tumour and tissue types. Visualising variations across all clinical contexts can be used to validate important findings, improve patient treatments and refine tumour classifications.    In this talk I will show how, by using advanced R packages, such as maftools and oncoplots, we are able to interrogate our database and make sense of variants from thousands of patients across multiple cancer types. I will discuss the need for implementing such solutions into our routine curation QC process, and will describe the challenges around working with data stored in various file formats and implementing solutions into a curation system in production.  
",r,2019
"The revolution in high-throughput sequencing technologies, which enables the acquisition of gigabases of DNA sequences, is leading to great applications for understanding the relationships from the genotype to the phenotype. The accumulation of massive amounts of omics data is then providing relevant biological information thanks to the integration of annotation information coming from sources available in the ""omics"" domains. These knowledge resources are then essential for interpreting the functional activity of genes, even though, managing the large number of annotation terms associated to a gene set level is a difficult task. To address this issue, we introduce rGSAn, a R package dedicated to the Gene Set Annotation. By adopting a strategy combining semantic similarity measures, and data mining approaches, rGSAn is able to perform an unified and synthetic annotation for a gene set of interest.To do so, the best partition of the annotation terms is computed using semantic similarity measures. Then, the most relevant terms are identified using a decision tree algorithm and an heuristic implementation of the general set cover problem. In addition, rGSAn offers new visualization facilities to interactively explore the annotation results according to the hierarchical structure of Gene Ontology.
",r,2019
"RNA-seq based expression profiling allows us to quantify gene expression changes in pertinent biology and to discover a set of biomarkers that are potentially the origin of the biological phenomenon of interest. Moreover, investigating the collective behaviour of the biomarker genes as a set, utilising databases such as Gene Ontology (GO), KEGG (Kyoto Encyclopedia of Genes and Genomes), STRINGdb (Search Tool for the Retrieval of Interacting proteins database), play an important role in gaining a comprehensive knowledge about the relevant biological inquiry. Here, we built a web-based interactive RShiny tool in order to gain further insights into the alterations within gene sets and the changes in the interactions between them. PathwayVisualiseR implements a fit object, which is the output from a linear model that was fit for each gene for a given series of samples, from the limma R package. PathwayVisualiseR, therefore, enables the user to incorporate the results from gene set tests, such as Camera, Roast and Fry. To summary, visualising gene set interactions with PathwayVisualiseR helps in speeding up the process of comprehending large or complex data sets and unfolds the relationship among the functional pathways and biological mechanisms.
",r,2019
"Understanding the global patterns and drivers of plant transpiration and its physiological control requires compiling and harmonising heterogeneous ecophysiological datasets. The SAPFLUXNET project (http://sapfluxnet.creaf.cat/) has compiled the first global database of transpiration from sap flow measurements, including > 10 million sub-daily records from 2732 plants and 176 species, measured in > 200 globally distributed datasets.  Here we present the SAPFLUXNET data infrastructure, which allows implementing a reproducible workflow in the R environment. The ""sapfluxnetQC1"" package (https://github.com/sapfluxnet/sapfluxnetQC1) is an internal-use package which implements data harmonisation and quality control, using Rmarkdown-generated reports and interactive Shiny apps. The ""sapfluxnetr"" package (https://github.com/sapfluxnet/sapfluxnetr) is aimed to ease data access, aggregation and analysis, using two new S4 classes (""sfn-data"", and ""sfn_data_multi""). The SAPFLUXNET database and its data infrastructure have been designed to be open and publicly available, supporting new data-driven analysis of global vegetation functioning and also facilitating future updating and maintenance.
",r,2019
"Although Bayesian methods are expanding considerably, their applications in the field of pharmacokinetic/pharmacodynamic (PK/PD) modelling is still relatively limited. In this work, Bayesian techniques are used to estimate a novel PK/PD model developed to quantify the PD synergy between two compounds using historical in vivo data. The model is fitted using package rstan, the R interface to Stan. Stan is a recently developed software package which allows an efficient estimation using the No-U-Turn Sampler (NUTS). rstan facilitates the use of this powerful tool, allowing to run Stan models within R. Since the data consist of a series of trials performed sequentially, a Bayesian sequential integration is considered: the posteriors resulting from the analysis of one trial are used to specify the priors of the next trial. Challenges and opportunities of this technique will be discussed with respect to: (i) prior specification, (ii) random effect choice, (iii) experimental design. In addition, the results from an extensive simulation study assessing the performance of the Bayesian sequential integration for an increasing model complexity are evaluated. The results suggest that the Bayesian sequential integration is promising in certain settings. The specification of informative priors and a suitable experimental design are nevertheless advisable, especially in preclinical studies.
",r,2019
"In vaccine clinical development, it is important to assess the ability of a candidate vaccine to generate immune responses. Cellular immunogenicity is usually measured by Intra-Cellular Staining (ICS) after specific stimulation. The usual method for the analyzing such data is to i) first subtract the response observed in non-stimulated cells from each stimulated sample; ii) then perform an inter- or intra-arm comparison of the percentages of cells producing cytokine(s) of interest. Step i) aims at capturing the antigen-specific response, but the subtraction may induce biased estimates and compromise type 1 error and harm statistical power. We have proposed a bivariate regression model as a new method for an accurate estimation of the vaccine effect from ICS data in vaccine trials. This allows to model a linear relationship between the non-stimulated response and the antigenic responses, while taking into account both of their measurement errors, taking all available cellular response information into account. Our method displays good performances in terms of bias, type I error control and statistical power under various scenarios in numerical simulations. We present a user-friendly R Shiny application (relying on the nlme package) that we implemented for immunologists to be able to use our method directly, and we applied it to analyze data from two HIV vaccine trials.
",r,2019
"Modern microscopy techniques allow to image a high number of biological samples (cells, tissues, whole organs and organisms). Some tools even allow to follow a sample over time. Image processing techniques can extract many information from the images about biological objects (number, location in 2D or 3D space and time, biological status, shape...). Analysis of such biological data is facing to two main challenges: the need to take into account the spatial and temporal context of each piece of information, and the need to gather and compare many images pre-processed with multiple softwares from many samples. The R package {cellviz3d} (https://github.com/marionlouveaux/cellviz3d) is a wrapper on the {plotly} package to help visualization of meshes and points structures in 2D, 3D and 3D+time of one or several samples. Data can be extracted directly from images in R or with image analyses softwares like Image/Fiji or MorphoGraphX. Typical use cases that will be presented in this talk are the visualization of the surface of a biological tissue or the visualization of nuclei. Data presented will be data outputs read with packages like {mgx2r}, which bridges MorphoGraphX with R, or {mamut2r}, which bridges MaMut Fiji plugin with R, both available on GitHub (@marionlouveaux).
",r,2019
"Increasing medical expenses throughout the world necessitate more reasonable utilization of all healthcare resources, as well as clinical laboratory tests. Laboratory tests are known to be ordered either to screen individuals for diseases or to confirm the diagnosis of individuals with disease symptoms. Diagnostic tests should be utilized appropriately according to current guidelines and clinical algorithms. Overutilization of tests is considered as incorrect practices in both medical and financial contexts. In this study, to assess utilization of test requests in a hospital environment, we applied apriori based method of association analysis to determine the relationship between different laboratory tests and clinical settings. Using the apriori and shiny package we have development interactive application. Our application is user-friendly and users who are not familiar with R programming or statistical software can import data from Laboratory Information System and oversee test utilization. Managing test requests is an emerging topic in clinical laboratory; data science tools can help laboratory medicine specialists to be more proactive.
",r,2019
"At Netflix, our data scientists apply machine learning to an ever-increasing range of business problems, from title popularity predictions to quality of streaming optimizations. However, building and operating production grade machine-learning systems is a highly non-trivial endeavor requiring expertise in two highly evolved disciplines: systems engineering and ML. To empower our data scientists to be able to build, deploy and operate machine learning solutions without any external support, we provide a robust infrastructure for machine learning, ensuring models can be promoted quickly and reliably from prototype to production, and enabling reproducible and easily shareable results. Given the breadth of techniques and the state-of-the-art algorithms available within the R ecosystem, a significant fraction of our data scientists, prefer R as their lingua franca, necessitating the need to provide first-class infrastructure support in R. In this talk, we introduce the techniques and underlying principles driving our approach with a particular focus on models written in R. We talk about Metaflow, our human-centric machine learning infrastructure, which enables our users to iterate on their machine learning pipelines quickly by leveraging existing libraries and familiar abstractions.
",r,2019
"Data scientists face multiple challenges when deploying machine learning models at the scale required for many commercial applications. In this talk, I will discuss best practices for deploying R models in production, including: how to operationalize predictive workflows with event- or frequency-based scheduling; how to scale model training and scoring operations with cloud-based clusters of virtual machines; how to maintain parity between development and production environments using Docker containers; the use of continuous integration and continuous delivery (CI/CD) tools to build and test production code. I will present a demo from a retail product forecasting context, in which R models are deployed in an end-to-end predictive workflow on cloud-based architecture.
",r,2019
"R is a great language for rapid prototyping and experimentation, but putting an R model in production is still more complex and time-consuming than it needs to be. With the growing popularity of serverless computing frameworks that offer Functions-as-a-Service (like AWS Lambda, Azure Functions) or Container-as-a-Service (ECS and ACI) we see a a huge chance to allow R developers to more easily deploy their code into production. We want to show you how you can use serverless computing to easily put models in production. We will discuss the pros and cons of various approaches and how we implemented a completely serverless data science platform for R in Microsoft Azure that you can arbitrarily scale (as long as your credit card allows:) up and down. While we come from an Azure background, porting the ideas over to AWS or Google Cloud should be straight forward.
",r,2019
"So you've built an amazing model in R. It generates great predictions or recommendations on your desktop. Now, how do you get it into production? Deploying R functions within Docker containers, and exposing them with the 'plumber' package, is a simple and effective way to integrate R into applications via a REST API. We can further provide scale for high-volume workloads by deploying that container to Kubernetes. As the complexity grows, however, managing and updating deployments can be challenging. In this talk, we describe a CI/CD based process in Azure DevOps to automate the entire build, test and deploy process for R-based models in production. The process emphasizes model reproducibility, by capturing and tracking changes in the code, data, tests and configurations that define the model. You will learn how, with a check-in to GitHub as the trigger, your model can be automatically retrained, optimized, built, validated, and – with human approval if necessary – released. Once deployed to the production environment – in this talk we'll focus on scalable open-source frameworks like Kubernetes – the model will be subject to continuous monitoring and performance tracking until such time that a new model version is warranted, and the DevOps lifecycle begins again.
",r,2019
"Application Programming Interfaces (APIs) have become the most common way how services ""talk"" to each other, e.g. in a typical frontend-backend design. The plumber package offers capabilities to implement an API in R, making it possible to use R for software development use cases that often require security best practices. In my talk, I will show how we can use plumber filters to secure plumber APIs. I then present the sealr package (github.com/jandix/sealr) which provides standardized strategies for authentication and authorization, namely JSON Web Tokens (JWT) (implemented), OAuth2 and general API tokens (under development at time of submission). sealr is inspired by the manifold passport.js package for Node.js. The main functionality of sealr is verifying tokens sent to plumber - how those tokens are issued by plumber is not covered (yet) as it is highly application-specific. However, we provide implementation examples for each method. As authentication middleware specifically developed for the plumber framework, sealr differs from packages such as sodium, openssl, jose and bcrypt that implement specific encryption and/or hashing algorithms. secret, digest and keyring are used for securely storing (R) objects. With sealr, we aim to make authentication and authorization as easy as possible to implement for R users so that plumber APIs can be used in security-sensitive environments.
",r,2019
"pak is a new package manager that makes package installation fast, safe and convenient. Fast   Fast downloads and HTTP queries. pak performs all HTTP requests concurrently.   Fast installs. pak builds and installs packages concurrently.   Metadata and package cache. pak caches package metadata and all downloaded packages locally. It does not download the same package files over and over again.   Lazy installation. pak only installs the packages that are really necessary for the installation.   Safe   Private library (pak's own package dependencies do not affect your regular package libraries and vice versa).   Every pak operation runs in a sub-process, and the packages are loaded from the private library. pak avoids loading packages from your regular package libraries.   pak warns and requests confirmation for loaded packages.   Dependency solver. pak makes sure that you end up in a consistent, working state of dependencies. If finds conflicts up front, before attempting installation.   Convenient   BioC packages. pak supports Bioconductor packages out of the box. It uses the Bioconductor version that is appropriate for your R version.   GitHub packages. pak supports GitHub packages out of the box. It also supports the Remotes entry in DESCRIPTIONfiles, so that GitHub dependencies of GitHub packages will also get installed.    Package sizes. For CRAN packages pak shows the total sizes of packages it would download.
",r,2019
"data.table is an R package that allows for fast and memory efficient data manipulation with a concise and flexible syntax. It has over 8000 questions on StackOverflow, is frequently a featured package and one of the most downloaded packages (https://www.r-pkg.org), has over 60 contributors (https://github.com/Rdatatable/data.table/graphs/contributors) and is active in development for over 10 years.
Over the last couple of years, several new functionalities and improvements, including parallelisation of several operations, have been made including file reading (`fread`), file writing (`fwrite`), subset operations, automatic secondary indexing, ordering, non equi joins, overlapping joins, rolling functionalities etc.
In this talk, all such improvements will be summarised showcasing different tasks of varying complexity that can be tackled using data.table with ease and with little code. The intuition behind the syntax of data.table will also be explained.
Finally, a quick benchmark of some common tasks will be highlighted to clearly show the performance of data.table. This is particularly useful to R users who might benefit a lot from performance.
Main author: Matt Dowle, hacker at H2O.ai
Co-author: Arun Srinivasan (me), Analyst, Millennium Management.
Project page: https://github.com/Rdatatable/data.table/wiki
",r,2019
"File import in R could be considered a solved problem, with multiple widely used packages (data.table, readr, and others) providing fast, robust import of common formats in addition to the functions available in base R. However I feel there is still room for improvement in existing approaches. vroom is able to index and then query multi-Gigabyte files, including those with categorical, text and temporal data, in near real-time. This is a huge boon for interactive data analysis as you can jump directly into exploratory analysis without sampling or long waits for full import. vroom leverages the Altrep framework introduced in R 3.5 along with lazy, just-in-time parsing of the data to provide this improved latency without requiring changes to existing data manipulation code. I will throughly explain the techniques used in vroom to ensure good performance, describe challenges overcome in implementing it, and provide an interactive demonstration of its capabilities.
",r,2019
"In this talk, I'll present the future framework, how to use it, what is new, and what is on the roadmap. The future ecosystem provides a simple, unified framework for parallel and distributed processing in R. It allows you to ""write parallel code once"" regardless of the operating system and compute environment. At the same time, it allows the user to decide on where and how to parallelize your code. In programming, a 'future' is an abstraction for a value that may be available at some point in the future. The non-blocking nature of futures makes them ideal for asynchronous evaluation of expressions in R, e.g. in parallel on the local machine, on a set of remote machines, via an HPC job scheduler, or in the cloud. At its core, there are two construct: 'f
",r,2019
"FastR is an open source alternative R implementation that aims to be compatible with GNU-R, provide significantly faster execution of R code, and high-performance interoperability with other languages including Python. Although FastR can run complex R packages including Rcpp, dplyr or ggplot2, achieving near full compatibility with GNU-R is still work in progress. However, there is already a lot that FastR can offer, but switching completely to FastR may look like a big step. The goal of FastRCluster is to provide the intermediate ground and let the GNU-R users easily try FastR and/or use FastR to run only selected parts of their apps. FastRCluster is an R package, targeted at GNU-R, that provides a seamless way to run FastR inside GNU-R using the existing interface of the parallel package, which also integrates well with the future and promise packages. The talk will, for example, present how FastRCluster can be leveraged in the context of Shiny applications.
",r,2019
"Visualization is an invaluable tool for gaining insight into complex high-dimensional data. Here we introduce prVis: a polynomial-based 2-D visualization method. PrVis, or polynomial visualization, captures potential nonlinear relationships in datasets by performing polynomial expansion and applying PCA on the resulting expanded polynomials. Our method produces an arguably better outcome on the famous Swiss Roll test case than do t-sne, UMAP, and ordinary PCA, and generally does as well or better than those methods on other datasets. PrVis offers options to further investigate how the clusters of data relate to the original variables. The prVis software package includes several features to support easier interpretation of visualized data, including color coding for discrete and continuous variables, alpha blending, adding row numbers to data points, and removing outliers. PrVis also provides users with options for random sampling and memory-mapping to better work with large datasets.
",r,2019
"Partial Least Squares (PLS) methods have been heavily exploited to analyze the association between two blocks of data. These powerful approaches can be applied to data sets where the number of variables is greater than the number of observations and in the presence of high collinearity between variables. Different sparse versions of PLS have been developed to integrate multiple data sets while simultaneously selecting the contributing variables. Sparse modeling is a key factor in obtaining better estimators and identifying associations between multiple data sets. The cornerstone of the sparse PLS methods is the link between the singular value decomposition (SVD) of a matrix (constructed from deflated versions of the original data) and least squares minimization in linear regression. We review four popular PLS methods for two blocks of data. A unified algorithm is proposed to perform all four types of PLS including their regularised versions. We present various approaches to decrease the computation time and show how the whole procedure can be scalable to big data sets. The bigsgPLS R package implements our unified algorithm and is available at https://github.com/matt-sutton/bigsgPLS
",r,2019
"Classification problems involving high dimensional data are extensive in many fields such as finance, marketing, and bioinformatics, with unique challenges with such datasets being numerous and well known. We first introduce the R package multiDA (https://github.com/sarahromanes/multiDA), a new method of performing high dimensional discriminant analysis. Starting from multiclass diagonal discriminant analysis classifiers which avoid the problem of high dimensional covariance estimation we construct a hybrid model that seamlessly integrates feature selection components. We compare our method with several other statistical machine learning packages in R, showing marked improvements in regard to prediction accuracy, interpretability of chosen features, and fast run time. We then introduce genDA (in active development), a DA method capable for use in multi-distributional response data - generalising the capabilities of DA beyond Gaussian response. It utilises Generalised Linear Latent Variable Models (GLLVMs) to capture covariance structure between the different response types and provide an efficient classifier for such datasets. This model leverages the highly efficient TMB package in R for fast and accurate gradient calculations in C++.
",r,2019
"In high-dimensional prediction problems, especially in the case where the number of features exceeds the number of observations, feature selection is an essential and often required tool. Component-wise gradient boosting is a modelling technique which provides embedded feature selection for additive statistical models. It allows automatic and unbiased selection of ensemble components from a pool of - often univariate - base learners. Boosting these kinds of models maintains interpretability of effects.  The R package compboost implements component-wise boosting in C++ and Rcpp. It provides a modular object-oriented system which can be extended either in R for convenient prototyping or directly in C++ for optimized speed, the latter at runtime without full recompilation of the framework. The package also allows visual inspection and selection of effects, feature importance, risk behaviour and optimal number of iterations. In contrast to the well-known mboost package, compboost is more suitable for larger datasets and also easier to extend, whereas compboost currently lacks some of the large functionality mboost provides.  Project page: https://github.com/schalkdaniel/compboost
",r,2019
"The VSURF package is a general tool to perform variable selection for regression or supervised classification problems. It implements a fully data-driven variable selection procedure based on random forests (RF). Its overall run-time depends obviously on the data characteristics (number of observations, n, and number of variables, p, mostly), but also on the computational performance of the randomForest package (since VSURF is heavily based on it). In the last few years, other packages implementing RF (ranger, Rborist) have emerged, and their authors claim that they are faster than randomForest: in the high-dimensional case (p very large and larger than n) for ranger, and in the Big data context (n extremely large) for Rborist. Therefore, one way to speed-up VSURF is to use those other packages instead of randomForest in the procedure. In this talk, RF basics are first given, then the VSURF procedure is detailed, and finally different implementations of RF into VSURF are compared in several frameworks and on different examples, to shed light on when each implementation can or cannot alleviate the overall computation burden.
",r,2019
"The timeseriesdb framework maps R time series representations to PostgreSQL key-value pair storage and links data descriptions in a relational manner. Combining key-value pairs with relational database features results in a light-weight but powerful architecture that keeps maintenance at a minimum as it allows to store a large number of records without the need for multiple partitions. The timeseriesdb framework was tailored to the needs of official and economic statistics with long term data conservation in mind: It handles data revisions (vintages), release dates and elaborate, multi-lingual meta information. Apart from implementation insights, this talk discusses how data management in a well etablished, enterprise level, relational database as opposed to file based management opens up the opportunity to use standard web technologies such as REST to easily build interfaces and expose time series data online.
",r,2019
"Modern time series are often high-dimensional and observed at high frequency, but most existing R packages for time series are designed to handle low-dimensional and low frequency data such as annual, monthly and quarterly data. The feasts package is part of new collection of tidyverts packages designed for modern time series analysis using the tidyverse framework and structures. It uses the tsibble package to provide the basic data class and data manipulation tools. feasts provides Feature Extraction And Statistics for Time Series, and includes tools for exploratory data analysis, data visualization, and data summary. For example, it includes autocorrelation plots, seasonality plots, time series decomposition, tests for units roots and autocorrelations, etc. I will demonstrate the design and use of the feasts package using a variety of real data, highlighting its power for handling large collections of related time series in an efficient and user-friendly manner.  
",r,2019
"The R ecosystem knows a vast number of time series classes: ts, xts, zoo, tsibble, tibbletime or timeSeries. The plethora of standards causes confusion. As different packages rely on different classes, it is hard to use them in the same analysis. tsbox provides a set of tools that make it easy to switch between these classes. It also allows the user to treat time series as plain data frames, facilitating the use with tools that assume rectangular data.
The package is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic, and allow the user to combine multiple non-standard and irregular frequencies. Because coercion works reliably, it is easy to write functions that work identically for all classes. So whether we want to smooth, scale, differentiate, chain-link, forecast, regularize or seasonally adjust a time series, we can use the same tsbox-command for any time series class. The talk provides a general overview of time series classes in R and shows how tsbox can be used to facilitate the interaction between them.
",r,2019
"More and more infra-annual statistics are produced to evaluate the short-term evolution of an economy (GDP, chocolate sales...) or even to analyse in detail the downloads of CRAN packages! Most of these series are affected by seasonal and trading days effects that must be removed to perform temporal and spatial comparisons. RJDemetra (https://github.com/jdemetra/rjdemetra and soon on CRAN) is an R interface to JDemetra+, the European seasonal adjustment software officially recommended by Eurostat and the European Central Bank and used by more than 80 institutes in the world. This package offers access to all options and outputs of JDemetra+ that implements the two leading seasonal adjustment methods TRAMO/SEATS and X-13ARIMA-SEATS and their pre-adjustment methods (automatic RegARIMA modelling, outlier detection, trading days adjustment). The presentation will highlight the main options of RJDemetra and show how all the resources available in R can be used to improve the production of seasonal adjusted series (producing dashboards, quality reports...) or to carry out studies.
",r,2019
"Sensors are omnipresent in our modern world. From manufacturing industry, finance, up to biology in nearly every domain sensors provide us with essential information. Although everybody seems to be well-equipped with sensors, operating these can be error prone. Especially missing values are a problem often encountered. Since follow-up analysis and processes often require complete data, missing data replacement ('imputation') is needed as a preprocessing step. In this talk we will give insights about our experiences on handling missing data in several industry related projects. We will also give a short introduction to the imputeTS package, which we developed as a result of these experiences.  Imputation itself is a well-established area in statistical surveys and CRAN offers a surprisingly large choice of packages (i.a. 'mice', 'missMDA', 'Amelia'). However, there are differences between imputation for statistical surveys and sensor time series. We will discuss typical problems arising with missing data imputation in sensor time series and also present for which kind of data / missing data patterns / missing data mechanisms imputation proved to be successful. Overall, this talk is supposed to provide a first glance at sensor data imputation in R along with giving an introduction to the imputeTS package.
",r,2019
"Over the last few years, a quiet revolution has been brewing in the R book publishing industry. Since the publication of early open source books, such as Advanced R (published in 2014), many authors have switched to a hybrid system, in which books are published online and in print. This approach has several advantages, including (1) people can choose how to read the book, online or in print, and make an informed decision before buying it; (2) the book retains the process of reviews and professional copy-editing provided by publishers; (3) the wider community can contribute, leading to many improvements in the code and text. Several developments have enabled this revolution, including the bookdown package, online version control services (e.g., GitHub), continuous integration services (e.g., Travis CI), and many more. This talk will share our experience of writing the open source book Geocomputation with R (https://geocompr.github.io/). We will show how to start writing an open-source book, which tools are useful, and how to collaborate on a large project while being a continent apart. We will also point out a few issues in the process, and how we navigated them. Overall, we advocate writing an open source book and hope the talk will be useful to others thinking about starting such a project.
",r,2019
"The Comprehensive R Archive Network (CRAN) is a diverse software ecosystem that is growing fast in size, both in terms of packages and package authors. The numbers at the time of writing this abstract are that CRAN involves more than 13.7K packages from about 19.2K authors. Keeping track of the ecosystem is, at least to me, hard, particularly in terms of effectively searching packages and authors through the network, summarising it and discovering new package and author associations. In this talk, I will introduce the cranly R package (https://github.com/ikosmidis/cranly) which streamlines and simplifies the analysis of the CRAN ecosystem. In particular, we will cover cranly's capabilities for cleaning up and organising the information in the CRAN package database, building package directives networks (depends, imports, suggests, enhances, linking to) and collaboration networks, and computing summaries and producing interactive visualisations from the resulting networks. I will also touch upon modelling, particularly on how directives and collaboration networks can be coerced to igraph (https://CRAN.R-project.org/package=igraph) objects for further analyses and modelling.
",r,2019
"Navigating the R Package Universe was intensively discussed at useR! 2017 and is still a hot topic. Ordinary R users expect to find the best packages that match their needs whereas task view maintainers have to identify all packages that fall in the scope of their task views. With the size of CRAN, the task is immense and a dedicated search engine is required. Some new solutions appeared in 2018 and 2019 and will be discussed in this talk.   Among them, the 'RWsearch' package was developed to facilitate the maintenance of the 'Distributions' task view (which is ranked number five by the number of referred packages) but can be used by every R users. It includes a comprehensive set of functions to download the packages and task view master files from CRAN, explore these files, search packages by keywords, date of publication and sophisticated options, display the results as a data.frame or as a classical text in console, txt, html or pdf outputs, download the whole package documentation in one click (a wonderful function for people who need to work offline) and, for task views, compare the results with the list of already referred packages. Since its inception, 'RWsearch' has helped us discover more than 100 new packages.   Along with the CRAN exploration tools, 'RWsearch' has direct links to more than 60 external web search engines. We expect this list to grow with the contribution of everyone.
",r,2019
"Not mastering English language can be a very high entrance barrier for people who want to learn how to program, specially in contexts like Latin America, where learning English is not accessible to everyone due to socioeconomic inequalities.  The aim of this talk is to present the collaborative translation to Spanish of ""R for Data Science"" (Whickham & Grolemund, 2017), a project that the Latin American R community is currently leading as a way to shorten this linguistic gap. Unlike other translations, this one not only considers translating the text to Spanish, but also the datasets and part of the code used in the book. After briefly describing the general context of the project, the presentation will focus on how we developed ""datos"" (Ruiz, Quiroga, Vargas & Lepore, 2019; https://cienciadedatos.github.io/datos/), the package that contains the R4DS datasets translated. The talk will present the motivations to develop this package, the workflow used for programatically translate the datasets using ""datalang"" package (Ruiz, 2018; https://github.com/edgararuiz/datalang), and the challenges we have faced. Opportunities for other non-English speaking R communities will be discussed.
",r,2019
"R Consortium (ISC) working groups are where the difficult collaborative work gets done, and there is a lot going on! In this talk, I will describe what is happening in some of the high profile working groups including the R / Medicine and R / Pharma conference planning groups; the R Validation Hub group that is developing validation standards for R packages for the pharmaceutical industry; The Census group that is working with the U.S. Census Bureau to improve the accessibility of Census Data, the R Certification group, which is attempting to establish a common certification program for proficiency in R; and the R Community Diversity and Inclusion Working Group (RCDI-WG) is to broadly consider how the R Consortium can best encourage and support diverse and inclusive community activities. Working groups are generally open to all interested individuals whether or not their companies are members of the R Consortium. I'll describe how you can get involved, or perhaps even start your own working group.  
",r,2019
"The development of new drugs in oncology often requires the identification of predictive biomarkers for patients where a specific drug is more beneficial. The multiplicity of available treatments as well as the vast collection of biomarkers makes the design of clinical trials difficult. A platform trial with an adaptive design allows the investigator to learn from the accumulating data and to modify prespecified components of the trial while it is ongoing. This can improve power, reduce the number of patients and costs, treat more patients with effective therapies, correctly identifying subgroup of responding patients, and shortening the time of the trial. However, adaptive designs are more complex than traditional studies and require particular attention in their design and analysis. I will present the design and implementation of the ProBio study, a biomarker driven platform trial for improving treatment decision in patients with metastatic castrate resistant prostate cancer. I will describe how to assess the operating characteristics (type I error and power) in R through extensive simulations and calibrations. I will illustrate Bayesian methods for survival analysis employed in the evaluation of the accumulating data, and outline the advantages of using R in the diverse phases of the trial, such as producing randomization lists, generating automatic reports, and relevant dashboards.
",r,2019
"During the last few decades, dose-response analysis, which is fitting and interpreting results obtained from using fully parametric nonlinear dose-response (regression) models, has undergone dramatic changes, from cumbersome, more or less manual calculations and transformations to blink-of-an-eye operations on any laptop. The first version of the R extension package ""drc"" for doing dose-response analysis was developed in 2005. Originally, it was developed for fitting log-logistic models that were used routinely in toxicology and pesticide science. Subsequently, the package has been extensively revised, mostly in response to feedback from the user community. By now, it has developed into a veritable ecosystem for all kinds of dose-response analyses. Similar functionality for dose-response analysis does not exist in any other statistical software.
This talk briefly reviews the development, mentinoning methods that have become obsolete, methods that have stayed in use, and new methods that have been incorporated. Changes have largely been propelled by the advent of powerful extension packages. Specifically, we will talk about modular use of multiple packages when doing dose-response analysis. We will show examples on modelling of event times, binary mixtures and species sensitivity distributions. Finally, we will outline a number of directions for future developments.
",r,2019
"With over 650 dependent packages, two guiding lights for the survival package are that ""you can't do everything"" and ""don't break it"". As a consequence, although there has been continuous improvement in the underlying algorithms, additions of major new functionality are rare. Version 3 is an exception. Major additions include much broader support for multi-state models, cacluation of absolute risk estimates, and data checking. A design goal was to make multi-state models as easy to fit and use as a single state coxph or survfit model, and to make absolute risk estimates as easy as a Kaplan-Meier curve --- all without rocking the boat with respect to current software. Why do this? The classic triad of Kaplan-Meier, log-rank, and Cox model are reliable tools for time-to-event data that has a single endpoint. However, in the authors' own work single endpoint models have now become the minority. We will give an example from the Mayo Clinic Study of Aging where keeping track of multiple endpoints is a key part of the analysis: the progression from cognitively normal (CN) to mild cognitive impairment (MCI) to dementia; low/medium/high levels of underlying neurodegeneration, the accumulation of amyloid plaques in the brain, and of course status of alive/dead. We will illustrate the new usages, and discuss how other packages can plug into this functionality and where to find further documentation and examples.
",r,2019
"In epidemiological studies of time-to-event data, a quantity of interest to the clinician and the patient is the absolute risk of an event given a covariate profile. However, time matching or risk set sampling (including Cox partial likelihood) eliminates the baseline hazard from the likelihood expression for the hazard ratios. This has to be recovered using a non-parametric method which leads to step-wise estimates of the cumulative incidence that are difficult to interpret. The analysis can be further complicated in the presence of competing events. Using case-base sampling, we explain how a single event type or competing risk analysis that directly models the hazard function parametrically can be performed using logistic or multinomial regression. This formulation thus allows us to leverage all the machinery developed for GLMs including goodness-of-fit tests and variable selection. Furthermore, because we are explicitly modeling time, we obtain smooth estimates of the cumulative incidence that are easy to interpret. We demonstrate our approach using data from patients who received stem-cell transplant for acute leukemia. We provide our method in the casebase R package published on CRAN with extensive documentation and examples.
",r,2019
"The package mixmeta offers an extended mixed-effects framework for meta-analysis, where potentially complex patterns of effect sizes are modelled through a flexible structure of fixed and random terms. This allows the definition of standard univariate fixed and random-effects models, but also extensions separately proposed in the literature, such as multivariate, network, multilevel, longitudinal, and dose-response meta-analysis and meta-regression. The main function mixmeta provides a simple way to specify alternative models through a formula syntax consistent with standard linear mixed-effects packages. Likelihood-based estimation procedures are implemented using an efficient and robust hybrid approach consisting of a combination of iterative generalized least squares and Newton-type algorithms. Methods functions for standard regression tasks and specific meta-analytical summaries. The package is thoroughly documented and includes illustrative examples that replicate published analyses using a variety of meta-analytical models.
",r,2019
"Drawing from fifteen years of concrete examples and true stories, from algorithmic uses (computational Bayesian statistics, then deep learning and reinforcement learning at DeepMind) to more recent and more applied ""AI for Good"" projects (joint with Amnesty International: detecting burned villages on satellite imagery in conflict zones, or studying abuse against women on Twitter, etc), we discuss here how the tools of several communities sometimes rival and sometimes complement each other - in particular R and Python, with a bit of Lua, C++ and Javascript thrown into the mix. Indeed, from mathematical statistics to computational statistics to data science to machine learning to the latest ""Artificial Intelligence"" effervescence, our large variety of practices are co-existing, overlapping, splitting, merging, and co-evolving. Their scientific ingredients are remarkably similar, yet their technical tools differ and can seem alien to each other. By the end of this talk, we hope each attendee will leave with, in the worst case some interesting stories, in a better case some more thoughts on how this multitude of approaches play together to their own strengths, and, in the best case, some intense conversations in the Q&A!
",r,2019
"In about 20 minutes on the morning of January 27, 2020, one engineer launched over 500 individual cloud server instances for workshop attendees at RStudio::conf and managed them for the duration of the workshops — all from a Shiny app. The RStudio team manages a variety of production systems using Shiny apps including our workshop infrastructure and access to our sales demo server.
The Shiny apps are robust enough for these mission-critical activities because of an important lesson from web engineering: separation of concerns between front-end user interaction logic and back-end business logic. This design pattern can be implemented in R by creating user interfaces in Shiny and managing interactions with other systems with Plumber APIs and R6 classes.
This pattern allows for even complex Shiny apps to still be understandable and maintainable. Moreover, this pattern of designing and building large Shiny apps is broadly applicable to any app that has substantial interaction with outside systems. Session attendees will gain an understanding of this pattern, which can be useful for many large Shiny apps.
Coauthor(s): Cole Arendt .
",r,2020
"Buffer is a brand new and innovative banking product by one of the largest retail banks in Norway, Sparebanken Vest, and it is powered by R.
In fact, the product’s decision engine is written entirely in R. We analyze whether a customer should get a loan and how much loan they should be allocated by analyzing large amounts of data from various sources. An essential part is analyzing the customer’s invoices using machine learning (XGBoost).
In this talk, we will cover:
How we use ML and Bayesian statistics to estimate the probability of an invoice being repaid.
How we successfully put the decision engine in production, using e.g. Plumber, Docker, CircleCI and Kubernetes.
What we have learned from using R in production at scale.
",r,2020
"We propose an innovative approach based on Bayesian nonparametric methods to the signal extraction of astronomical sources in gamma-ray count maps under the presence of a strong background contamination. Our model simultaneously induces clustering on the photons using their spatial information and gives an estimate of the number of sources, while separating them from the irregular signal of the background component that extends over the entire map. From a statistical perspective, the signal of the sources is modeled using a Dirichlet Process mixture, that allows to discover and locate a possible infinite number of clusters, while the background component is completely reconstructed using a new flexible Bayesian nonparametric model based on b-spline basis functions. The resultant can be then thought of as a hierarchical mixture of nonparametric mixtures for flexible clustering of highly contaminated signals. We provide also a Markov chain Monte Carlo algorithm to infer on the posterior distribution of the model parameters, and a suitable post-processing algorithm to quantify the information coming from the detected clusters. Results on different datasets confirm the capacity of the model to discover and locate the sources in the analysed map, to quantify their intensities and to estimate and account for the presence of the background contamination.
Coauthor(s): Mauro Bernardi, Alessandra R. Brazzale, Roberto Trotta, David A. van Dyk .
",r,2020
"Using the learnr package you can create interactive tutorials in your R Markdown documents. For a long time, you could only use the built-in question types, including R coding exercises and quizzes with single or multiple choice answers. Since the release of learnr version 0.10.0, it has been possible to create custom question types. The new framework allows you to define different behaviour for the appearance and behaviour of your questions. The sortable package uses this capability to introduce new question types for ranking questions and bucketing questions. With ranking questions you can ask your students to arrange answer options in the correct order. With bucketing questions you can ask them to arrange answer options into different buckets. The sortable package achieves this by exposing an htmlwidget wrapper around the SortableJS JavaScript library. This library lets you sort any object in a Shiny app, with dynamic drag-and-drop behaviour. For example, you can arrange items in a list, or drag-and-drop the order of shiny tabs. During this presentation you will see how easy it is to add dynamic behaviour to your shiny app, and how simple it is to use the new sorting and bucketing tasks in your tutorials.
Coauthor(s): Barret Schloerke, Kenton Russell .
",r,2020
"Sampling from multivariate distributions is a fundamental problem in statistics that plays important role in modern machine learning and data science. Many important problems such as convex optimization and multivariate integration can be efficiently solved via sampling. This talk presents the CRAN package volesti which offers to R community efficient C++ implementations of state-of-the-art algorithms for sampling and volume computation of convex sets. It scales up to hundred or thousand dimensions, depending the problem, providing the most efficient implementations for sampling and volume computation to date. Thus, volesti allows users to solve problems in dimensions and order of magnitude higher than before. We present the basic functionality of volesti and show how it can be used to provide approximate solutions to intractable problems in combinatorics, financial modeling, bioinformatics and engineering. We stand out two famous applications in finance. We show how volesti can be used to detect financial crises and evaluate portfolios performance in large stock markets with hundreds of assets, by giving real life examples using public data.
Coauthor(s): Vissarion Fisikopoulos .
",r,2020
"Assumed products have been a longstanding and growing pain for companies around the globe. In addition to impacting company revenue, they damage brand reputation and customer confidence. Companies were asked to build a solution for a global electronics brand that can identify fake products by just taking one picture on a smartphone.
In this session, we will look into the building blocks that make this AI solution work. We’ll find out that there is much more to it than just training a convolutional neural network.
We look at challenges like how to manage and monitor the AI model and how to build and improve the model in a way that fits your DevOps production chain.
Learn how we used Azure Functions, Cosmos DB and Docker to build a solid foundation. See how we used the Azure Machine Learning service to train the models. And find out how we used Azure DevOps to control, build and deploy this state-of-the-art solution.
",r,2020
"Open-source development is a great source of satisfaction and fulfillment, but someone has to pay the bills. A straightforward solution is to consult customers and help them to pick the right tools. As a small group of R enthusiasts, we try to align open source development by supporting our clients to accomplish their goals, contributing to the community along the way. It turns out that the benefits work in both ways: In addition to funding, consulting work allows us to test our tools and to improve their usability in a practical setting. At the same time, the involvement in open source development sharpens our analytical skills and serves as a first stop for new customers. Ideally, consulting projects lead to new developments, which in turn lead to new consulting projects.
Coauthor(s): Kirill Müller .
",r,2020
"We demonstrate in this talk, how we used a containerized R and PostgreSQL data pipeline to deduplicate 60 million real estate ads from Germany and Switzerland using a multi-step Naive Bayes record linkage approach. Real estate platforms publish millions of rental flat and condominium ads yearly. A given region or country of interest is normally covered by various competing platforms, leading to multiple published ads for a single real world object. Because quantifying and modeling the real estate market requires unbiased input data, our aim was to deduplicate real estate ads using Naive Bayes record linkage. We used commercially available German and Swiss real estate ad data from 2012 to 2019 consisting of approximately 60 million individual records. After multiple data cleaning and preparation steps we employed a Naive Bayes weighting of 12-14 variables to calculate similarity scores between ads and determined a linkage threshold based on expert judgment. The deduplication pipeline consisted of three steps: linking ads based on identity comparisons, linking similar ads within small regional areas (municipalities) and linking similar ads within large regional areas (cantons, states). The pipeline was deployed as a containerized setup with in-memory calculations in R and out-of-memory calculations and data storage in PostgreSQL. Deduplication linked the around 60 million ads to around 14 million object groups (Germany: 10 millions, Switzerland: 4 millions). The distribution of similarity scores showed high separation power and the resulting object groups displayed high homogeneity in geographic location and price distribution. Furthermore, yearly results corresponded well with published relocation rates. Using Naive Bayes record linkage to deduplicate real estate ads resulted in a sensible grouping of ads into object groups (rental flats, condominiums). We were able to combine similarities across different variables into a single similarity score. An advantage of the Naive Bayes approach is the high interpretability of the influence of individual variables. However, by manually determining the linkage threshold our results are heavily influenced by possible expert biases. The containerized R and PostgreSQL setup proved it’s portability and scaling capabilities. The same approach could easily be transferred to other domains requiring deduplication of multivariate data sets.
",r,2020
"Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This talk with provide examples of using {polite} package for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice.
",r,2020
"Eco-hydrological and biophysical models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, watershed or regional scale. However, to satisfy the researchers and stakeholders demand, user friendly interfaces are needed. The integration of such models in the powerful software environment of R greatly eases the application, input data preparation, output elaboration and visualization. In this work we present new developments for a R interface (R open-source package geotopbricks (https://CRAN.R-project.org/package=geotopbricks) and related) for the GEOtop hydrological distributed model (www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins and under scenarios of climate change and variability are shown. In particular, we will present an application to predict with the model winter snow conditions, which play a critical role in governing the spatial distribution of fauna in temperate ecosystems.
Coauthor(s): Giacomo Bertoldi .
",r,2020
"Interpreting the results from RNA-seq transcriptome experiments can be a complex task, where the essential information is distributed among different tabular and list formats - normalized expression values, results from differential expression analysis, and results from functional enrichment analyses.
The identification of relevant functional patterns, as well as their contextualization in the data and results at hand, are not straightforward operations if these pieces of information are not combined together efficiently.
Interactivity can play an essential role in simplifying the way how one accesses and digests RNA-seq data analysis in a more comprehensive way.
I introduce GeneTonic (https://github.com/federicomarini/GeneTonic), an application developed in Shiny and based on many essential elements of the Bioconductor project, that aims to reduce the barrier to understanding such data better, and to efficiently combine the different components of the analytic workflow.
For example, starting from bird’s eye perspective summaries (with interactive bipartite gene-geneset graphs, or enrichment maps), it is easy to generate a number of visualizations, where drill-down user actions enable further insight and deliver additional information (e.g., gene info boxes, geneset summary, and signature heatmaps).
Complex datasets interpretation can be wrapped up into a single call to the GeneTonic main function, which also supports built-in RMarkdown reporting, to both conclude an exploration session, or also to generate in batch the output of the available functionality, delivering an essential foundation for computational reproducibility.
",r,2020
"With the widespread usage of wearable devices great amount of data became available and new fields of application arised, like health monitoring and activity detection.
Our work focused on inactivity and sleep detection from continuous raw tri-axis accelerometer data, recorded using different accelerometers brands having sampling frequencies below and above 1Hz.
The algorithm implemented is the SPT-window detection algorithm described in literature slighty modified to met the flexibility requirement we imposed ourselves.
The R package developed provides functions to clean data, to identify inactivity/sleep windows and to visualize the results.
The main function has a parameter to specify the measurement unit of the data, a threshold to distinguish low and high activity and also a parameter to handle non-wearing periods, where a non wear period is defined as a period of time where all the accelerometers are equal to zero. Other functions allow to separate overlapped accelerometer signals, i.e. when a device is replaced by another, and to visualize the obtained results.
Coauthor(s): Ilaria Ceppa, Marco Calderisi, Davide Massidda, Matteo Papi, Gabriele Galatolo, Andrea Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli .
",r,2020
"The ‘progressr’ package provides a minimal, unifying API for scripts and packages to report progress from anywhere including when using parallel processing to anywhere.
It is designed such that the developer can focus on what to report progress on without having to worry about how to present it. The end user has full control of how, where, and when to render these progress updates. Progress bars from popular progress packages are supported and more can be added.
The ‘progressr’ is inclusive by design. Specifically, no assumptions are made how progress is reported, i.e. it does not have to be a progress bar in the terminal. Progress can also be reported as audio (e.g. unique begin and end sounds with intermediate non-intrusive step sounds), or via a local or online notification system.
Another novelty is that progress updates are controlled and signaled via R’s condition framework. Because of this, there is no need for progress-specific arguments and progress can be reported from nearly everywhere in R, e.g. in classical for and while loops, within map-reduce APIs like the ‘lapply()’ family of functions, ‘purrr’, ‘plyr’, and ‘foreach’. It also works with parallel processing via the ‘future’ framework, e.g. ‘future.apply’, ‘furrr’, and ‘foreach’ with ‘doFuture’. The package is compatible with Shiny applications.
",r,2020
"In regression models for spatial data, it is often assumed that the marginal effects of covariates on the response are constant over space. In practice, this assumption might often be questionable. Spatially varying coefficient (SVC) models are commonly used to account for spatial structure within the coefficients.
With the R package varycoef, we provide the frame work to estimate Gaussian process-based SVC models. It is based on maximum likelihood estimation (MLE) and in contrast to existing model-based approaches, our method scales better to data where both the number of spatial points is large and the number of spatially varying covariates is moderately-sized, e.g., above ten.
We compare our methodology to existing methods such as a Bayesian approach using the stochastic partial differential equation (SPDE) link, geographically weighted regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation study and an application where the goal is to predict prices of real estate apartments in Switzerland. The results from both the simulation study and application show that our proposed approach results in increased predictive accuracy and more precise estimates.
Coauthor(s): Fabio Sigrist, Reinhard Furrer .
",r,2020
"In this presentation, we will discuss using the latest techniques in computer vision as an important part of “AI for Good” efforts, namely, enhancing wildlife preservation. We will present how to make use of the latest technical advancements in an R setup even if they are originally implemented in Python.
A topic rightfully receiving growing attention among Machine Learning researchers and practitioners is how to make good use of the power obtained with the advancement of the tools. One of the avenues in these efforts is assisting wildlife conservation by employing computer vision in making observations of wildlife much more effective. We will discuss several of such efforts during the talk.
One of the very promising frameworks for computer vision developed recently is the Fast.ai wrapper of PyTorch, a Python framework used for computer vision among other things. While it incorporates the latest theoretical developments in the field (such as one cycle policy training) it provides an easy to use framework allowing a much wider audience to benefit from the tools, such as AI for Good initiatives run by people who are not formally trained in Machine Learning.
During the presentation we will show how to make use of a model trained using the Python’s fastai library within an R workflow with the use of the reticulate package. We will focus on use cases concerning classifying species of African wildlife based on images from camera traps.
Coauthor(s): Marek Rogala .
",r,2020
"The COVID-19 pandemic has turned the world upside down, and like everyone else the R Community is learning how to adapt to rapid change in order to carry on important work while looking for ways to contribute to the fight against the pandemic. In this talk, I will report on continuing R Community work being organized through the R Consortium such as the R Hub, R User Group Support Program and Diversity and Inclusion Projects; and through the various working groups including the Validation Hub, R / Pharma, R / Medicine and R Business. Additionally, I will describe some of the recently funded ISC projects and report on the COVID-19 Data Forum, a new project that the R Consortium is organizing in partnership with Stanford’s Data Science Institute.
",r,2020
"Turing e-Atlas is a research project under the Urban Analytics research theme at Alan Turing Institute (ATI). The ATI is UK’s national institute for data science and Artificial Intelligence based at the British Library in London.
The research is a grand vision for which we have been trying to take baby steps under the banner of an e-Atlas. And we believe R is positioned to play a foundation role in any scalable solution to analyse and visualize large scale datasets especially geospatial datasets.
The application presented is built using RStudio’s Plumber package which relies on solid libraries to develop web applications. The front-end is made up of Uber’s various visualization packages using Facebook’s React JavaScript framework.
Coauthor(s): Dr Nik Lomax, Dr Roger Beecham .
",r,2020
"Knowledge management is an indispensable component of modern-day, fast changing and flexible software engineering environments. A clear overview on how software developers collaborate can reveal interesting insights such as the general structure of collaboration, crucial resources, and risks in terms of knowledge preservation that can arise when a programmer decides to leave the company. Version control system (VCS) logs, which keep track of what team members work on and when, contain data to provide these insights. We present an R package that provides an algorithm which extracts and visualizes a collaboration graph from VCS log data. The algorithm is based on principles from graph theory, cartography and process mining. Its structure consists of four phases: (1) building the base graph, (2) calculating weights for nodes and edges, (3) simplifying the graph using aggregation and abstraction, and (4) extending it to include specific insights of interest. Each of these phases offers the user a lot of flexibility in deciding which parameters and metrics to include. This makes it possible for the human expert to exploit his existing knowledge about the project and team to guide the algorithm in building the graph that best fits his specific use case, and hence will provide the most accurate insights.
Coauthor(s): Gert Janssenswillen, Mathijs Creemers, Mieke Jans, Benoît Depaire .
",r,2020
"The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis or diffusion tensor imaging. In many cases, the available data are georeferenced, making spatial dependence a non-negligible data characteristic. Modeling and accounting for it, typically, is not trivial, because of the non-linear geometry of the manifold. In this contribution, we present the Manifoldgstat R package, which provides a set of fast routines allowing to efficiently analyze sets of spatial Riemannian data, based on state-of-the-art statistical methodologies. The package stems from the need to create an efficient and reproducible environment allowing to run extensive simulation studies and bagging algorithms for spatial prediction of symmetric positive definite matrices. The package implements three main algorithms (Pigoli et al, 2016, Menafoglio et al, 2019, Sartori & Torriani, 2019). The latter two are particularly computationally demanding, as they rely on Random Domain Decompositions of the geographical domain. To substantially improve performances, the package exploits dplyr and Rcpp to integrate R with C++ code, where template factories handle all run-time choices. In this communication, we shall revise the characteristics of the three methodologies considered, and the key-points of their implementation.
Coauthor(s): Alessandra Menafoglio, Piercesare Secchi .
",r,2020
"In studies of elections, voting outcomes are point-referenced at voting stations while socioeconomic covariates are areal data available at census tracts. The misaligned spatial structure of these two data sources makes the regression analysis to identify socioeconomic factors that affect the voting outcomes a challenge. Here we propose a novel approach to link these two sources of spatial data through Voronoi tessellation. Our proposal is creating a Voronoi tessellation with respect to the point-referenced data, with this outset, the spatial points become a set of mutually exclusive polygons named Voronoi cells. The extraction of data from the census tracts is proportional to the intersection area of each census tract polygon and Voronoi cells. Consequently, we use 100% of the available information and preserve the polygons’ autocorrelation structure. When visualised through our Shiny App, the method provides a finer spatial resolution than municipalities and facilitates the identification of spatial structures at a most detailed level. The technique is applied for the 2018 Brazilian presidential election data. The tool provides deep access to Brazilian election results by enabling to create general maps, plots, and tables by states and cities.
Coauthor(s): Lucas Godoy, Douglas Azevedo, Augusto Marcolin, Jun Yan .
",r,2020
"In this talk you’ll learn the tools and best practices for making clean, reproducible R code in a working environment ready to be shared and productionalised. Save your time for maintenance, adjusting and struggling with packages.
R is a great tool for fast data analysis. It’s simplicity in setup combined with powerful features and community support makes it a perfect language for many subject matter experts e.g. in finance or bioinformatics. Yet often what started as a pet project or proof of concept begins to grow and expand, with additional collaborators working on it. It is then crucial that you have your project organised well, reusable, with an environment set, so that the code works every time and on any machine. Otherwise the solution won’t be used by anyone but you. By following a few patterns and with appropriate tools it won’t be overwhelming or disturbing and will highlight the true value of the code.
Both Appsilon and I personally have taken part in many R projects for which the goal was to clean and organise the code as well as the project structure. We would like to share our experience, best practices and useful tools to share code shamelessly.
During the presentation I will show:
setting up the development environment with packrat, renv and docker,
organising the project structure,
the best practices in writing R code, automated with linter,
sharing the code using git,
organising workflow with drake,
optimising the Shiny apps and data loading with plumber and database,
preparing the tests and continuous integration circle CI.
",r,2020
"How we are using R as the foundation of all the data-related tasks in the biggest dealer digital program at FCA.
From simple tasks as dashboarding or reporting to more strategic capabilities as predicting advertising ROI through Tensorflow or developing a production-grade, data-driven microservices, we leverage R ecosystem to deliver better results and increase the data awareness for all the project stakeholders.
",r,2020
"The KOF Swiss Economic Institute at ETH Zurich (Switzerland) regularly surveys more than 10K companies, computes economic indicators and forecasts as it monitors the national economy. Today, the institute updates its website in automated fashion and operates automated data pipelines to partners such as regional statistical offices or the Swiss National Bank. At KOF, production is based on an open source ecosystem to a large degree. More and more processes are being migrated to an environment that consists of open source components Apache Airflow, Docker, Gitlab Continous Integration, PostgreSQL and R. This talk shows not only how well R interfaces and works with all parts from workflow automation to databases, but also how R’s advantages impact this system: From R Studio Servers to internal packages and an own internal mini-CRAN, the use of the R language is crucial in making the environment stable and convenient to maintain with the software carpentry type of background.
",r,2020
"High dimensional data generated by modern high-throughput platforms pose a great challenge in selecting a small number of informative variables, for biomarker discovery and classification. Machine learning is an appropriate approach to derive general knowledge from data, identifying highly discriminative features and building accurate prediction models. To this end, we developed the R/Bioconductor package DaMiRseq, which (i) helps researchers to filter and normalize high dimensional datasets, arising from RNA-Seq experiments, by removing noise and bias and (ii) exploits a custom machine learning workflow to select the minimum set of robust informative features able to discriminate classes.
Here, we present the version 2.0 of the DaMiRseq package, an extension that provides a flexible and convenient framework for managing high dimensional data such as omics data, large-scale medical histories, or even social media and financial data. Specifically, DaMiRseq 2.0 implements new functions that allow training and testing of several different classifiers and selection of the most reliable one, in terms of classification performance and number of selected features. The resulting classification model can be further used for any prediction purpose. This framework will give users the ability to build an efficient prediction model that can be easily replicated in further related settings.
Coauthor(s): Chiara Vavassori, Gualtiero I. Colombo, Luca Piacentini .
",r,2020
"Lots of data is registered within hospitals, for financial, clinical and administrative purposes. Today, this data is barely used. Due to not knowing the existence of the data, the possible applications and the skills to execute the analysis, …
In this presentation we show how we can apply R on this data and what the possibilities are using standard available hospital-wide data on a low cost budget.
1. Reporting with R
- using R and markdown as a tool for management reporting
- Using R for data handling (ETL)
- Shiny applications as alternative for dashboarding
2. Using R as a statistical tool
- Performing regression models to gain insight in certain predictor
3. Using R a data science tool
- Using R to perform Machine Learning analysis, e.g. Random Forests
- Using R for the data wrangling and handle the high dimensional data
4. Requirements for all of the above
",r,2020
"R’s ability to do symbolic mathematics is largely restricted to finding derivatives. There are many tasks involving symbolic math and that are of interest to R users, e.g. inversion of symbolic matrices, limits and solving non-linear equations. Users must resort to other computer algebra systems (CAS) for such tasks and many R users (especially outside of academia) do not readily have access to such software. There are also other indirect use-cases of symbolic mathematics in R that can exploit other strengths of R, including Shiny apps with auto-generated mathematics exercises.
We are maintaining two packages enabling symbolic mathematics in R: Ryacas and caracas. Ryacas is based on Yacas (Yet Another Computer Algebra System) and caracas is based on SymPy (Python library). Each have their advantages: Yacas is extensible and has a close integration to R which makes auto-generated mathematics exercises easy to make. SymPy is feature-rich and thus gives many possibilities.
In this talk we will discuss the two packages and demonstrate various use-cases including uses that help understanding statistical models and Shiny apps with auto-generated mathematics exercises.
Coauthor(s): Søren Højsgaard .
",r,2020
"Recently, generative Deep Learning approaches were shown to have a huge potential for e.g. retrieving compact, latent representations of high-dimensional omics data such as single-cell RNA-Seq data. However, there are no established methods to infer how these latent representations relate to the observed variables, i.e. the genes.
For extracting interpretable patterns from gene expression data that indicate distinct sub-populations in the data, we here employ log-linear models, applied to the synthetic data and corresponding latent representations, sampled from generative deep models, which were trained with single-cell gene expression data.
While omics data are routinely analyzed in R and powerful toolboxes, tailored to omics data are available, there are no established and truely accessible approaches for Deep Learning applications here.
To close this gap, we here demonstrate how easily customizable Deep Learning frameworks, developed for the Julia programming language, can be leveraged in R, to perform accessible and interpretable Deep Learning with omics data.
Coauthor(s): Stefan Lenz, Harald Binder .
",r,2020
"Shiny App developers have warmly welcomed the concept of Shiny modules as a way to simplify the app development process through the introduction of reusable building blocks. Shiny modules are similar in spirit to the concept of functions in R, except each is implemented with paired ui and server codes along with their own namespace. The {tidymodules} R package introduces a novel structure that harmonizes module development based on R6 (https://r6.r-lib.org/), which is an implementation of encapsulated object-oriented programming for R, thus knowledge of R6 is a prerequisite for using {tidymodules} to develop Shiny modules. Some key features of this package are module encapsulation, reference semantics, central module store and an innovative framework for enabling and facilitating cross module – module communication. It does this through the creation of “ports”, both input and output, where users may pass data and information through pipe operators. Because the connections are strictly specified, the module network may be visualized which shows how data move from one module to another. We feel the {tidymodules} framework will simplify the module development process and will reduce the code complexity through programming concepts like inheritance.
Coauthor(s): Doug Robinson, Xiao Ni, David Granjon .
",r,2020
"Brain functional connectivity is widely investigated in neuroscience. In recent years, the study of brain connectivity has been largely aided by graph theory. The link between time series recorded at multiple locations in the brain and a graph is usually an adjacency matrix. This converts a measure of the connectivity between two time series, typically a correlation coefficient, into a binary choice on whether the two brain locations are functionally connected or not. As a result, the choice of a threshold over the correlation coefficient is key.
In the present work, we propose a multiple testing approach to the choice of a suitable threshold using the Bayes false discovery rate (FDR) and a new estimator of the statistical power called average power function (APF) to balance the two types of statistical error.
We show that the proposed APF behaves well in case of independence of the tests and it is reliable under several dependence conditions. Moreover, we propose a robust method for threshold selection using the 5% and 95% percentiles of APF and FDR bootstrap distributions, respectively, to improve stability.
In addition, we developed a R-package called APFr which performs APF and Bayes FDR robust estimation and provides simple examples to improve usability. The package has attracted more than 3200 downloads since its publication online (June 2019) at https://CRAN.R-project.org/package=APFr.
Coauthor(s): Piero Quatto .
",r,2020
"Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts.
Coauthor(s): Andreas Brandmaier .
",r,2020
"Computational-mathematical models can be efficiently used to provide new insights into drivers of a disease spread, investigating different explanations of observed resurgence and predicting potential effects of different therapies. In this context, we present a new general modeling framework for the analysis of epidemiological and biological systems, characterized by features that make easy its utilization even by researchers without advanced mathematical and computational skills. The implementation of the R package, called “Epimod”, provides a friendly interface to access the model creation and the analysis techniques implemented in the framework. In details, by exploiting the graphical formalism of the Petri Net it is possible to simplify the model creation phase, providing a compact graphical description of the system and an automatically derivation of the underlying stochastic or deterministic process. Then, by using four functions it is possible to deal with Model Generation, Sensitivity Analysis, Model Calibration and Model Analysis phases. Finally, the Docker containerization of all analysis techniques grants a high level of portability and reproducibility. We applied Epimod to model pertussis epidemiology, investigating alternative explanations of its resurgence and to predict potential effects of different vaccination strategies.
Coauthor(s): Simone Pernice, Matteo Sereno, Marco Beccuti. .
",r,2020
"Data scientists with their valuable skills have enormous potential to contribute to the social good. This is also true for the R community - and R users seem to be especially motivated to use their skills for the social good, as the overwhelmingly positive reception of Julien Cornebise’s keynote “AI for Good” at useR2019 (Cornebise 2019) has shown. However, specific strategies for putting the abstract goal “use data science for the social good” into practice are often missing, especially in volunteering contexts like the R community, where resources are often limited.
In our talk, we present formats that we have implemented on the local level to build R-focused, data-for-good communities across Europe. Originating from the German data4good network CorrelAid with its over 1600 members, we have established 9 local CorrelAidX groups in Germany, the Netherlands, and France.
The specific formats build on a three-pillared concept of community building, namely group-bonding, social entrepreneurship and outreach. We present multiple examples that illustrate how our local chapters operate to put data science for good into practice - using the formats of data dialogues, local data4good projects, and CorrelAidX workshops. Lastly, we also outline possibilities to implement such formats in cooperation between CorrelAidX chapters and R community groups such as R user groups or RLadies chapters.
Coauthor(s): Konstantin Gavras .
",r,2020
"Hundreds of speakers may describe the same circumstance - e.g. explaining a fixed route to a goal - without producing two identical texts. The enormous variability of language and the complexity involved in encoding meaning poses a real difficulty for linguists analyzing text databases. In order to aid linguists in identifying patterns to perform comparative research, we developed an interactive shiny app that enables quantitative analysis of text corpora without oversimplifying the structure of language. Route directions are an example of complex texts, in which speakers take cognitive decisions such as segmenting the route, selecting landmarks and organizing spatial concepts into sentences. In the data visualization, symbols and colors representing linguistic concepts are plotted into coordinates that relate the information to fixed points along the route. Six interconnected layers of meaning represent the multi-layered form-to-meaning mapping characteristic of language. The shiny app allows to select and deselect information on these different layers, offering a holistic linguistic analysis way beyond the complexity attempted within traditional linguistics. The result is a kind of visual language in itself that deconstructs the interconnected layers of meaning found in natural language.
Coauthor(s): Paula González Ávalos .
",r,2020
"Literature is characterized by a broad class of mathematical models which can be used for fitting cancer growth time series, but with no a global consensus or biological evidence that can drive the choice of the correct model. The conventional perception is that the mechanistic models enable the biological understanding of the systems under study. However, there is no way that these models can capture the variability characterizing the cancer progression, especially because of the irregularity and sparsity of the available data.
For this reason, we propose CONNECTOR, an R package built on the model-based approach for clustering functional data. Such method is based on the clustering and fitting of the data through a combination of cubic natural splines basis with coefficients treated as random variables. Our approach is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. CONNECTOR provides a tool set to easily guide through the parameters selection, i.e., (i) the dimension of the spline basis, (ii) the dimension of the mean space and (iii) the number of clusters to fit, to be properly chosen before fitting. The effectiveness of CONNECTOR is evaluated on growth curves of Patient Derived Xenografts (PDXs) of ovarian cancer. Genomic analyses of PDXs allowed correlating fitted and clustered PDX growth curves to cell population distribution.
Coauthor(s): Beccuti Marco, Sirovich Roberta, Cordero Francesca .
",r,2020
"Weighted Quantile Sum (WQS) regression is a statistical model for multivariate regression in high-dimensional datasets commonly encountered in environmental exposures. The model constructs a weighted index estimating the mixture effect associated with all predictor variables on an outcome. The package gWQS extends WQS regression to applications with continuous, categorical and count outcomes. We provide four examples to illustrate the usage of the package.
Coauthor(s): Chris Gennings, Paul C. Curtin .
",r,2020
"This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption.
Reporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited.
As part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand.
The resulting ‘R toolbox for journalists’ is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions.
The accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective.
",r,2020
"Shiny is nice technology to write interactive R-based applications. It is broadly adopted and the R community has collaborated on many
interesting extensions. Until recently, though, deployments in larger organizations and companies required proprietary solutions. ShinyProxy fills this gap and
offers a fully open source alternative to run and manage shiny applications at large. In this talk we detail the
ShinyProxy architecture and demonstrate how it meets the needs of organizations.
We will discuss how it scales to thousands of concurrent users and how it offers authentication and
authorization functionality using standard technologies (LDAP, ActiveDirectory, OpenID Connect, SAML 2.0 and Kerberos).
Also, we will discuss the management interface and how it allows to monitor application usage to collect
usage statistics in event logging databases. Finally, we will demonstrate that Shiny applications
can now be easily embedded in broader applications and (responsive) web sites using the ShinyProxy API.
Learn how academic institutions, governmental organizations and industry roll out Shiny apps with
ShinyProxy and how you can do this too. See https://shinyproxy.io.
",r,2020
